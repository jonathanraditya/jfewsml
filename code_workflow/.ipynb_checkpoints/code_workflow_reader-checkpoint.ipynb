{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e6058e-cdbe-40e2-bf67-db9ef7170cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e7f255-746f-4afa-8bde-17c82a95c335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "***Created by Jonathan Raditya Valerian - jonathanraditya@live.com***\n",
       "\n",
       "Data Visualization Table of contents:\n",
       "- [ ] Sadewa Visual Preview (sadewa_visualpreview.ipynb)\n",
       "- [ ] 4px\\*4px & 28px\\*28px Situation Map Input Data Extent (cartopy_inputextent.ipynb)\n",
       "- [ ] Mini Map Study Area Extent (cartopy_inputextent.ipynb)\n",
       "- [ ] 4px\\*4px, 8px\\*8px, 16px\\*16px, and 28px\\*28px LSTM RNN Spatial Optimization (cartopy_inputextent.ipynb)\n",
       "- [ ] Katulampa Rain Series (cartopy_inputextent.ipynb)\n",
       "- [ ] Katulampa WL and Sadewa Data Availability (available_data.ipynb)\n",
       "- [ ] Deep Neural Network Simulation Results (DNNsimulation_results.ipynb)\n",
       "- [ ] Timeseries Results Visualization (preview_model_result.ipynb)\n",
       "\n",
       "\n",
       "\n",
       "Data Gathering & Preprocessing Table of contents:\n",
       "- [ ] Fetching Sadewa Data (sadewa_mining.py)\n",
       "- [ ] Fetching Water Level Data (DSDA Data Fetch 2.ipynb)\n",
       "- [ ] Wind Data Error Handling (windCheck1.py & windCheck2.py) \n",
       "- [ ] Creating Deep Neural Network Input Datasets (imgcrop.py)\n",
       "- [ ] Creating Master Raw-unstacked Datasets for RNN (imgcrop.py)\n",
       "- [ ] Restacking Simple RNN Input Datasets (restackRecurrent.py)\n",
       "- [ ] Restacking LSTM RNN Input Datasets (LSTM_restacking-1.py)\n",
       "- [ ] Restacking Flagged LSTM RNN Input Datasets (LSTM_restacking!-1.py)\n",
       "\n",
       "Machine Learning Models\n",
       "- [ ] Deep Neural Network Model\n",
       "- [ ] Deep Neural Network Flagged Model (DNNsimulation-8-1-flag.ipynb)\n",
       "- [ ] Simple Recurrent Neural Network Model\n",
       "- [ ] LSTM Recurrent Neural Network Model\n",
       "- [ ] LSTM Recurrent Neural Network Flagged Model\n",
       "- [ ] LSTM Recurrent Neural Network Unflagged 4K Data Model\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "# General Functions\n",
       "\n",
       "\n",
       "```python\n",
       "# import modules\n",
       "import sqlite3\n",
       "from sqlite3 import Error\n",
       "import os\n",
       "import matplotlib.pyplot as plt\n",
       "import matplotlib.image as mpimg\n",
       "import datetime\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import random\n",
       "from skimage import color\n",
       "import h5py\n",
       "import time\n",
       "import tensorflow as tf\n",
       "from sklearn.metrics import r2_score\n",
       "import hydroeval\n",
       "import keras\n",
       "from PIL import Image\n",
       "import copy\n",
       "%matplotlib inline\n",
       "\n",
       "\n",
       "def hello():\n",
       "    print('Hello!')\n",
       "\n",
       "def showSampleSadewaData():\n",
       "    '''\n",
       "    showing sadewa sampel data \n",
       "    current : cloud-0\n",
       "    '''\n",
       "    himawariPath='../mining_sadewa/sadewa/'\n",
       "    himawari=getHimawariFilename()\n",
       "\n",
       "    data=himawari['cloud'][0]\n",
       "    fullPath='{}{}/{}'.format(himawariPath, 'cloud', data)\n",
       "    img=mpimg.imread(fullPath)\n",
       "    plt.imshow(img, cmap='rainbow')\n",
       "    plt.colorbar()\n",
       "\n",
       "def create_connection(db_file):\n",
       "    '''\n",
       "    create a database connection to a SQLite database\n",
       "    specified by db_file\n",
       "    :param db_file : database file\n",
       "    :return: Connection Object or None\n",
       "    '''\n",
       "    conn=None\n",
       "    try:\n",
       "        conn=sqlite3.connect(db_file)\n",
       "        return conn\n",
       "    except Error as e:\n",
       "        print(e)  \n",
       "    \n",
       "def printSadewaFetchMissingData():\n",
       "    '''\n",
       "    printing total count of missing data for every sadewa-himawari dataset\n",
       "    '''\n",
       "    sadPath='../mining_sadewa/sadewaerr.txt'\n",
       "    with open(sadPath, 'r') as sErrFile:\n",
       "        errContent=sErrFile.read()\n",
       "\n",
       "    rawErrList=errContent.split('.png')\n",
       "    cleanErrUrl=[]\n",
       "    for err in rawErrList:\n",
       "        cleanErrUrl.append('{}.png'.format(err))\n",
       "\n",
       "    database={\n",
       "            'IR1':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/IR1/{}/{}/{}/',\n",
       "                'fname':'H89_IR1_{}{}{}{}00.png',\n",
       "                'yearStart':'2020'\n",
       "            },\n",
       "            'IR3':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/IR3/{}/{}/{}/',\n",
       "                'fname':'H89_IR3_{}{}{}{}00.png',\n",
       "                'yearStart':'2020'\n",
       "            },\n",
       "            'VIS':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/VIS/{}/{}/{}/',\n",
       "                'fname':'H89_VIS_{}{}{}{}00.png',\n",
       "                'yearStart':'2020'\n",
       "            },\n",
       "            'B04':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/B04/{}/{}/{}/',\n",
       "                'fname':'H89_B04_{}{}{}{}00.png',\n",
       "                'yearStart':'2020'\n",
       "            },\n",
       "            'CCLD':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/komposit/{}/{}/{}/',\n",
       "                'fname':'H89_CCLD_{}{}{}{}00.png',\n",
       "                'yearStart':'2020'\n",
       "            },\n",
       "            'rain':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'rain_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'cloud':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'cloud_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'psf':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'psf_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'qvapor':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'qvapor_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'sst':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'sst_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'wind':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'wind_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'winu':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'winu_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            },\n",
       "            'wn10':{\n",
       "                'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "                'fname':'wn10_{}{}{}_{}.png',\n",
       "                'yearStart':'2019'\n",
       "            }\n",
       "        }\n",
       "\n",
       "    # making list of data keys through list comprehension\n",
       "    dataKeys=[x for x in database.keys()]\n",
       "    errClassification={}\n",
       "    # establishing dictionary\n",
       "    for key in dataKeys:\n",
       "        errClassification[key]=[]\n",
       "\n",
       "    # loop to append error URL for each data type\n",
       "    for err in cleanErrUrl:\n",
       "        # loop for each key to check for True value\n",
       "        for key in dataKeys:\n",
       "            # check for match data type\n",
       "            if key in err:\n",
       "                errClassification[key].append(err)\n",
       "                continue\n",
       "\n",
       "    # print count of missing data for each data \n",
       "    for errKey in errClassification.keys():\n",
       "        print(errKey, len(errClassification[errKey]))\n",
       "        \n",
       "def idealDataCount():\n",
       "    '''\n",
       "    returning (2019idc, 2020idc) ideal data count for Himawari dataset\n",
       "    '''\n",
       "    # calculating ideal data count for each entry date\n",
       "    early2019=[2019,1,1]\n",
       "    early2020=[2020,1,1]\n",
       "    minedDate=[2021,3,14]\n",
       "    fdE19=datetime.datetime(*(early2019))\n",
       "    fdE20=datetime.datetime(*(early2020))\n",
       "    fdMD=datetime.datetime(*(minedDate))\n",
       "\n",
       "    dateRange2019=(fdMD-fdE19).days\n",
       "    dateRange2020=(fdMD-fdE20).days\n",
       "\n",
       "    # ideal data count for each entry date\n",
       "    dataCount2019=dateRange2019*24\n",
       "    dataCount2020=dateRange2020*24\n",
       "    \n",
       "    return dataCount2019, dataCount2020\n",
       "\n",
       "def manggaraiFullData():\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM manggarai', conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "    \n",
       "    return manggaraiConv\n",
       "\n",
       "def manggaraiDataList(maxData=True, hourOffset=0, wlstation='manggarai'):\n",
       "    '''\n",
       "    Returning a tuple of list (date, data) of manggarai TMA data with 10-minutes-interval from DSDA dataset in year 2020\n",
       "    '''\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM {}'.format(wlstation), conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "\n",
       "    # slicing data to 2020 timeframe\n",
       "    #mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00') & (manggaraiConv['currentdate'] <= '2021-04-03 23:50')\n",
       "    mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00')\n",
       "    manggaraiSlice2020=manggaraiConv.loc[mask]\n",
       "\n",
       "    # converting 10-minute-data to hourly data\n",
       "    startDate=datetime.datetime(2019,2,1)\n",
       "    minutes=[x*10 for x in range(6)]\n",
       "    hours=[x for x in range(24)]\n",
       "    days=[x for x in range(780)]\n",
       "\n",
       "    dateListHourly=[]\n",
       "    dataListHourly=[]\n",
       "    for day in days:\n",
       "        for hour in hours:\n",
       "            hourlyData=[]\n",
       "\n",
       "            # set error indicator back to false\n",
       "            error=False\n",
       "\n",
       "            for minute in minutes:\n",
       "                # perform data fetch, add to list, and get max value\n",
       "                dateLoop=startDate+datetime.timedelta(days=day, hours=hour+hourOffset, minutes=minute)\n",
       "                rowFetch=manggaraiSlice2020.loc[(manggaraiSlice2020['currentdate'] == dateLoop)]\n",
       "                #print(rowFetch)\n",
       "\n",
       "                # try to fetch if the result is not zero\n",
       "                try:\n",
       "                    dataFetch=rowFetch['data'].item()\n",
       "                    hourlyData.append(dataFetch)\n",
       "                except ValueError:\n",
       "                    error=True\n",
       "\n",
       "            # insert data if error indicator is False\n",
       "            if not error:\n",
       "                # make hourly date using timedelta\n",
       "                hourlyDate=startDate+datetime.timedelta(days=day, hours=hour)\n",
       "                \n",
       "                if maxData:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=max(hourlyData)\n",
       "                else:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=hourlyData.mean()\n",
       "\n",
       "                # insert value to global list\n",
       "                dateListHourly.append(hourlyDate)\n",
       "                dataListHourly.append(maxDataHourly)\n",
       "            else: # if error occured during data fetch (null or something else)\n",
       "                continue # to next loop\n",
       "    return dateListHourly, dataListHourly\n",
       "\n",
       "def getHimawariFilename():\n",
       "    '''\n",
       "    Return dictionary of available himawari data based on filename inside\n",
       "    folder as a key\n",
       "    '''\n",
       "    himawariPath='../mining_sadewa/sadewa/'\n",
       "    # load folder name\n",
       "    directory=[directory for directory in os.listdir(himawariPath)]\n",
       "\n",
       "    # store fileame\n",
       "    himawari={}\n",
       "\n",
       "    # load all filename stored on disk to dictionary with each folder name as keys\n",
       "    for direct in directory:\n",
       "        fpath='{}{}'.format(himawariPath, direct)\n",
       "        himawari[direct]=[fname for fname in os.listdir(fpath)]\n",
       "        \n",
       "    return himawari\n",
       "\n",
       "def extractHimawariDatetime():\n",
       "    '''\n",
       "    Extract every filename in sadewa-himawari data to datetime object for easier handling\n",
       "    \n",
       "    Returns :\n",
       "    extractedDate -- dictionary containing list of datetime object for each filename inside dictionary keys for every data\n",
       "    '''\n",
       "    himawari=getHimawariFilename()\n",
       "\n",
       "    # extract date for each himawari data type to datetime.datetime object\n",
       "    observations=['CCLD','B04','IR1','IR3','VIS']\n",
       "    extractedDate={}\n",
       "    for obs in observations:\n",
       "        extractedDate[obs]=[datetime.datetime.strptime(x.replace('H89_{}_'.format(obs),'').replace('.png',''), '%Y%m%d%H%M') for x in himawari[obs]]\n",
       "\n",
       "    predictions=['cloud','psf','qvapor','rain','sst','wind','winu','wn10']\n",
       "    for pred in predictions:\n",
       "        extractedDate[pred]=[datetime.datetime.strptime(x.replace('{}_'.format(pred),'').replace('.png','').replace('_','')+'00', '%Y%m%d%H%M') for x in himawari[pred]]\n",
       "        \n",
       "    return extractedDate\n",
       "\n",
       "def getAvailableSlicedData(maxData=True, hourOffset=0, dataScope='combination', wlstation='manggarai', flagged=False):\n",
       "    '''\n",
       "    check through all available dataset, including manggarai TMA, sadewa-himawari IR1, IR3, VIS, B04, and CCLD\n",
       "    and return a tuple containing datetime object and manggarai hourly TMA data that are synced through all available dataset\n",
       "    \n",
       "    This function doesn't return sadewa-himawari data, because using the datetime format and the sadewa-himawari data types,\n",
       "    the full name of the file required can be constructed.\n",
       "    \n",
       "    return : (slicedDate, slicedData) # both are lists inside a tuple\n",
       "    '''\n",
       "    extractedDate = extractHimawariDatetime()\n",
       "        \n",
       "    # getting date-data slice from himawari and manggarai TMA data\n",
       "\n",
       "    # using function to get manggarai available date-data\n",
       "    dateListHourly, dataListHourly = manggaraiDataList(maxData, hourOffset, wlstation=wlstation)\n",
       "    \n",
       "    # check if the data is flagged above the mean or not\n",
       "    if flagged:\n",
       "        dateListHourly, dataListHourly = flagData(dateListHourly, dataListHourly)\n",
       "\n",
       "    # loop to every data\n",
       "    # check algorithm : manggarai checked against every himawari data, and if all true, date is inserted to sliced data\n",
       "    slicedDate=[]\n",
       "    slicedData=[]\n",
       "    for i in range(len(dateListHourly)):\n",
       "        \n",
       "        if dataScope == 'combination':\n",
       "            usedData=['CCLD','B04','IR1','IR3','VIS','rain','cloud','psf','qvapor','sst']\n",
       "        elif dataScope == 'prediction':\n",
       "            usedData=('cloud','psf','qvapor','rain','sst','wind','winu','wn10')\n",
       "\n",
       "        # defining control mechanism\n",
       "        checked=True\n",
       "\n",
       "        # loop through every himawari data\n",
       "        for used in usedData:\n",
       "            if dateListHourly[i] not in extractedDate[used]:\n",
       "                checked=False # set checked to False if there are no complementary data found in another dataset\n",
       "\n",
       "        # input data if all checked\n",
       "        if checked:\n",
       "            slicedDate.append(dateListHourly[i])\n",
       "            slicedData.append(dataListHourly[i])\n",
       "    return slicedDate, slicedData\n",
       "\n",
       "def flagData(adte, adta):\n",
       "    '''\n",
       "    Filter date and data above the mean\n",
       "    '''\n",
       "    adtaDF = pd.DataFrame(adta).astype('int32')\n",
       "    adteDF = pd.DataFrame(adte)\n",
       "    flaggedAdta = adtaDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    flaggedAdte = adteDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    return list(flaggedAdte[0].dt.to_pydatetime()), list(flaggedAdta[0].astype('object'))\n",
       "\n",
       "\n",
       "def statisticsRaw(maxData=True):\n",
       "    '''\n",
       "    Return pandas dataframe of statistics in all available data\n",
       "    \n",
       "    column 0 : date\n",
       "    column 1 : tma\n",
       "    column 2 - 152 : obs/pred * dataset * med/mean/stdev/min/max\n",
       "    '''\n",
       "    adte, adta = getAvailableSlicedData(maxData)\n",
       "\n",
       "    himawariData = {'o100' : {'fname' : 'observation100',\n",
       "                              'dataset' : ['CCLD','B04','IR1','IR3','VIS']},\n",
       "                    'o196' : {'fname' : 'observation196',\n",
       "                              'dataset' : ['CCLD','B04','IR1','IR3','VIS']},\n",
       "                    'o400' : {'fname' : 'observation400',\n",
       "                              'dataset' : ['CCLD','B04','IR1','IR3','VIS']},\n",
       "                    'p100' : {'fname' : 'prediction100',\n",
       "                              'dataset' : ['rain','cloud','psf','qvapor','sst']},\n",
       "                    'p196' : {'fname' : 'prediction196',\n",
       "                              'dataset' : ['rain','cloud','psf','qvapor','sst']},\n",
       "                    'p400' : {'fname' : 'prediction400',\n",
       "                              'dataset' : ['rain','cloud','psf','qvapor','sst']}}\n",
       "\n",
       "    df = {'date':adte,\n",
       "          'tma':adta}\n",
       "    dtDF = pd.DataFrame(df)\n",
       "    dtDF['tma'] = dtDF['tma'].astype('int64')\n",
       "\n",
       "    for himawari in himawariData:\n",
       "\n",
       "        # start statistics\n",
       "        tick = time.time()\n",
       "\n",
       "        # initialize new list for each column\n",
       "        statistics=[[[],[],[],[],[]],\n",
       "                   [[],[],[],[],[]],\n",
       "                   [[],[],[],[],[]],\n",
       "                   [[],[],[],[],[]],\n",
       "                   [[],[],[],[],[]]]\n",
       "        statisticsHeader=['med','mean','stdv','min','max']\n",
       "\n",
       "        fname = himawariData[himawari]['fname']\n",
       "        dataset = himawariData[himawari]['dataset']\n",
       "\n",
       "        # print current dataset\n",
       "        print(dataset)\n",
       "\n",
       "        # open file\n",
       "        with h5py.File('{}.hdf5'.format(fname), 'r') as f:\n",
       "            fetchData = f['datas'][()]\n",
       "\n",
       "\n",
       "        # loop for each data row\n",
       "        for i in range(len(fetchData)):\n",
       "\n",
       "            # loop for each dataset\n",
       "            for j in range(len(fetchData[i])):\n",
       "                # fetch image data\n",
       "                imageData = fetchData[i][j]\n",
       "\n",
       "                # convert rgba to rgb\n",
       "                rgb = color.rgba2rgb(imageData)\n",
       "\n",
       "                statistics[0][j].append(np.median(rgb)) \n",
       "                statistics[1][j].append(np.mean(rgb))\n",
       "                statistics[2][j].append(np.std(rgb))\n",
       "                statistics[3][j].append(np.min(rgb))\n",
       "                statistics[4][j].append(np.max(rgb))\n",
       "\n",
       "        # end statistics\n",
       "        tock = time.time()\n",
       "        print('Elapsed time : {}'.format(tock-tick))\n",
       "\n",
       "        print('Inserting to dataframe')\n",
       "\n",
       "        # after fetching statistics value for each dataset, insert to pandas dataframe\n",
       "        # loop over statistics data array\n",
       "        for i in range(len(statistics)):\n",
       "            statHeader = statisticsHeader[i]\n",
       "            # loop over dataset inside statistics data array\n",
       "            for j in range(len(dataset)):\n",
       "                datasetHeader = dataset[j]\n",
       "\n",
       "                # constructing header name\n",
       "                header = '{}_{}_{}'.format(fname, datasetHeader, statHeader)\n",
       "\n",
       "                # append to existing dataframe\n",
       "                dtDF[header] = statistics[i][j]\n",
       "                \n",
       "    return dtDF\n",
       "\n",
       "# FUNCTIONS #\n",
       "\n",
       "def cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath, predData=False):\n",
       "    '''\n",
       "    Crop image data based on defined crop bound in horizontal (x) and vertical (y) direction,\n",
       "    and append the cropped data to nd numpy array with format : (m datas, datatypes, imgdim1, imgdim2, number of channels)\n",
       "    \n",
       "    Parameters :\n",
       "    imgCropX -- list of start and end bound of horizontal slice index image numpy array\n",
       "    imgCropY -- list of start and end bound of horizontal slice index image numpy array\n",
       "    adte -- list of available date in datetime object\n",
       "    usedDatas -- list of want-to-crop data\n",
       "    imgPath -- complete image path with string format placeholder relative from current working directory\n",
       "    datef -- main date formatted to inserted into placeholder in imgPath\n",
       "    dateh -- optional date format for prediction data\n",
       "    \n",
       "    Returns :\n",
       "    croppedData -- numpy array of cropped data with format : (m datas, datatypes, imgdim1, imgdim2, number of channels)\n",
       "    '''\n",
       "    # loop conditional\n",
       "    firstColumn=True\n",
       "    i=0\n",
       "    for date in adte:\n",
       "        # loop conditional\n",
       "        firstRow=True\n",
       "        for data in usedDatas:\n",
       "            if predData:\n",
       "                datef = date.strftime('%Y%m%d')\n",
       "                dateh = date.strftime('%H')\n",
       "            else:\n",
       "                datef = date.strftime('%Y%m%d%H%M')\n",
       "                dateh = None\n",
       "\n",
       "            imgPathF=imgPath.format(data, data, datef, dateh)\n",
       "            # fetching image data\n",
       "            #print(imgPath)\n",
       "            image=mpimg.imread(imgPathF)\n",
       "            # cropping image to defined dimension(s)\n",
       "            image=image[imgCropX[0]:imgCropX[1], imgCropY[0]:imgCropY[1]]\n",
       "            \n",
       "            image=image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
       "            \n",
       "            # check for first loop \n",
       "            if firstRow:\n",
       "                sameDate=np.copy(image)\n",
       "                firstRow=False\n",
       "            else:\n",
       "                sameDate=np.vstack((sameDate, image))\n",
       "        \n",
       "        # reshaping numpy array\n",
       "        sameDate=sameDate.reshape(1, sameDate.shape[0], sameDate.shape[1], sameDate.shape[2], sameDate.shape[3])\n",
       "        \n",
       "        # check for first loop\n",
       "        if firstColumn:\n",
       "            croppedData=np.copy(sameDate)\n",
       "            firstColumn=False\n",
       "        else:\n",
       "            croppedData=np.vstack((croppedData, sameDate))\n",
       "        if i%100 == 0:\n",
       "            print(croppedData.shape)\n",
       "        i+=1\n",
       "            \n",
       "    return croppedData \n",
       "    \n",
       "\n",
       "def cropImagePredictionData(dim, usedDatas=['rain','cloud','psf','qvapor','sst'], hourOffset=0, wlstation='manggarai'):\n",
       "    '''\n",
       "    Returning numpy array with dimension of (m training data, nodes), that nodes = (rain, cloud, psf, qvapor, sst) cropped data\n",
       "    based on defined dimensions : 100 (10x10), 196 (14x14), 400 (20x20)\n",
       "    '''\n",
       "    if dim == 72:\n",
       "        imgCropX=[324, 336] # 12x6\n",
       "        imgCropY=[234, 240] # 12x6\n",
       "    elif dim == 100:\n",
       "        imgCropX=[323,333] # 10x10\n",
       "        imgCropY=[233,243] # 10x10\n",
       "    elif dim == 196:\n",
       "        imgCropX=[320,334] # 14x14\n",
       "        imgCropY=[230,244] # 14x14\n",
       "    elif dim == 240:\n",
       "        imgCropX=[318, 338] # 20x12\n",
       "        imgCropY=[231, 243] # 20x12  \n",
       "    #elif dim == 400:\n",
       "        #imgCropX=[317,337] # 20x20\n",
       "        #imgCropY=[227,247] # 20x20\n",
       "    elif dim == 400:\n",
       "        imgCropX=[318, 338] # 20x20v2 shifted down 1 cell\n",
       "        imgCropY=[227, 247] # 20x20v2 shifted down 1 cell\n",
       "    # Katulampa crop extent\n",
       "    elif dim == 16: # Katulampa 4x4 input cell\n",
       "        imgCropX=[332, 336]\n",
       "        imgCropY=[236, 240]\n",
       "    elif dim == 784: # Katulampa 28x28 input cell\n",
       "        imgCropX=[320, 348]\n",
       "        imgCropY=[224, 252]\n",
       "    \n",
       "    adte, adta = getAvailableSlicedData(dataScope='prediction', hourOffset=hourOffset, wlstation=wlstation)\n",
       "    imgPath = '../mining_sadewa/sadewa/{}/{}_{}_{}.png'\n",
       "\n",
       "    return cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath, predData=True)\n",
       "\n",
       "def cropImageObservationData(dim, usedDatas=['IR1','IR3','B04','VIS','CCLD']):\n",
       "    '''\n",
       "    Returning 3 dimensions numpy array with (m training data, nodes), that nodes = (IR1, IR3, B04, VIS, CCLD) cropped data\n",
       "    based on defined dimensions : 100 (10x10), 196 (14x14), 400 (20x20)\n",
       "    '''\n",
       "    if dim == 45:\n",
       "        imgCropX=[932,941]\n",
       "        imgCropY=[517,522]\n",
       "    elif dim == 100:\n",
       "        imgCropX=[910,920]\n",
       "        imgCropY=[405,415]\n",
       "    elif dim == 196:\n",
       "        imgCropX=[908,922]\n",
       "        imgCropY=[403,417]\n",
       "    elif dim == 198: #v2\n",
       "        imgCropX=[928,942]\n",
       "        imgCropY=[512,526]\n",
       "    elif dim == 400:\n",
       "        imgCropX=[905,925]\n",
       "        imgCropY=[400,420]\n",
       "       \n",
       "    adte, adta = getAvailableSlicedData()\n",
       "    imgPath = '../mining_sadewa/sadewa/{}/H89_{}_{}.png'\n",
       "\n",
       "    return cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath)\n",
       "\n",
       "def performIndividualCropPredictionData(dim):\n",
       "    '''\n",
       "    Perform individual database creation of prediction data\n",
       "    '''\n",
       "    # initializing individual variables\n",
       "    usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "    for usedData in usedDatas:\n",
       "        \n",
       "        crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "        with h5py.File('dataset/{}{}.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "            f.create_dataset('datas', data=crop)\n",
       "    \n",
       "\n",
       "def dataLabel(entryBound, endBound, dim=100):\n",
       "    '''\n",
       "    Returning ndarray of input and label by specifying entry and end bound of available data\n",
       "    '''\n",
       "    dateBound=adte[entryBound:endBound]\n",
       "    labels=np.array(adta[entryBound:endBound]).astype('int16')\n",
       "    labels=labels.reshape(1, labels.shape[0])\n",
       "\n",
       "    if dim == 100:\n",
       "        imgCropX=[910,920]\n",
       "        imgCropY=[405,415]\n",
       "    elif dim == 196:\n",
       "        imgCropX=[908,922]\n",
       "        imgCropY=[403,417]\n",
       "    elif dim == 400:\n",
       "        imgCropX=[905,925]\n",
       "        imgCropY=[400,420]\n",
       "\n",
       "    # only used IR1 (top cloud temp), IR3 (water vapor), and CCLD (cloud growth)\n",
       "    usedDatas=['IR1','IR3','CCLD']\n",
       "\n",
       "    totalNodes=(imgCropX[1]-imgCropX[0])*(imgCropY[1]-imgCropY[0])*len(usedDatas)\n",
       "    totalTrainingData=endBound-entryBound\n",
       "\n",
       "    first=True\n",
       "    for date in dateBound:\n",
       "        reshaped=np.array([])\n",
       "        for data in usedDatas:\n",
       "            datef=date.strftime('%Y%m%d%H%M')\n",
       "            imgPath='../mining_sadewa/sadewa/{}/H89_{}_{}.png'.format(data,data, datef)\n",
       "            image=color.rgb2gray(color.rgba2rgb(mpimg.imread(imgPath)))\n",
       "            #image=color.rgba2rgb(mpimg.imread(imgPath))\n",
       "            image=image[imgCropX[0]:imgCropX[1], imgCropY[0]:imgCropY[1]]\n",
       "            # crop image\n",
       "            reshapedP=image.reshape(image.shape[0]*image.shape[1])\n",
       "            reshaped=np.append(reshaped, reshapedP)\n",
       "            #plt.imshow(image, cmap='gray')\n",
       "        # transpose image\n",
       "        transposed=reshaped.reshape(reshaped.shape[0],1)\n",
       "\n",
       "        if first:\n",
       "            trainingData=np.copy(transposed)\n",
       "            first=False\n",
       "        else:\n",
       "            trainingData=np.hstack((trainingData,transposed))\n",
       "            \n",
       "    return trainingData, labels\n",
       "\n",
       "def storeDataLabelHDF5(filename, datas, labels):\n",
       "    '''\n",
       "    Store data to HDF5 format to prevent prefetching from scratch\n",
       "    '''\n",
       "    with h5py.File(filename, 'w') as f:\n",
       "        f.create_dataset('datas', data=datas)\n",
       "        f.create_dataset('labels', data=labels)\n",
       "        \n",
       "def readDataLabelHDF5(filename):\n",
       "    '''\n",
       "    Read stored data -- and -- label data in HDF5 format, back to numpy array\n",
       "    '''\n",
       "    with h5py.File(filename, 'r') as f:\n",
       "        data=f['datas'][()]\n",
       "        labels=f['labels'][()]\n",
       "    return data, labels\n",
       "\n",
       "def sigmoid(z):\n",
       "    '''\n",
       "    Compute the sigmoid of z\n",
       "    \n",
       "    Arguments :\n",
       "    z -- A scalar or numpy array of any size\n",
       "    \n",
       "    Return :\n",
       "    s -- sigmoid(z)\n",
       "    '''\n",
       "    s=1/(1+np.exp(-z))\n",
       "    return s\n",
       "\n",
       "def initialize_with_zeros(dim):\n",
       "    '''\n",
       "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0\n",
       "    \n",
       "    Argument :\n",
       "    dim -- size of the w vector we want (or number of parameters in this case)\n",
       "    \n",
       "    Returns :\n",
       "    w -- initialized vector of shape (dim, 1)\n",
       "    b -- initialized scalar (corresponds to the bias)\n",
       "    '''\n",
       "    \n",
       "    w = np.zeros((dim, 1))\n",
       "    b = 0\n",
       "    \n",
       "    return w,b\n",
       "\n",
       "def propagate(w, b, X, Y):\n",
       "    '''\n",
       "    Implement the cost function and it's gradient fot the propagation\n",
       "    \n",
       "    Arguments :\n",
       "    w -- weights, a numpy array of size (num_px*num_px*num_channels, 1)\n",
       "    b -- bias, a scalar\n",
       "    X -- data of size (num_px*num_px*num_channels, number of examples)\n",
       "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
       "    \n",
       "    Return :\n",
       "    cost -- negative log-likelihood cost for logistic regression\n",
       "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
       "    db -- gradient of the loss with respect to b, thus same shape as b\n",
       "    '''\n",
       "    # number of training examples\n",
       "    m = X.shape[1]\n",
       "    \n",
       "    # forward propagation\n",
       "    # w.T to make sure the shapes is aligned to create dot product (a,b) dot (b,c) dimensions\n",
       "    # shape of A is (1, m train ex)\n",
       "    # computing activation\n",
       "    A = sigmoid(np.dot(w.T, X)+b)\n",
       "    \n",
       "    # sum over m training examples\n",
       "    # compute cost\n",
       "    cost = -1/m*(np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)))\n",
       "    cost = np.squeeze(cost)\n",
       "    \n",
       "    # backward propagation\n",
       "    # be careful for the placement of X and transpose over substraction of A with Y\n",
       "    # because the required dimension(s) are (nodes, m) dot (m, 1) -> (nodes, 1)\n",
       "    dw = 1/m*(np.dot(X, ((A-Y).T)))\n",
       "    # sum over m training examples after substraction\n",
       "    db = 1/m*(np.sum(A-Y))\n",
       "    \n",
       "    grads = {'dw':dw, # dw shapes : (nodex, 1)\n",
       "             'db':db} # db is a float number, not a matrix\n",
       "    return grads, cost\n",
       "\n",
       "def optimize(w,b,X,Y, num_iterations, learning_rate, print_cost=True):\n",
       "    '''\n",
       "    This function optimizes w and b by running a gradient descent algorithm\n",
       "    \n",
       "    Arguments :\n",
       "    w -- weights, a numpy array of size(num_px*num_px*num_channel,1)\n",
       "    b -- bias, a scalar\n",
       "    X -- data of shape (num_px*num_px*num_channel, number of examples)\n",
       "    Y -- label vector of shape (1, number of examples)\n",
       "    num_iterations -- number of iterations of the optimization loop\n",
       "    learning_rate -- learning rate of the gradient descent update rule\n",
       "    print_cost -- True to print the loss every 100 steps\n",
       "    \n",
       "    Returns :\n",
       "    params -- dictionary containing the weights w and bias b\n",
       "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
       "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
       "    '''\n",
       "    costs = []\n",
       "    \n",
       "    for i in range(num_iterations):\n",
       "        # cost and gradient calculation\n",
       "        grads, cost = propagate(w,b,X,Y)\n",
       "        \n",
       "        # retrieve derivatives from grads\n",
       "        dw = grads['dw']\n",
       "        db = grads['db']\n",
       "        \n",
       "        # update rule\n",
       "        w = w - learning_rate*dw\n",
       "        b = b - learning_rate*db\n",
       "        \n",
       "        # record the costs\n",
       "        if i % 100 == 0:\n",
       "            costs.append(cost)\n",
       "            \n",
       "        # print the cost every 100 training iterations\n",
       "        if print_cost and i % 100 == 0:\n",
       "            print('Cost after iteration {} : {}'.format(i, cost))\n",
       "    \n",
       "    params = {'w':w,\n",
       "              'b':b}\n",
       "    \n",
       "    grads = {'dw':dw,\n",
       "             'db':db}\n",
       "    \n",
       "    return params, grads, costs\n",
       "\n",
       "def predict(w, b, X):\n",
       "    '''\n",
       "    Predict wether the label using learned logistic regression parameters (w, b)\n",
       "    \n",
       "    Arguments:\n",
       "    w -- weights, a numpy array of size (num_pc*num_px*num_channel, 1)\n",
       "    b -- bias, a scalar\n",
       "    X -- data of size (num_px*num_px*num_channel, number of examples)\n",
       "    \n",
       "    Returns :\n",
       "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
       "    '''\n",
       "    m = X.shape[1]\n",
       "    #Y_prediction = np.zeros((1,m))\n",
       "    w = w.reshape(X.shape[0],1)\n",
       "    \n",
       "    # compute vector 'A' predicting y_hat value\n",
       "    A = sigmoid(np.dot(w.T, X) + b)\n",
       "    \n",
       "    return A\n",
       "\n",
       "def executeModel(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=True):\n",
       "    '''\n",
       "    Builds the logistic regression model by calling component functions\n",
       "    \n",
       "    Arguments :\n",
       "    X_train -- training set represented by a numpy array of shape (num_px*num_px*num_channel, m_train)\n",
       "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
       "    X_test -- test set represented by a numpy array of shape (num_px*num_px*num_channel, m_test)\n",
       "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
       "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
       "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
       "    print_cost -- set to true to print the cost every 100 iterations\n",
       "    \n",
       "    Returns :\n",
       "    d -- dictionary containing information about the model\n",
       "    '''\n",
       "    # initialize parameters with zeros\n",
       "    w, b = initialize_with_zeros(X_train.shape[0])\n",
       "    \n",
       "    # gradient descent\n",
       "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
       "    \n",
       "    # retrieve parameters w and b from dictionary 'parameters'\n",
       "    w = parameters['w']\n",
       "    b = parameters['b']\n",
       "    \n",
       "    # predict test/train set examples\n",
       "    Y_prediction_test = predict(w, b, X_test)\n",
       "    Y_prediction_train = predict(w, b, X_train)\n",
       "    \n",
       "    # print train/test Errors\n",
       "    #trainAcc = (100 - np.mean(np.abs(Y_prediction_train - Y_train))) * 100\n",
       "    #testAcc = (100 - np.mean(np.abs(Y_prediction_test - Y_test))) * 100\n",
       "    trainAcc = (1-np.mean(np.abs(Y_prediction_train - Y_train)))*100\n",
       "    testAcc = (1-np.mean(np.abs(Y_prediction_test - Y_test)))*100\n",
       "    print('Train accuracy : {} %'.format(trainAcc))\n",
       "    print('Test accuracy : {} %'.format(testAcc))\n",
       "    \n",
       "    d = {'costs':costs,\n",
       "         'Y_prediction_test':Y_prediction_test,\n",
       "         'Y_prediction_train':Y_prediction_train,\n",
       "         'w':w,\n",
       "         'b':b,\n",
       "         'learning_rate':learning_rate,\n",
       "         'num_iterations':num_iterations}\n",
       "    \n",
       "    return d\n",
       "\n",
       "def coordinatesTable(sRight, sLeft, sBottom, sTop, resW, resH):\n",
       "    '''\n",
       "    Returning coordinates table according to defined parameters\n",
       "    \n",
       "    Parameters :\n",
       "    sRight -- most right bound of the coordinates in decimal degrees\n",
       "    sLeft -- most left bound of the coordinates in decimal degrees\n",
       "    sBottom -- most bottom bound of the coordinates in decimal degrees\n",
       "    sTop -- most top bound of the coordinates in decimal degrees\n",
       "    resW -- image resolution in vertical direction (in pixels)\n",
       "    resH -- image resolution in horizontal direction (in pixels)\n",
       "    \n",
       "    Returns :\n",
       "    coordX -- 1d numpy array of entry coordinates for each pixel in result image in X coordinates\n",
       "    coordY -- 1d numpy array of entry coordinates for each pixel in result image in Y coordinates\n",
       "    '''\n",
       "    # maximum value for latitude/longitude coordinates\n",
       "    maxWidth=180\n",
       "    maxHeight=90\n",
       "\n",
       "    # calculating width and height in decimal degrees\n",
       "    if sLeft >= 0 & sRight < 0:\n",
       "        sWidth=(maxWidth-sLeft)+(maxWidth+sRight)\n",
       "    elif sLeft >= 0 & sRight >=0:\n",
       "        sWidth = sRight - sLeft\n",
       "    else:\n",
       "        raise Exception(\"Condition haven't been defined. Please define first\")\n",
       "    sHeight=sTop-sBottom\n",
       "    print(sWidth, sHeight)\n",
       "   \n",
       "    # initialize pixel index in coordinates (x,y)\n",
       "    coordX = np.zeros(resW)\n",
       "    coordY = np.zeros(resH)\n",
       "    \n",
       "    # calculating X and Y coordinates for each picture pixel\n",
       "    for y in range(len(coordY)):\n",
       "        yCoord=sTop-y/len(coordY)*sHeight\n",
       "        coordY[y]=yCoord\n",
       "    for x in range(len(coordX)):\n",
       "        xCoord=sLeft+x/len(coordX)*sWidth\n",
       "        # check for timezone pass \n",
       "        if not(xCoord < 180):\n",
       "            xCoord=-180+(xCoord-180)\n",
       "        coordX[x]=xCoord\n",
       "        \n",
       "    return coordX, coordY\n",
       "\n",
       "def coordinatesPredictionTable():\n",
       "    '''\n",
       "    Creating prediction data image coordinates for each pixel at entry point\n",
       "    \n",
       "    The (0,0) entry point is located on the top left corner of the image\n",
       "    '''\n",
       "    # sadewa prediction data bound\n",
       "    sRight = 145\n",
       "    sLeft = 95\n",
       "    sBottom = -10\n",
       "    sTop = 10\n",
       "    \n",
       "    # sadewa image data resolution\n",
       "    resW = 1000\n",
       "    resH = 400\n",
       "        \n",
       "    coordX, coordY = coordinatesTable(sRight, sLeft, sBottom, sTop, resW, resH)\n",
       "        \n",
       "    return coordX, coordY\n",
       "\n",
       "def coordinatesObservationTable():\n",
       "    '''\n",
       "    Creating observation data image coordinates for each pixel at entry point\n",
       "    \n",
       "    The (0,0) entry point is located on the top left corner of the image\n",
       "    \n",
       "    Returns :\n",
       "    coordX -- 1d numpy array of entry coordinates for each pixel in result image in X coordinates\n",
       "    coordY -- 1d numpy array of entry coordinates for each pixel in result image in Y coordinates\n",
       "    '''\n",
       "    # sadewa observation data bound\n",
       "    sRight=-150\n",
       "    sLeft=70\n",
       "    sBottom=-60\n",
       "    sTop=60\n",
       "\n",
       "    # sadewa image data resolution\n",
       "    resW=1565\n",
       "    resH=1686\n",
       "        \n",
       "    coordX, coordY = coordinatesTable(sRight, sLeft, sBottom, sTop, resW, resH)\n",
       "        \n",
       "    return coordX, coordY\n",
       "\n",
       "def crop(coordX, coordY, right=107.2, left=106.5, bottom=-6.7, top=-6.2):\n",
       "    '''\n",
       "    Crop Sadewa IR1, IR2, VIS, CCLD, B04 data\n",
       "    with right,left,bottom, and top coordinates bound (in deg)\n",
       "    \n",
       "    Paramters :\n",
       "    *default value will crop the image to 10x10 pixels\n",
       "    coordX -- coordinates table in horizontal direction for each pixel\n",
       "    coordY -- coordinates table in vertical direction for each pixel\n",
       "    \n",
       "    Returns :\n",
       "    resx -- A numpy array containing index (or pixel) in horizontal direction of cropped image\n",
       "    resy -- A numpy array containing index (or pixel) in vertical direction of cropped image\n",
       "    '''\n",
       "    # creating Boolean list to crop image\n",
       "    yEntryTruthValues = coordY < top\n",
       "    yEndTruthValues = coordY > bottom\n",
       "    xEntryTruthValues = coordX > left\n",
       "    xEndTruthValues = coordX < right\n",
       "\n",
       "    # merging boolean list to get truth table\n",
       "    xTruthValues = xEntryTruthValues & xEndTruthValues\n",
       "    yTruthValues = yEntryTruthValues & yEndTruthValues\n",
       "\n",
       "    # get index of picture where the truth value is true\n",
       "    resx = np.where(xTruthValues == True)\n",
       "    resy = np.where(yTruthValues == True)\n",
       "\n",
       "    return resx, resy\n",
       "\n",
       "# converting dataset \n",
       "# observation data only\n",
       "def prepareObservation(obs, grayscale=False):\n",
       "    # loop through all available data\n",
       "    firstData = True\n",
       "    for i in range(len(obs)):\n",
       "        # loop through dataset\n",
       "        firstDataset = True\n",
       "        for j in range(len(obs[i])):\n",
       "            if j == 2 or j == 3:\n",
       "                continue\n",
       "            else :\n",
       "                # check if grayscale or not\n",
       "                if grayscale:\n",
       "                    img = color.rgb2gray(color.rgba2rgb(obs[i][j]))\n",
       "                    flat = img.reshape(obs[i][j].shape[0]*obs[i][j].shape[1])\n",
       "                else:\n",
       "                    img = obs[i][j]\n",
       "                    flat = img.reshape(obs[i][j].shape[0]*obs[i][j].shape[1]*obs[i][j].shape[2])\n",
       "                \n",
       "                if firstDataset:\n",
       "                    flattened = flat.copy()\n",
       "                    firstDataset = False\n",
       "                else :\n",
       "                    flattened = np.hstack((flattened, flat))\n",
       "        if firstData:\n",
       "            data = flattened.copy()\n",
       "            data = data.reshape(1, data.shape[0])\n",
       "            firstData = False\n",
       "        else :\n",
       "            flattened = flattened.reshape(1, flattened.shape[0])\n",
       "            data = np.vstack((data, flattened))\n",
       "    return data\n",
       "\n",
       "# prediction data only\n",
       "def preparePrediction(pred, grayscale=False):\n",
       "    # loop through all available data\n",
       "    firstData = True\n",
       "    for i in range(len(pred)):\n",
       "        # loop through dataset\n",
       "        firstDataset = True\n",
       "        for j in range(len(pred[i])):\n",
       "            if False:\n",
       "                continue\n",
       "            else :\n",
       "                # check if grayscale or not\n",
       "                if grayscale:\n",
       "                    img = color.rgb2gray(color.rgba2rgb(pred[i][j]))\n",
       "                    flat = img.reshape(pred[i][j].shape[0]*pred[i][j].shape[1])\n",
       "                else:\n",
       "                    img = pred[i][j]\n",
       "                    flat = pred[i][j].reshape(pred[i][j].shape[0]*pred[i][j].shape[1]*pred[i][j].shape[2])\n",
       "                \n",
       "                \n",
       "                if firstDataset:\n",
       "                    flattened = flat.copy()\n",
       "                    firstDataset = False\n",
       "                else :\n",
       "                    flattened = np.hstack((flattened, flat))\n",
       "        if firstData:\n",
       "            data = flattened.copy()\n",
       "            data = data.reshape(1, data.shape[0])\n",
       "            firstData = False\n",
       "        else :\n",
       "            flattened = flattened.reshape(1, flattened.shape[0])\n",
       "            data = np.vstack((data, flattened))\n",
       "    return data\n",
       "\n",
       "# observation and prediction data\n",
       "def prepareCombination(obs, pred, grayscale=False):\n",
       "    # loop through all available data\n",
       "    firstData = True\n",
       "    for i in range(len(pred)):\n",
       "        # loop through dataset\n",
       "        firstDataset = True\n",
       "        for j in range(len(pred[i])):\n",
       "            # check if grayscale or not\n",
       "            if grayscale:\n",
       "                img = color.rgb2gray(color.rgba2rgb(pred[i][j]))\n",
       "                flatP = img.reshape(pred[i][j].shape[0]*pred[i][j].shape[1])\n",
       "            else:\n",
       "                img = pred[i][j]\n",
       "                flatP = img.reshape(pred[i][j].shape[0]*pred[i][j].shape[1]*pred[i][j].shape[2])\n",
       "            \n",
       "            obsCheck = j == 2 or j == 3\n",
       "            if not obsCheck:\n",
       "                # check if grayscale or not\n",
       "                if grayscale:\n",
       "                    img = color.rgb2gray(color.rgba2rgb(obs[i][j]))\n",
       "                    flatO = img.reshape(obs[i][j].shape[0]*obs[i][j].shape[1])\n",
       "                else:\n",
       "                    img = obs[i][j]\n",
       "                    flatO = img.reshape(obs[i][j].shape[0]*obs[i][j].shape[1]*obs[i][j].shape[2])\n",
       "            \n",
       "            if firstDataset:\n",
       "                flattened = flatP.copy()\n",
       "                if not obsCheck:\n",
       "                    flattened = np.hstack((flattened, flatO))\n",
       "                firstDataset = False\n",
       "            else :\n",
       "                flattened = np.hstack((flattened, flatP))\n",
       "                if not obsCheck:\n",
       "                    flattened = np.hstack((flattened, flatO))\n",
       "                \n",
       "        if firstData:\n",
       "            data = flattened.copy()\n",
       "            data = data.reshape(1, data.shape[0])\n",
       "            firstData = False\n",
       "        else :\n",
       "            flattened = flattened.reshape(1, flattened.shape[0])\n",
       "            data = np.vstack((data, flattened))\n",
       "    return data\n",
       "\n",
       "# Normalizing input data\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "    \n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    \n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "    \n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "    \n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "    \n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "\n",
       "def splitTrainTestSequential(data, label, startBound=None, endBound=None, split=0.8):\n",
       "    return splitTrainTest(data, label, startBound, endBound, split)\n",
       "\n",
       "# resize function for wind data\n",
       "\n",
       "def correctingWindData():\n",
       "    dataset = ('winu', 'wn10', 'wind')\n",
       "    paths = {}\n",
       "    for data in dataset:\n",
       "        paths[data] = os.listdir(folderPath.format(data))\n",
       "\n",
       "    for path in paths:\n",
       "        print('Processing {} data'.format(path))\n",
       "        for filename in paths[path]:\n",
       "            # check if readable\n",
       "            if filename in readError:\n",
       "                # use previous data\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), prevImg)\n",
       "            # check if in correct dimension\n",
       "            elif filename in nonStdDim:\n",
       "                # use previous data\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), prevImg)\n",
       "            else:\n",
       "                img = mpimg.imread('../mining_sadewa/sadewa/{}/{}'.format(path, filename))\n",
       "\n",
       "                # resize image to correct dimension(s)\n",
       "                resized = skimage.transform.resize(img, (400,1000))\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), resized)\n",
       "\n",
       "                prevImg = copy.deepcopy(resized)\n",
       "\n",
       "def checkDataError(datasetList, stdDimension):\n",
       "    '''\n",
       "    Check for read and dimension error in dataset\n",
       "    Input datasetList : array like list of data\n",
       "    stdDimension : a tuple containing standard dimension (and color channel(s)) of image\n",
       "    Returning 2 list : readError and nonStdDim\n",
       "    '''\n",
       "    paths = {}\n",
       "    for data in dataset:\n",
       "        paths[data] = os.listdir(folderPath.format(data))\n",
       "\n",
       "    # read test\n",
       "    readError = []\n",
       "    nonStdDim = []\n",
       "    for path in paths:\n",
       "        for filename in paths[path]:\n",
       "            try :\n",
       "                img = mpimg.imread('../mining_sadewa/sadewa/{}/{}'.format(path, filename))\n",
       "                if img.shape != stdDimension:\n",
       "                    print('Non standard dimensions : {}'.format(filename))\n",
       "                    nonStdDim.append(filename)\n",
       "            except Exception:\n",
       "                print('Error occured : {}'.format(filename))\n",
       "                readError.append(filename)\n",
       "\n",
       "    return readError, nonStdDim\n",
       "\n",
       "def preparingSimulationData(usedDatas, hourOffsets=(0,), dimension=72, wlstation='manggarai'):\n",
       "    '''\n",
       "    Input :\n",
       "    -- usedDatas : array like array\n",
       "    -- hourOffsets : array like integer for costumizing manggarai date input data\n",
       "    -- dimension : input data dimension (default : 72)\n",
       "    -- !split : split slice between train/allavailabledata\n",
       "    -- !shuffle : wether or not the x->y data pairs randomly shuffled or just sequence\n",
       "    -- !randomSeed : random batch identification\n",
       "    \n",
       "    Returning dictionary of :\n",
       "    -- fname : dataset name\n",
       "    -- adta : available sliced input data between manggarai WL and sadewa\n",
       "    -- adte : available sliced input date between manggarai WL and sadewa\n",
       "    -- norm : normalized manggarai WL data\n",
       "    -- minStat : minimum value of manggarai WL data\n",
       "    -- maxStat : maximum value of manggarai WL data\n",
       "    -- dataset : raw input data\n",
       "    -- flattened : flattened raw input data\n",
       "    -- traintest : (trainData, trainLabel), (testData, testLabel) tuple\n",
       "    '''\n",
       "\n",
       "    himawariData={}\n",
       "    for hourOffset in hourOffsets:\n",
       "        adte, adta = getAvailableSlicedData(dataScope='prediction', hourOffset=hourOffset, wlstation=wlstation)\n",
       "        adta = np.array(adta).astype('float32')\n",
       "        # normalizing input data\n",
       "        norm, minStat, maxStat = normalizingLabels(adta)\n",
       "        for usedData in usedDatas:\n",
       "            # load data\n",
       "            inputCombination = len(usedData)\n",
       "            if inputCombination == 1:\n",
       "                dictKey = '{}{}-{}'.format(usedData[0], dimension, hourOffset)\n",
       "                fname = 'dataset/{}.hdf5'.format(dictKey)\n",
       "            elif inputCombination == 2:\n",
       "                dictKey = '{}{}{}-{}'.format(usedData[0], usedData[1], dimension, hourOffset)\n",
       "                fname = 'dataset/{}.hdf5'.format(dictKey)\n",
       "            elif inputCombination == 3:\n",
       "                dictKey = '{}{}{}{}-{}'.format(usedData[0], usedData[1], usedData[2], dimension, hourOffset)\n",
       "                fname = 'dataset/{}.hdf5'.format(dictKey)\n",
       "            elif inputCombination == 4:\n",
       "                dictKey = '{}{}{}{}{}-{}'.format(usedData[0], usedData[1], usedData[2], usedData[3],dimension, hourOffset)\n",
       "                fname = 'dataset/{}.hdf5'.format(dictKey)\n",
       "                \n",
       "                \n",
       "\n",
       "            with h5py.File(fname, 'r') as f:\n",
       "                data = f['datas'][()]\n",
       "\n",
       "            flattened = preparePrediction(data, grayscale=True)\n",
       "            \n",
       "            himawariData[dictKey]={'fname':'{}'.format(dictKey),\n",
       "                                    'hourOffset':hourOffset,\n",
       "                                    'adta':adta,\n",
       "                                    'adte':adte,\n",
       "                                    'norm':norm,\n",
       "                                    'minStat':minStat,\n",
       "                                    'maxStat':maxStat,\n",
       "                                    'dataset':data,\n",
       "                                    'flattened':flattened,\n",
       "                                    'traintest': splitTrainTest(flattened, norm, split=0.7, shuffle=True, randomSeed=10)}\n",
       "    return himawariData\n",
       "\n",
       "def generateRNNInput(adte, adta, recurrentCount=1):\n",
       "    '''\n",
       "    Check and return a tuple of date containing available data for recurrent configuration\n",
       "    \n",
       "    This is a sub-function to restack current cropped data into rnn enabled data based on recurrentCount number\n",
       "    \n",
       "    Return:\n",
       "    recurrentIndexList = [(index-2, index-1, index+0), (index-1, index+0, index+1), (index-recurrentCount+index, index-recurrentCount+1+index, index-recurrentCount+2+index), ...]\n",
       "    availableRecurrentDate = array like containing available date in recurrent configuration (in t=0)\n",
       "    availableRecurrentLabel = array like containing available data label in recurrent configuration\n",
       "    '''\n",
       "    \n",
       "    # defining start index\n",
       "    # defining list to store the recurrent index\n",
       "    recurrentIndexList = []\n",
       "    availableRecurrentDate = []\n",
       "    availableRecurrentLabel = []\n",
       "    for idx in range(len(adte[recurrentCount:])):\n",
       "        # check sequence\n",
       "        checkSeq = [adte[idx+recurrentCount]+datetime.timedelta(hours=-recurrentCount)+datetime.timedelta(hours=x) for x in range(recurrentCount+1)]\n",
       "        realSeq = [adte[idx+x] for x in range(recurrentCount+1)]\n",
       "        if checkSeq != realSeq:\n",
       "            continue\n",
       "        else:\n",
       "            recurrentIndexList.append([idx+x for x in range(recurrentCount+1)])\n",
       "            availableRecurrentDate.append(adte[idx+recurrentCount])\n",
       "            availableRecurrentLabel.append(adta[idx+recurrentCount])\n",
       "    \n",
       "    return recurrentIndexList, availableRecurrentDate, availableRecurrentLabel\n",
       "\n",
       "\n",
       "def restackRNNInput(recurrentIndexList, dataset, flattened=False, grayscale=True):\n",
       "    '''\n",
       "    Create a new datasets in rnn mode by passing recurrentIndexList and dataset that want to be restacked\n",
       "    \n",
       "    Input:\n",
       "    flattened : False(default)/True\n",
       "    \n",
       "    Output :\n",
       "    restacked dataset (flattened / not flattened)\n",
       "    '''\n",
       "    firstData = True\n",
       "    for sequences in recurrentIndexList:\n",
       "        first = True\n",
       "        for sequence in sequences:\n",
       "            if first:\n",
       "                stacked = copy.deepcopy(dataset[sequence])\n",
       "                first = False\n",
       "            else:\n",
       "                stacked = np.vstack((stacked, dataset[sequence]))\n",
       "        # reshape stacked data\n",
       "        stacked = stacked.reshape(1, stacked.shape[0], stacked.shape[1], stacked.shape[2], stacked.shape[3])\n",
       "        if firstData:\n",
       "            allStacked = copy.deepcopy(stacked)\n",
       "            firstData = False\n",
       "        else:\n",
       "            allStacked = np.vstack((allStacked, stacked))\n",
       "    \n",
       "    if flattened:\n",
       "        print(allStacked.shape)\n",
       "        return preparePrediction(allStacked, grayscale=grayscale)\n",
       "    else:\n",
       "        return allStacked\n",
       "\n",
       "    \n",
       "def performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True):\n",
       "    '''\n",
       "    Performing RNN Data Creation by passing data combination that want to be recreated as RNN sequence and list of number that acting as\n",
       "    how much sequence that want to be added before the t+0 data. For ex if the recurrentLists[0] says 2, it means that there will be 3 stacked data,\n",
       "    t-2, t-1, t+0.\n",
       "    \n",
       "    This function can process from 1 to 6 data combination(s)\n",
       "    \n",
       "    Input:\n",
       "    -usedDatas : array like of array of data combination(s) (up to 6) in sequence with dims\n",
       "    -dims : array like of dimensions, in squence with usedDatas\n",
       "    -recurrentLists : array like of lists of number that acting as how much sequence that want to be added before the t+0 data (>=1)\n",
       "    \n",
       "    '''\n",
       "\n",
       "    adte, adta = getAvailableSlicedData(maxData=True, hourOffset=0, dataScope=dataScope, wlstation=wlstation)\n",
       "    recurrentIndexLists=[]\n",
       "    for recurrentList in recurrentLists:\n",
       "        recurrentIndexList, availableRecurrentDate, availableRecurrentLabel = generateRNNInput(adte, adta, recurrentCount=recurrentList)\n",
       "        recurrentIndexLists.append(recurrentIndexList)\n",
       "\n",
       "    for j in range(len(usedDatas)):\n",
       "        usedData = usedDatas[j]\n",
       "        dim = dims[j]\n",
       "        # define the length of data\n",
       "        dataLength = len(usedData)\n",
       "        # read stored data\n",
       "        if dataLength == 1:\n",
       "            fileName = '{}{}'.format(usedData[0], dim)\n",
       "        elif dataLength == 2:\n",
       "            fileName = '{}{}{}'.format(usedData[0], usedData[1], dim)\n",
       "        elif dataLength == 3:\n",
       "            fileName = '{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], dim)\n",
       "        elif dataLength == 4:\n",
       "            fileName = '{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim)\n",
       "        elif dataLength == 5:\n",
       "            fileName = '{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], dim)\n",
       "        elif dataLength == 6:\n",
       "            fileName = '{}{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], dim)\n",
       "        print(fileName)\n",
       "        fpath = 'dataset/manggaraiRNN/{}.hdf5'.format(fileName)\n",
       "        with h5py.File(fpath,'r') as f:\n",
       "            data = f['datas'][()]\n",
       "            \n",
       "        for i in range(len(recurrentLists)):\n",
       "            print('{}-{}-{}'.format(fileName, dim, recurrentLists[i]))\n",
       "            # restacking the data\n",
       "            allStacked = restackRNNInput(recurrentIndexLists[i], data, flattened=flattened)\n",
       "            \n",
       "            # save restacked data to file\n",
       "            with h5py.File('dataset/manggaraiRNN/{}r{}f.hdf5'.format(fileName, recurrentLists[i]), 'w') as f:\n",
       "                f.create_dataset('datas', data=allStacked)\n",
       "\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "# Data Visualization\n",
       "\n",
       "## Sadewa Visual Preview\n",
       "\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import matplotlib.image as mpimg\n",
       "from matplotlib import gridspec\n",
       "import datetime\n",
       "from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox\n",
       "\n",
       "figsizeMultiplication = 1.5\n",
       "ROW = 1 # number of datas\n",
       "COLUMN = 13\n",
       "fig, ax = plt.subplots(figsize=(COLUMN*figsizeMultiplication,ROW*figsizeMultiplication), dpi=200)\n",
       "\n",
       "ax.set_xlim(0,1)\n",
       "ax.set_ylim(0,1)\n",
       "\n",
       "cropExtents = (([905,925],[400,420]),([317,337],[227,247])) #20x20 respectively\n",
       "usedDatas = (('CCLD','B04','IR1','IR3','VIS'),('cloud','psf','qvapor','rain','sst','wind','winu','wn10'))\n",
       "predictionPath = '../mining_sadewa/sadewa/{}/{}_{}_{}.png'\n",
       "observationPath = '../mining_sadewa/sadewa/{}/H89_{}_{}.png'\n",
       "\n",
       "startDate = datetime.datetime(2020,2,1,0,0)\n",
       "dateLists = [startDate+datetime.timedelta(hours=x) for x in range(ROW)]\n",
       "\n",
       "paths = (observationPath, predictionPath)\n",
       "\n",
       "for j in range(COLUMN):\n",
       "    # due to different params to fetch prediction and observation, we need to do this\n",
       "    if j >= len(usedDatas[0]):\n",
       "        pos = 1\n",
       "        roll = j - len(usedDatas[0])\n",
       "    else:\n",
       "        pos = 0\n",
       "        roll = j\n",
       "    for i in range(ROW):\n",
       "        \n",
       "        dateformats = ((dateLists[i].strftime('%Y%m%d%H%M'),None),(dateLists[i].strftime('%Y%m%d'),dateLists[i].strftime('%H')))\n",
       "        formatPath = paths[pos].format(usedDatas[pos][roll], usedDatas[pos][roll], dateformats[pos][0], dateformats[pos][1])\n",
       "        img = mpimg.imread(formatPath)\n",
       "\n",
       "        img = img[cropExtents[pos][0][0]:cropExtents[pos][0][1], cropExtents[pos][1][0]:cropExtents[pos][1][1]]\n",
       "\n",
       "        imagebox = OffsetImage(img, zoom=2.5*figsizeMultiplication)\n",
       "        ab = AnnotationBbox(imagebox, (j/COLUMN+0.5/COLUMN,i/ROW+0.5/ROW), pad=0.1)\n",
       "        ax.add_artist(ab)\n",
       "\n",
       "#plt.axis('off')\n",
       "ax.tick_params(which='both', width=0)\n",
       "\n",
       "sides = ('left','top','right','bottom')\n",
       "for side in sides:\n",
       "    vis = ax.spines[side]\n",
       "    vis.set_visible(False)\n",
       "        \n",
       "plt.yticks([])\n",
       "plt.xticks(ticks=np.arange(0.5/COLUMN,1,1/COLUMN), labels=('CCLD','B04','IR1','IR3','VIS','cloud','psf','qvapor','rain','sst','wind','winu','wn10'))\n",
       "plt.xlabel('Data type') \n",
       "plt.draw()\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "\n",
       "![png](output_5_0.png)\n",
       "\n",
       "\n",
       "## 4px\\*4px & 28px\\*28px Situation Map Input Data Extent \n",
       "\n",
       "\n",
       "```python\n",
       "import cartopy.crs as ccrs\n",
       "import cartopy\n",
       "import matplotlib.pyplot as plt\n",
       "from cartopy.io import shapereader\n",
       "import cartopy.io.img_tiles as cimgt\n",
       "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
       "from cartopy.io.shapereader import Reader\n",
       "from cartopy.feature import ShapelyFeature\n",
       "import os\n",
       "import matplotlib.patches as mpatches\n",
       "from cartopy import config\n",
       "import cartopy.feature as cfeature\n",
       "import matplotlib.gridspec as gridspec\n",
       "import datetime\n",
       "import numpy as np\n",
       "import matplotlib\n",
       "\n",
       "'''\n",
       "zorder\n",
       "4 : basemap\n",
       "5 : districts\n",
       "9 : sadewadata\n",
       "9.5 : rectangle\n",
       "10 : watershed\n",
       "11 : river\n",
       "12 : wlstation \n",
       "300 : annotation\n",
       "1000 : coordinate ticks\n",
       "'''\n",
       "# defining extent and basemap style\n",
       "#extent = [106.296091,107.300000,-7.170509,-6.000000]\n",
       "#extent = [106.296091,107.6,-7.02,-5.8]\n",
       "extent = [106.1,108.35,-7.5,-5.7]\n",
       "basemap = cimgt.Stamen()\n",
       "\n",
       "# define feature to be added\n",
       "title = 'Manggarai WL Input Data Extent'\n",
       "districts = './maptest/feature/Indo_Kab_Kot.shp'\n",
       "watershed = './maptest/feature/katulampaWatershed.shp'\n",
       "river = './maptest/feature/katulampaRiverNetwork.shp'\n",
       "wlstations = ((106.837150,-6.633528),)\n",
       "wlstation_titles = ('Katulampa WMS',)\n",
       "sadewa_rain = '../mining_sadewa/sadewa/rain/rain_20191231_13.png'\n",
       "rectangles = ((106.8,-6.8,0.2,0.2),(106.2,-7.4,1.4,1.4))\n",
       "rectangle_titles = ('4px*4px Data Extent','28px*28px Data Extent')\n",
       "rectangle_colors = ('lawngreen','royalblue')\n",
       "annotation_gap = [0.125,-0.06]\n",
       "annotation_fontsize = 15\n",
       "\n",
       "DEFTRANSFORM = ccrs.PlateCarree()\n",
       "UTM48 = ccrs.UTM(zone=48, southern_hemisphere=True)\n",
       "\n",
       "plt.figure(figsize=(7.5,7.5), dpi=200)\n",
       "ax = plt.axes(projection=DEFTRANSFORM)\n",
       "\n",
       "# drawing basemap\n",
       "ax.add_image(basemap, 10, zorder=4)\n",
       "\n",
       "# drawing districts boundary\n",
       "districs_feature = ShapelyFeature(Reader(districts).geometries(),\n",
       "                               DEFTRANSFORM, facecolor='none')\n",
       "ax.add_feature(districs_feature, zorder=5)\n",
       "\n",
       "# drawing sadewa data\n",
       "img_extent = (95, 145, -10, 10)\n",
       "img = plt.imread(sadewa_rain)\n",
       "ax.imshow(img, origin='upper', extent=img_extent, transform=DEFTRANSFORM, zorder=9, alpha=0.8)\n",
       "\n",
       "# drawing watershed boundary\n",
       "watershed_feature = ShapelyFeature(Reader(watershed).geometries(),\n",
       "                               UTM48, edgecolor='red', facecolor='none')\n",
       "ax.add_feature(watershed_feature, linewidth=1.5, zorder=10)\n",
       "\n",
       "# drawing ciliwung river\n",
       "river_feature = ShapelyFeature(Reader(river).geometries(),\n",
       "                               UTM48, edgecolor='blue', facecolor='green')\n",
       "ax.add_feature(river_feature, linewidth=1, zorder=11)\n",
       "\n",
       "# drawing water level station and the labels\n",
       "ax.scatter([x[0] for x in wlstations], [y[1] for y in wlstations], transform=DEFTRANSFORM, zorder=12, edgecolor='darkorange', facecolor='darkorange', s=75)\n",
       "for i, wlstationcoord in enumerate(wlstations):\n",
       "    ax.annotate(wlstation_titles[i], (wlstationcoord[0]+0.075, wlstationcoord[1]+0.075), \n",
       "                fontsize=annotation_fontsize-7, color='white', weight='bold', backgroundcolor='darkorange',\n",
       "                transform=DEFTRANSFORM, zorder=300)\n",
       "    \n",
       "# drawing a rectangle as extent boundary\n",
       "for i, rectangle in enumerate(rectangles):\n",
       "    # For line \n",
       "    ax.add_patch(mpatches.Rectangle(xy=(rectangle[0],rectangle[1]), width=rectangle[2],\n",
       "                                    height=rectangle[3], transform=DEFTRANSFORM,\n",
       "                                    edgecolor=rectangle_colors[i], facecolor='none',\n",
       "                                    linewidth=4, linestyle='dashed', zorder=9.5+i*0.1))\n",
       "    # For facecolor\n",
       "    ax.add_patch(mpatches.Rectangle(xy=(rectangle[0],rectangle[1]), width=rectangle[2],\n",
       "                                    height=rectangle[3], transform=DEFTRANSFORM,\n",
       "                                    facecolor=rectangle_colors[i], alpha=0.275, zorder=9.5+i*0.1))\n",
       "    \n",
       "    ax.annotate(rectangle_titles[i], \n",
       "                (rectangle[0], rectangle[1]+rectangle[3]+annotation_gap[i]), \n",
       "                fontsize=annotation_fontsize, color=rectangle_colors[i], backgroundcolor='white',\n",
       "                transform=DEFTRANSFORM, weight='bold', zorder=300)\n",
       "\n",
       "# drawing extent coordinates\n",
       "gl = ax.gridlines(crs=DEFTRANSFORM, draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--', zorder=1000)\n",
       "gl.xformatter = LONGITUDE_FORMATTER\n",
       "gl.yformatter = LATITUDE_FORMATTER\n",
       "    \n",
       "ax.set_extent(extent)\n",
       "#plt.title(title)\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "    C:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\ipykernel_launcher.py:111: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
       "    C:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\cartopy\\mpl\\geoaxes.py:388: MatplotlibDeprecationWarning: \n",
       "    The 'inframe' parameter of draw() was deprecated in Matplotlib 3.3 and will be removed two minor releases later. Use Axes.redraw_in_frame() instead. If any parameter follows 'inframe', they should be passed as keyword, not positionally.\n",
       "      inframe=inframe)\n",
       "    \n",
       "\n",
       "\n",
       "![png](output_7_1.png)\n",
       "\n",
       "\n",
       "## Mini Map Study Area Extent\n",
       "\n",
       "\n",
       "```python\n",
       "import cartopy.crs as ccrs\n",
       "import cartopy\n",
       "import matplotlib.pyplot as plt\n",
       "from cartopy.io import shapereader\n",
       "import cartopy.io.img_tiles as cimgt\n",
       "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
       "from cartopy.io.shapereader import Reader\n",
       "from cartopy.feature import ShapelyFeature\n",
       "import os\n",
       "import matplotlib.patches as mpatches\n",
       "from cartopy import config\n",
       "import cartopy.feature as cfeature\n",
       "import matplotlib.gridspec as gridspec\n",
       "import datetime\n",
       "import numpy as np\n",
       "import matplotlib\n",
       "\n",
       "'''\n",
       "zorder\n",
       "4 : basemap\n",
       "5 : districts\n",
       "9 : sadewadata\n",
       "9.5 : rectangle\n",
       "10 : watershed\n",
       "11 : river\n",
       "12 : wlstation \n",
       "300 : annotation\n",
       "1000 : coordinate ticks\n",
       "'''\n",
       "# defining extent and basemap style\n",
       "#extent = [106.296091,107.300000,-7.170509,-6.000000]\n",
       "#extent = [106.296091,107.6,-7.02,-5.8]\n",
       "extent = [104.5,110,-8,-5.2]\n",
       "#basemap = cimgt.Stamen()\n",
       "\n",
       "# define feature to be added\n",
       "title = 'Katulampa WL Input Data Extent'\n",
       "districts = './maptest/feature/Indo_Kab_Kot.shp'\n",
       "watershed = './maptest/feature/katulampaWatershed.shp'\n",
       "river = './maptest/feature/katulampaRiverNetwork.shp'\n",
       "wlstations = ((106.837150,-6.633528),)\n",
       "wlstation_titles = ('Katulampa WMS',)\n",
       "sadewa_rain = '../mining_sadewa/sadewa/rain/rain_20191231_13.png'\n",
       "rectangles = ((106.2,-7.4,1.4,1.4),)\n",
       "rectangle_titles = ('Study area extent',)\n",
       "rectangle_colors = ('tab:orange',)\n",
       "annotation_gap = [0.04,-0.06,0.07]\n",
       "annotation_fontsize = 10\n",
       "\n",
       "DEFTRANSFORM = ccrs.PlateCarree()\n",
       "UTM48 = ccrs.UTM(zone=48, southern_hemisphere=True)\n",
       "\n",
       "plt.figure(figsize=(3,3), dpi=200)\n",
       "ax = plt.axes(projection=DEFTRANSFORM)\n",
       "\n",
       "# drawing basemap\n",
       "#ax.add_image(basemap, 10, zorder=4)\n",
       "\n",
       "# drawing districts boundary\n",
       "districs_feature = ShapelyFeature(Reader(districts).geometries(),\n",
       "                               DEFTRANSFORM, facecolor='none')\n",
       "ax.add_feature(districs_feature, zorder=5)\n",
       "\n",
       "# drawing sadewa data\n",
       "img_extent = (95, 145, -10, 10)\n",
       "img = plt.imread(sadewa_rain)\n",
       "ax.imshow(img, origin='upper', extent=img_extent, transform=DEFTRANSFORM, zorder=9, alpha=0.8)\n",
       "\n",
       "# drawing watershed boundary\n",
       "watershed_feature = ShapelyFeature(Reader(watershed).geometries(),\n",
       "                               UTM48, edgecolor='red', facecolor='none')\n",
       "ax.add_feature(watershed_feature, linewidth=3, zorder=10)\n",
       "\n",
       "    \n",
       "# drawing a rectangle as extent boundary\n",
       "for i, rectangle in enumerate(rectangles):\n",
       "    # For line \n",
       "    ax.add_patch(mpatches.Rectangle(xy=(rectangle[0],rectangle[1]), width=rectangle[2],\n",
       "                                    height=rectangle[3], transform=DEFTRANSFORM,\n",
       "                                    edgecolor=rectangle_colors[i], facecolor=rectangle_colors[i], alpha=0.6,\n",
       "                                    linewidth=3, linestyle='dashed', zorder=9.5+i*0.1))\n",
       "        \n",
       "    ax.annotate(rectangle_titles[i], \n",
       "                (rectangle[0], rectangle[1]+rectangle[3]+annotation_gap[i]), \n",
       "                fontsize=annotation_fontsize, color=rectangle_colors[i], backgroundcolor='white',\n",
       "                transform=DEFTRANSFORM, weight='bold', zorder=300)\n",
       "\n",
       "# drawing extent coordinates\n",
       "gl = ax.gridlines(crs=DEFTRANSFORM, draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--', zorder=1000)\n",
       "gl.xformatter = LONGITUDE_FORMATTER\n",
       "gl.yformatter = LATITUDE_FORMATTER   \n",
       "ax.set_extent(extent)\n",
       "plt.show()\n",
       "\n",
       "```\n",
       "\n",
       "    C:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
       "    \n",
       "\n",
       "\n",
       "![png](output_9_1.png)\n",
       "\n",
       "\n",
       "## 4px\\*4px, 8px\\*8px, 16px\\*16px, and 28px\\*28px LSTM RNN Spatial Optimization\n",
       "\n",
       "\n",
       "```python\n",
       "import cartopy.crs as ccrs\n",
       "import cartopy\n",
       "import matplotlib.pyplot as plt\n",
       "from cartopy.io import shapereader\n",
       "import cartopy.io.img_tiles as cimgt\n",
       "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
       "from cartopy.io.shapereader import Reader\n",
       "from cartopy.feature import ShapelyFeature\n",
       "import os\n",
       "import matplotlib.patches as mpatches\n",
       "from cartopy import config\n",
       "import cartopy.feature as cfeature\n",
       "import matplotlib.gridspec as gridspec\n",
       "import datetime\n",
       "import numpy as np\n",
       "import matplotlib\n",
       "%matplotlib inline\n",
       "\n",
       "'''\n",
       "zorder\n",
       "4 : basemap\n",
       "5 : districts\n",
       "9 : sadewadata\n",
       "9.5 : rectangle\n",
       "10 : watershed\n",
       "11 : river\n",
       "12 : wlstation \n",
       "300 : annotation\n",
       "1000 : coordinate ticks\n",
       "'''\n",
       "# defining extent and basemap style\n",
       "#extent = [106.296091,107.300000,-7.170509,-6.000000]\n",
       "#extent = [106.296091,107.6,-7.02,-5.8]\n",
       "extent = [106,107.8,-7.6,-5.8]\n",
       "basemap = cimgt.Stamen()\n",
       "\n",
       "# define feature to be added\n",
       "title = 'Manggarai WL Input Data Extent'\n",
       "districts = './maptest/feature/Indo_Kab_Kot.shp'\n",
       "watershed = './maptest/feature/katulampaWatershed.shp'\n",
       "river = './maptest/feature/katulampaRiverNetwork.shp'\n",
       "wlstations = ((106.837150,-6.633528),)\n",
       "wlstation_titles = ('Katulampa WMS',)\n",
       "sadewa_rain = '../mining_sadewa/sadewa/rain/rain_20191231_13.png'\n",
       "rectangle_titles = ('28px*28px Data Extent','16px*16px Data Extent','8px*8px Data Extent','4px*4px Data Extent')\n",
       "rectangles = ((106.2,-7.4,1.4,1.4),(106.5,-7.1,0.8,0.8),(106.7,-6.9,0.4,0.4),(106.8,-6.8,0.2,0.2)) \n",
       "rectangle_colors = ('yellow','red','blue','lawngreen')\n",
       "annotation_gap = [0.125,-0.06]\n",
       "annotation_fontsize = 15\n",
       "\n",
       "DEFTRANSFORM = ccrs.PlateCarree()\n",
       "UTM48 = ccrs.UTM(zone=48, southern_hemisphere=True)\n",
       "\n",
       "plt.figure(figsize=(7.5,7.5), dpi=200)\n",
       "ax = plt.axes(projection=DEFTRANSFORM)\n",
       "\n",
       "# drawing basemap\n",
       "ax.add_image(basemap, 10, zorder=4)\n",
       "\n",
       "# drawing districts boundary\n",
       "districs_feature = ShapelyFeature(Reader(districts).geometries(),\n",
       "                               DEFTRANSFORM, facecolor='none')\n",
       "ax.add_feature(districs_feature, zorder=5)\n",
       "\n",
       "# drawing sadewa data\n",
       "img_extent = (95, 145, -10, 10)\n",
       "img = plt.imread(sadewa_rain)\n",
       "ax.imshow(img, origin='upper', extent=img_extent, transform=DEFTRANSFORM, zorder=9, alpha=0.8)\n",
       "\n",
       "# drawing watershed boundary\n",
       "watershed_feature = ShapelyFeature(Reader(watershed).geometries(),\n",
       "                               UTM48, edgecolor='red', facecolor='none')\n",
       "ax.add_feature(watershed_feature, linewidth=1.5, zorder=10)\n",
       "\n",
       "# drawing ciliwung river\n",
       "river_feature = ShapelyFeature(Reader(river).geometries(),\n",
       "                               UTM48, edgecolor='blue', facecolor='green')\n",
       "ax.add_feature(river_feature, linewidth=1, zorder=11)\n",
       "\n",
       "# drawing water level station and the labels\n",
       "ax.scatter([x[0] for x in wlstations], [y[1] for y in wlstations], transform=DEFTRANSFORM, zorder=12, edgecolor='darkorange', facecolor='darkorange', s=75)\n",
       " \n",
       "# drawing a rectangle as extent boundary\n",
       "for i, rectangle in enumerate(rectangles):\n",
       "    # For line \n",
       "    ax.add_patch(mpatches.Rectangle(xy=(rectangle[0],rectangle[1]), width=rectangle[2],\n",
       "                                    height=rectangle[3], transform=DEFTRANSFORM,\n",
       "                                    edgecolor=rectangle_colors[i], facecolor='none',\n",
       "                                    linewidth=4, linestyle='dashed', zorder=9.5+i*0.1))\n",
       "    # For facecolor\n",
       "    ax.add_patch(mpatches.Rectangle(xy=(rectangle[0],rectangle[1]), width=rectangle[2],\n",
       "                                    height=rectangle[3], transform=DEFTRANSFORM,\n",
       "                                    facecolor=rectangle_colors[i], alpha=0.275, zorder=9.5+i*0.1))\n",
       "\n",
       "# drawing extent coordinates\n",
       "gl = ax.gridlines(crs=DEFTRANSFORM, draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--', zorder=1000)\n",
       "gl.xformatter = LONGITUDE_FORMATTER\n",
       "gl.yformatter = LATITUDE_FORMATTER\n",
       "    \n",
       "ax.set_extent(extent)\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "    C:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\cartopy\\mpl\\geoaxes.py:388: MatplotlibDeprecationWarning: \n",
       "    The 'inframe' parameter of draw() was deprecated in Matplotlib 3.3 and will be removed two minor releases later. Use Axes.redraw_in_frame() instead. If any parameter follows 'inframe', they should be passed as keyword, not positionally.\n",
       "      inframe=inframe)\n",
       "    \n",
       "\n",
       "\n",
       "![png](output_11_1.png)\n",
       "\n",
       "\n",
       "## Katulampa Rain Series\n",
       "\n",
       "\n",
       "```python\n",
       "import cartopy.crs as ccrs\n",
       "import cartopy\n",
       "import matplotlib.pyplot as plt\n",
       "from cartopy.io import shapereader\n",
       "import cartopy.io.img_tiles as cimgt\n",
       "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
       "from cartopy.io.shapereader import Reader\n",
       "from cartopy.feature import ShapelyFeature\n",
       "import os\n",
       "import matplotlib.patches as mpatches\n",
       "from cartopy import config\n",
       "import cartopy.feature as cfeature\n",
       "import matplotlib.gridspec as gridspec\n",
       "import datetime\n",
       "import numpy as np\n",
       "import matplotlib\n",
       "\n",
       "# previous-current-future weather data (rain) extent : [106.7,107,6.12,6.8]\n",
       "'''\n",
       "zorder\n",
       "4 : basemap\n",
       "5 : districts\n",
       "9 : sadewadata\n",
       "9.5 : rectangle\n",
       "10 : watershed\n",
       "11 : river\n",
       "12 : wlstation \n",
       "300 : annotation\n",
       "1000 : coordinate ticks\n",
       "'''\n",
       "# defining extent and basemap style\n",
       "#extent = [106.296091,107.300000,-7.170509,-6.000000]\n",
       "#extent = [106.296091,107.6,-7.02,-5.8]\n",
       "extent = (106.8,107,-6.8,-6.6)\n",
       "basemap = cimgt.Stamen()\n",
       "\n",
       "# define feature to be added\n",
       "title = 'Katulampa WL Input Data Extent'\n",
       "districts = './maptest/feature/Indo_Kab_Kot.shp'\n",
       "watershed = './maptest/feature/katulampaWatershed.shp'\n",
       "river = './maptest/feature/katulampaRiverNetwork.shp'\n",
       "wlstations = ((106.837150,-6.633528),)\n",
       "sadewa_rain = '../mining_sadewa/sadewa/rain/rain_20191231_{}.png'\n",
       "\n",
       "rainsequences = ('09','10','11','12','13','14','15','16','17','18','19','20')\n",
       "\n",
       "COLUMN = 6\n",
       "assert len(rainsequences) % COLUMN == 0 #to fit map square baseline\n",
       "ROW = len(rainsequences) // COLUMN\n",
       "\n",
       "DEFTRANSFORM = ccrs.PlateCarree()\n",
       "UTM48 = ccrs.UTM(zone=48, southern_hemisphere=True)\n",
       "\n",
       "# subplots initialization\n",
       "figsizeMultiplication = 1.4\n",
       "fig = plt.figure(figsize=(COLUMN*figsizeMultiplication,2*ROW*figsizeMultiplication), dpi=125)\n",
       "gs = fig.add_gridspec(ROW,COLUMN)\n",
       "\n",
       "for i in range(ROW):\n",
       "    for j in range(COLUMN):\n",
       "        count = i*COLUMN + j\n",
       "        ax = fig.add_subplot(gs[i,j], projection=DEFTRANSFORM)\n",
       "\n",
       "        # drawing basemap\n",
       "        ax.add_image(basemap, 10, zorder=4)\n",
       "\n",
       "        # drawing sadewa data\n",
       "        img_extent = (95, 145, -10, 10)\n",
       "        img = plt.imread(sadewa_rain.format(rainsequences[count]))\n",
       "        ax.imshow(img, origin='upper', extent=img_extent, transform=DEFTRANSFORM, zorder=9, alpha=0.8)\n",
       "\n",
       "        # drawing watershed boundary\n",
       "        watershed_feature = ShapelyFeature(Reader(watershed).geometries(),\n",
       "                                       UTM48, edgecolor='red', facecolor='none')\n",
       "        ax.add_feature(watershed_feature, linewidth=1, zorder=10)\n",
       "\n",
       "        # drawing ciliwung river\n",
       "        river_feature = ShapelyFeature(Reader(river).geometries(),\n",
       "                                       UTM48, edgecolor='blue', facecolor='green')\n",
       "        ax.add_feature(river_feature, linewidth=0.6, zorder=11)\n",
       "\n",
       "        # drawing water level station and the labels\n",
       "        ax.scatter([x[0] for x in wlstations], [y[1] for y in wlstations], transform=DEFTRANSFORM, zorder=12, edgecolor='darkorange', facecolor='darkorange', s=25)\n",
       "\n",
       "        # crop to specified extent\n",
       "        ax.set_extent(extent)\n",
       "\n",
       "        # add title to subplot\n",
       "        ax.title.set_text(f't+{count}')\n",
       "\n",
       "#fig.tight_layout()\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "\n",
       "![png](output_13_0.png)\n",
       "\n",
       "\n",
       "## Katulampa WL and Sadewa Data Availability\n",
       "\n",
       "\n",
       "```python\n",
       "from matplotlib import gridspec\n",
       "\n",
       "# Initialization\n",
       "# Combined data availability\n",
       "DATASCOPE = 'prediction'\n",
       "WLSTATION = 'katulampa'\n",
       "adteM, adtaM = getAvailableSlicedData(dataScope=DATASCOPE, wlstation=WLSTATION)\n",
       "adtaM = np.array(adtaM).astype('float32')\n",
       "\n",
       "# performing complete date assignation\n",
       "currentDate = datetime.datetime(2019,1,1,0,0)\n",
       "completeDate = []\n",
       "while currentDate < adteM[len(adteM)-1]:\n",
       "    completeDate.append(currentDate)\n",
       "    currentDate = currentDate+datetime.timedelta(hours=1)\n",
       "    \n",
       "manggaraiDataStatus = []\n",
       "for i, cDate in enumerate(completeDate):\n",
       "    if cDate in adteM:\n",
       "        # find adteM index\n",
       "        manggaraiDataStatus.append(adtaM[adteM.index(cDate)])\n",
       "    else:\n",
       "        manggaraiDataStatus.append(0)\n",
       "        \n",
       "# Manggarai data availability\n",
       "dateListHourly, dataListHourly = manggaraiDataList(wlstation=WLSTATION)\n",
       "\n",
       "normalizeManggaraiData = []\n",
       "dataListHourly = np.array(dataListHourly).astype('int16')\n",
       "for i, cDate in enumerate(completeDate):\n",
       "    if cDate in dateListHourly:\n",
       "        # find adteM index\n",
       "        normalizeManggaraiData.append(dataListHourly[dateListHourly.index(cDate)])\n",
       "    else:\n",
       "        normalizeManggaraiData.append(0)\n",
       "        \n",
       "# Sadewa data availability\n",
       "himawariExtractedDate = extractHimawariDatetime()\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# In one subplots\n",
       "# Variable(s) initialization\n",
       "COLOR = ('firebrick','greenyellow') # for unavailable and available data\n",
       "\n",
       "fig = plt.figure(figsize=(12,5), dpi=150)\n",
       "spec = gridspec.GridSpec(ncols=1, nrows=3, height_ratios=[1,4,1])\n",
       "\n",
       "# Katulampa WL data\n",
       "ax1 = fig.add_subplot(spec[0])\n",
       "plt.setp(ax1.get_xticklabels(), visible=False)\n",
       "ax1.plot(completeDate, normalizeManggaraiData, zorder=10, color='black')\n",
       "# show span color to show available and unavailable data\n",
       "for i, mds in enumerate(normalizeManggaraiData):\n",
       "    nextIndex = i if i == len(normalizeManggaraiData)-1 else i+1\n",
       "    if mds == 0:\n",
       "        ax1.axvspan(completeDate[i], completeDate[nextIndex], facecolor=COLOR[0])\n",
       "    else:\n",
       "        ax1.axvspan(completeDate[i], completeDate[nextIndex], facecolor=COLOR[1])  \n",
       "ax1.set_ylim((0,200))\n",
       "ax1.set_xlim(completeDate[0], completeDate[len(completeDate)-1])\n",
       "ax1.set_ylabel('Water\\nLevel\\n(+m)')\n",
       "ax1.set_title('Katulampa Water Level Data Availability')\n",
       "\n",
       "# Sadewa Data\n",
       "ax2 = fig.add_subplot(spec[1], sharex=ax1)\n",
       "plt.setp(ax2.get_xticklabels(), visible=False)\n",
       "for i, hed in enumerate(himawariExtractedDate):\n",
       "    ax2.plot(completeDate, [i for x in range(len(completeDate))], color='gray')  \n",
       "for i, hed in enumerate(himawariExtractedDate):\n",
       "    for j, dateloop in enumerate(completeDate):\n",
       "        nextIndex = j if j == len(completeDate)-1 else j+1\n",
       "        if dateloop in himawariExtractedDate[hed]:\n",
       "            ax2.axvspan(xmin=completeDate[j], xmax=completeDate[nextIndex],\n",
       "                        ymin=(1/len(himawariExtractedDate))*i, ymax=(1/len(himawariExtractedDate))*(i+1), facecolor=COLOR[1])\n",
       "        else:\n",
       "            ax2.axvspan(xmin=completeDate[j], xmax=completeDate[nextIndex],\n",
       "                        ymin=(1/len(himawariExtractedDate))*i, ymax=(1/len(himawariExtractedDate))*(i+1), facecolor=COLOR[0])  \n",
       "ax2.set_yticks(ticks=np.arange(0.5,len(himawariExtractedDate)+0.5,1))\n",
       "ax2.set_yticklabels(labels=list(himawariExtractedDate.keys()))\n",
       "ax2.set_ylim(0,len(himawariExtractedDate))\n",
       "ax2.set_xlim(completeDate[0], completeDate[len(completeDate)-1])\n",
       "ax2.set_ylabel('Sadewa data')\n",
       "ax2.set_title('Sadewa Data Availability')\n",
       "\n",
       "# Combined Data\n",
       "ax3 = fig.add_subplot(spec[2], sharex=ax1)\n",
       "ax3.plot(completeDate, manggaraiDataStatus, zorder=10, color='black')\n",
       "# show span color to show available and unavailable data\n",
       "for i, mds in enumerate(manggaraiDataStatus):\n",
       "    nextIndex = i if i == len(manggaraiDataStatus)-1 else i+1\n",
       "    if mds == 0:\n",
       "        ax3.axvspan(completeDate[i], completeDate[nextIndex], facecolor=COLOR[0])\n",
       "    else:\n",
       "        ax3.axvspan(completeDate[i], completeDate[nextIndex], facecolor=COLOR[1]) \n",
       "ax3.set_ylim((0,200))\n",
       "ax3.set_xlim(completeDate[0], completeDate[len(completeDate)-1])\n",
       "ax3.set_ylabel('Water\\nLevel\\n(+m)')\n",
       "#ax3.set_yticks([])\n",
       "ax3.set_title('Combined Data Availability : {} %'.format(round(len(adteM)/len(completeDate)*100,2)))\n",
       "\n",
       "fig.tight_layout()\n",
       "plt.savefig('./figure/dataavailability.png', dpi=300)\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "\n",
       "![png](output_16_0.png)\n",
       "\n",
       "\n",
       "## Deep Neural Network Simulation Results\n",
       "\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
       "import pandas as pd\n",
       "import math\n",
       "import itertools\n",
       "\n",
       "# Katulampa scenario data fetch\n",
       "_, kt16 = extractData((48,49,50,51,52,59,60,61),{48:generateCombinations([x for x in range(1,2)]),\n",
       "                                               49:generateCombinations([x for x in range(2,3)]),\n",
       "                                               50:generateCombinations([x for x in range(3,4)]),\n",
       "                                               51:generateCombinations([x for x in range(4,5)]),\n",
       "                                               52:generateCombinations([x for x in range(5,6)]),\n",
       "                                               59:generateCombinations([x for x in range(6,7)]),\n",
       "                                               60:generateCombinations([x for x in range(7,8)]),\n",
       "                                               61:generateCombinations([x for x in range(8,9)])})\n",
       "\n",
       "_, kt784 = extractData((53,54,55,56,57,62,63,64),{53:generateCombinations([x for x in range(1,2)]),\n",
       "                                               54:generateCombinations([x for x in range(2,3)]),\n",
       "                                               55:generateCombinations([x for x in range(3,4)]),\n",
       "                                               56:generateCombinations([x for x in range(4,5)]),\n",
       "                                               57:generateCombinations([x for x in range(5,6)]),\n",
       "                                               62:generateCombinations([x for x in range(6,7)]),\n",
       "                                               63:generateCombinations([x for x in range(7,8)]),\n",
       "                                               64:generateCombinations([x for x in range(8,9)])})\n",
       "\n",
       "def splitSortResultData(dataFrame, startComb, endComb):\n",
       "    '''\n",
       "    Perform split data from single dataframe to n different list for ach combinations, sort, and return its values\n",
       "    return :\n",
       "    datas, dlabels\n",
       "    '''\n",
       "    idts = mergeNameIdentifier([x for x in range(startComb,endComb+1)])\n",
       "    datas = []\n",
       "    dlabels = []\n",
       "    for idt in idts:\n",
       "        indvData = []\n",
       "        indvDataLabel = []\n",
       "        for s in idt:\n",
       "            indvData.append(dataFrame[dataFrame.index.str.fullmatch(s)]['test_r2'].values[0])\n",
       "            indvDataLabel.append(dataFrame[dataFrame.index.str.fullmatch(s)].index[0])\n",
       "        # sort data\n",
       "        sortered = sorted(zip(indvData, indvDataLabel))\n",
       "        indvData = [x for x,y in sortered]\n",
       "        indvDataLabel = [y for x,y in sortered]\n",
       "        datas.append(indvData)\n",
       "        dlabels.append(indvDataLabel)\n",
       "    return datas, dlabels\n",
       "    \n",
       "\n",
       "STARTCOMB = 1\n",
       "ENDCOMB = 8\n",
       "labels = ('1 Data', '2 Data', '3 Data', '4 Data', '5 Data', '6 Data', '7 Data', '8 Data')\n",
       "titles = ('4x4 Extent Result', '28x28 Extent Result')\n",
       "\n",
       "combinedDataFrames = (pd.DataFrame(kt16).T, pd.DataFrame(kt784).T)\n",
       "\n",
       "#plt.figure(figsize=(5*len(titles),2))\n",
       "fig, ax = plt.subplots(1, len(titles), figsize=(5*len(titles), 2.5), sharey=True, dpi=200)\n",
       "\n",
       "for i, title in enumerate(titles):\n",
       "    datas, dlabels = splitSortResultData(combinedDataFrames[i], STARTCOMB, ENDCOMB)\n",
       "    #ax = plt.subplot(1, 3, i+1)\n",
       "    for j in range(len(datas)):\n",
       "        ax[i].plot(dlabels[j], datas[j], label=labels[j])\n",
       "    ax[i].set_title(title)\n",
       "    ax[i].set_ylabel('R^2')\n",
       "    ax[i].set_xlabel('Combination number : ')\n",
       "    ax[i].grid(axis='y')\n",
       "    ax[i].set_xticks(np.arange(1, len(combinedDataFrames[i])+1, 50))\n",
       "    ax[i].set_xticklabels(np.arange(1, len(combinedDataFrames[i])+1, 50))\n",
       "    #ax.set_yticks(np.arange(0,0.35,0.07))\n",
       "    #ax.set_yticklabels(np.arange(0,0.35,0.07))\n",
       "\n",
       "ax[i].legend(loc='center right', bbox_to_anchor=(1.4,0.5), borderaxespad=0.)\n",
       "fig.tight_layout()\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "## Timeseries Results Visualization\n",
       "\n",
       "\n",
       "```python\n",
       "DATASETPATH = './dataset/recurrent_offset/sstqvaporpsfraincloud16_R24_O0_btf.hdf5'\n",
       "LABELPATH = './dataset/prequisites/katulampa_R24_O0_availableRecurrentLabel.hdf5'\n",
       "DATEPATH = './dataset/prequisites/katulampa_R24_O0_availableRecurrentDate.csv'\n",
       "MODELPATH = './models/timeseriespreview/sstqvaporpsfraincloud_R24_O0_LSTM_80_8_2_384.h5'\n",
       "\n",
       "# load dataset\n",
       "with h5py.File(DATASETPATH, 'r') as f:\n",
       "    data = f['datas'][()]\n",
       "    \n",
       "# load label\n",
       "with h5py.File(LABELPATH, 'r') as f:\n",
       "    label = f['datas'][()]\n",
       "    \n",
       "# Load Date\n",
       "recurrent_date = pd.read_csv(DATEPATH, index_col=0)\n",
       "\n",
       "# Getting rain data\n",
       "with h5py.File('./dataset/master_rain16f.hdf5', 'r') as f:\n",
       "    rain_raw = f['datas'][()]\n",
       "\n",
       "# Get sadewa date filtered\n",
       "sadewa_date = pd.read_csv('./dataset/prequisites/katulampa_R24_O0_sadewaDateFiltered.csv', names=['sd_idx','sadewa_date'], skiprows=1, usecols=[1])\n",
       "sadewa_date['sadewa_date'] = sadewa_date['sadewa_date'].astype('datetime64')\n",
       "sadewa_date = sadewa_date.assign(rain_mean=(rain_raw.mean(axis=1)))\n",
       "    \n",
       "norm, minStat, maxStat = normalizingLabels(label)\n",
       "\n",
       "# split train-test data\n",
       "(trainData, trainLabel), (testData, testLabel) = splitTrainTest(data, norm, split=0.7, shuffle=True, randomSeed=10)\n",
       "(_, trainDate), (_, testDate) = splitTrainTest(data, np.squeeze(recurrent_date), split=0.7, shuffle=True, randomSeed=10)\n",
       "\n",
       "# load model\n",
       "model = tf.keras.models.load_model(MODELPATH)\n",
       "\n",
       "# evaluating model accuracy\n",
       "prediction_model = tf.keras.Sequential([model])\n",
       "testPredictions = prediction_model.predict(testData)\n",
       "trainPredictions = prediction_model.predict(trainData)\n",
       "\n",
       "# make predictions\n",
       "testPredictions = testPredictions*(maxStat-minStat)+minStat\n",
       "trainPredictions = trainPredictions*(maxStat-minStat)+minStat\n",
       "realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "xTest = [x+1 for x in range(len(testPredictions))]\n",
       "xTrain = [x+1 for x in range(len(trainPredictions))]\n",
       "\n",
       "# R^2\n",
       "rsquaredTest = r2_score(realTestLabel,testPredictions)\n",
       "rsquaredTrain = r2_score(realTrainLabel,trainPredictions)\n",
       "\n",
       "\n",
       "# MULTIPLE PLOTS\n",
       "test_predictions = testPredictions\n",
       "train_predictions = trainPredictions\n",
       "test_labels = realTestLabel\n",
       "train_labels = realTrainLabel\n",
       "\n",
       "# Random-Sequential visualization\n",
       "trainDF = pd.DataFrame({'date':trainDate,'predictions':np.squeeze(train_predictions),'labels':train_labels})\n",
       "trainDF['date'] = trainDF['date'].astype('datetime64')\n",
       "trainDF = trainDF.sort_values(by=['date'])\n",
       "\n",
       "testDF = pd.DataFrame({'date':testDate,'predictions':np.squeeze(test_predictions),'labels':test_labels})\n",
       "testDF['date'] = testDF['date'].astype('datetime64')\n",
       "testDF = testDF.sort_values(by=['date'])\n",
       "\n",
       "fig = plt.figure(figsize=(10,5))\n",
       "spec = gridspec.GridSpec(2,2, width_ratios=[4,1], height_ratios=[1,1])\n",
       "f1 = fig.add_subplot(spec[0,0])\n",
       "f1.set_title('[0,:]')\n",
       "f1.plot(trainDF['date'], trainDF['labels'], label='true labels', color='C0')\n",
       "f1.plot(trainDF['date'], trainDF['predictions'], label='predictions', color='C1')\n",
       "f1_ax2 = f1.twinx()\n",
       "f1_ax2.plot(sadewa_date['sadewa_date'], sadewa_date['rain_mean'], label='Sadewa `rain`', color='C2')\n",
       "f1_ax2.set_ylim([-3,1])\n",
       "f1_ax2.set_ylabel('Sadewa `rain` Magnitude')\n",
       "f1_ax2.legend(loc='center left', framealpha=0.55).set_zorder(10)\n",
       "f1.set_ylim([0,200])\n",
       "f1.set_xlabel('Date')\n",
       "f1.set_ylabel('Water level (+cm)')\n",
       "f1.set_title('Train set')\n",
       "f1.legend(loc='right', framealpha=0.55).set_zorder(100)\n",
       "\n",
       "f2 = fig.add_subplot(spec[1,0])\n",
       "f2.plot(testDF['date'], testDF['labels'], label='true labels')\n",
       "f2.plot(testDF['date'], testDF['predictions'], label='predictions')\n",
       "f2_ax2 = f2.twinx()\n",
       "f2_ax2.plot(sadewa_date['sadewa_date'], sadewa_date['rain_mean'], label='Sadewa `rain`', color='C2')\n",
       "f2_ax2.set_ylim([-3,1])\n",
       "f2_ax2.set_ylabel('Sadewa `rain` Magnitude')\n",
       "f2_ax2.legend(loc='center left', framealpha=0.55).set_zorder(10)\n",
       "f2.set_ylim([0,200])\n",
       "f2.set_xlabel('Date')\n",
       "f2.set_ylabel('Water level (+cm)')\n",
       "f2.set_title('Test set')\n",
       "f2.legend(loc='right', framealpha=0.55).set_zorder(100)\n",
       "\n",
       "f3 = fig.add_subplot(spec[0,1])\n",
       "f3.scatter(train_labels, train_predictions)\n",
       "f3.plot([min(train_labels),max(train_labels)],[min(train_labels),max(train_labels)], color='red')\n",
       "f3.set_xlabel('True labels')\n",
       "f3.set_ylabel('Predicted data')\n",
       "f3.set_title('Train R^2 : {}'.format(round(r2_score(train_labels, train_predictions),2)))\n",
       "\n",
       "f4 = fig.add_subplot(spec[1,1])\n",
       "f4.scatter(test_labels, test_predictions)\n",
       "f4.plot([min(test_labels),max(test_labels)],[min(test_labels),max(test_labels)], color='red')\n",
       "f4.set_xlabel('True labels')\n",
       "f4.set_ylabel('Predicted data')\n",
       "f4.set_title('Test R^2 : {}'.format(round(r2_score(test_labels, test_predictions),2)))\n",
       "fig.tight_layout()\n",
       "\n",
       "plt.show()\n",
       "\n",
       "filt80train = trainDF[trainDF['labels'] >= 80]\n",
       "filt80test = testDF[testDF['labels'] >= 80]\n",
       "stdev = trainDF['labels'].std()\n",
       "STDRANGE = 1\n",
       "\n",
       "train_misspredict = filt80train[((filt80train.labels - filt80train.predictions) >= stdev*STDRANGE)]\n",
       "train_falsealarm = filt80train[((filt80train.predictions - filt80train.labels) >= stdev*STDRANGE)]\n",
       "\n",
       "# Calculate ratio\n",
       "hwl_train_misspredict_ratio = len(train_misspredict) / len(filt80train)\n",
       "hwl_train_falsealarm_ratio = len(train_falsealarm) / len(filt80train)\n",
       "print(f'TRAIN at {STDRANGE} STDEV -- Misspredicted : {round(hwl_train_misspredict_ratio*100,2)}% | Falsealarm : {round(hwl_train_falsealarm_ratio*100,2)}%')\n",
       "\n",
       "test_misspredict = filt80test[((filt80test.labels - filt80test.predictions) >= stdev*STDRANGE)]\n",
       "test_falsealarm = filt80test[((filt80test.predictions - filt80test.labels) >= stdev*STDRANGE)]\n",
       "\n",
       "# Calculate ratio\n",
       "hwl_test_misspredict_ratio = len(test_misspredict) / len(filt80test)\n",
       "hwl_test_falsealarm_ratio = len(test_falsealarm) / len(filt80test)\n",
       "print(f'TEST at {STDRANGE} STDEV -- Misspredicted : {round(hwl_test_misspredict_ratio*100,2)}% | Falsealarm : {round(hwl_test_falsealarm_ratio*100,2)}%')\n",
       "```\n",
       "\n",
       "    (9459, 25, 128) (9459,)\n",
       "    (9459, 25, 128) (9459,)\n",
       "    \n",
       "\n",
       "\n",
       "![png](output_20_1.png)\n",
       "\n",
       "\n",
       "    TRAIN at 1 STDEV -- Misspredicted : 0.0% | Falsealarm : 0.0%\n",
       "    TEST at 1 STDEV -- Misspredicted : 57.69% | Falsealarm : 0.0%\n",
       "    \n",
       "\n",
       "# Data Gathering & Preprocessing\n",
       "\n",
       "## Fetching Sadewa Data\n",
       "\n",
       "\n",
       "```python\n",
       "import requests\n",
       "import datetime\n",
       "import time\n",
       "import os\n",
       "\n",
       "# defining function to download the data\n",
       "\n",
       "def executeSadewa(data, sleepTime, inputStartDate=False, startDate=None):\n",
       "    '''\n",
       "    Execute sadewa download for specific index\n",
       "    '''\n",
       "    database={\n",
       "        'IR1':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/IR1/{}/{}/{}/',\n",
       "            'fname':'H89_IR1_{}{}{}{}00.png',\n",
       "            'yearStart':'2020'\n",
       "        },\n",
       "        'IR3':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/IR3/{}/{}/{}/',\n",
       "            'fname':'H89_IR3_{}{}{}{}00.png',\n",
       "            'yearStart':'2020'\n",
       "        },\n",
       "        'VIS':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/VIS/{}/{}/{}/',\n",
       "            'fname':'H89_VIS_{}{}{}{}00.png',\n",
       "            'yearStart':'2020'\n",
       "        },\n",
       "        'B04':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/himawari_merc/B04/{}/{}/{}/',\n",
       "            'fname':'H89_B04_{}{}{}{}00.png',\n",
       "            'yearStart':'2020'\n",
       "        },\n",
       "        'CCLD':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/HIMAWARI/komposit/{}/{}/{}/',\n",
       "            'fname':'H89_CCLD_{}{}{}{}00.png',\n",
       "            'yearStart':'2020'\n",
       "        },\n",
       "        'rain':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'rain_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'cloud':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'cloud_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'psf':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'psf_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'qvapor':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'qvapor_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'sst':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'sst_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'wind':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'wind_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'winu':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'winu_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "        'wn10':{\n",
       "            'url':'https://sadewa.sains.lapan.go.id/wrf/{}/{}/{}/',\n",
       "            'fname':'wn10_{}{}{}_{}.png',\n",
       "            'yearStart':'2019'\n",
       "        },\n",
       "    }\n",
       "    \n",
       "    # set or load initial loop\n",
       "    try:\n",
       "        with open('{}.txt'.format(data), 'r') as initf:\n",
       "            initLoop=initf.read()\n",
       "    except:\n",
       "        initLoop=0\n",
       "        with open('{}.txt'.format(data), 'w') as initf:\n",
       "            initf.write(str(initLoop))\n",
       "            \n",
       "    # make Directory (if not exists)\n",
       "    try:\n",
       "        os.makedirs('./sadewa/{}'.format(data))\n",
       "    except FileExistsError:\n",
       "        pass\n",
       "    \n",
       "    today=datetime.datetime.now()\n",
       "    yearStart=database[data]['yearStart']\n",
       "    if inputStartDate :\n",
       "        startDate=datetime.datetime(*startDate)\n",
       "    else:\n",
       "        startDate=datetime.datetime(int(yearStart), 1, 1)\n",
       "    \n",
       "    # offsetting start date if the loop have been started before\n",
       "    if int(initLoop) > 0:\n",
       "        startDate=startDate+datetime.timedelta(int(initLoop))\n",
       "    dateRange=(today-startDate).days\n",
       "    \n",
       "    \n",
       "    \n",
       "    for i in range(dateRange+1):\n",
       "        dateLoop=(startDate+datetime.timedelta(i))\n",
       "        \n",
       "        # loop from 0 to 23 \n",
       "        hours=['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
       "        for hour in hours:\n",
       "            # building URL\n",
       "            burl=database[data]['url']\n",
       "            fname=database[data]['fname']\n",
       "            furl='{}{}'.format(burl, fname)\n",
       "            url=furl.format(dateLoop.strftime('%Y'), dateLoop.strftime('%m'), dateLoop.strftime('%d'),\n",
       "                           dateLoop.strftime('%Y'), dateLoop.strftime('%m'), dateLoop.strftime('%d'), hour)\n",
       "            print('{}-{} Current URL : {}'.format(int(initLoop)+1+i, hour, url))\n",
       "            \n",
       "            # try to fetch data from the server \n",
       "            try:\n",
       "                response=requests.get(url)\n",
       "            \n",
       "            # except error occured during fetching, continue to next iteration after error logging to file\n",
       "            except:\n",
       "                # save error log to file for future trying\n",
       "                with open('sadewaerr.txt', 'a') as errlog:\n",
       "                    errlog.write(url)\n",
       "                    \n",
       "                # sleep for 10 seconds\n",
       "                time.sleep(10)\n",
       "                \n",
       "                # continue to next iteration\n",
       "                continue\n",
       "                \n",
       "                \n",
       "            # if server responded but bringing >=400 status code, continue to next iteration after error logging to file\n",
       "            if not response.ok:\n",
       "                # save error log to file for future trying\n",
       "                with open('sadewaerr.txt', 'a') as errlog:\n",
       "                    errlog.write(url)\n",
       "                    \n",
       "                # sleep for 10 seconds\n",
       "                time.sleep(10)\n",
       "                \n",
       "                # continue to next iteration\n",
       "                continue\n",
       "            else:\n",
       "                # save response to file\n",
       "                formatfname=fname.format(dateLoop.strftime('%Y'), dateLoop.strftime('%m'), dateLoop.strftime('%d'), hour)\n",
       "                with open('./sadewa/{}/{}'.format(data, formatfname), 'wb') as fsave:\n",
       "                    fsave.write(response.content)\n",
       "                    \n",
       "            # perform sleep between loop\n",
       "            time.sleep(sleepTime)\n",
       "\n",
       "        # update current loop to file\n",
       "        with open('{}.txt'.format(data), 'w') as initf:\n",
       "            initf.write(str(i+int(initLoop)))\n",
       "\n",
       "        \n",
       "        # perform sleep between loop\n",
       "        time.sleep(sleepTime)\n",
       "\n",
       "\n",
       "# Execute Sadewa Data\n",
       "executeSadewa(input('Enter Data type :'), int(input('How much time reserved for sleep?')), inputStartDate=False, startDate=None)\n",
       "```\n",
       "\n",
       "## Fetching Water Level Data\n",
       "\n",
       "\n",
       "```python\n",
       "import requests\n",
       "import datetime\n",
       "import bs4\n",
       "import time\n",
       "import sqlite3\n",
       "from sqlite3 import Error\n",
       "\n",
       "def dbDsda():\n",
       "    return r\"dsda.db\"\n",
       "\n",
       "\n",
       "def create_connection(db_file):\n",
       "    '''\n",
       "    create a database connection to a SQLite database\n",
       "    specified by db_file\n",
       "    :param db_file : database file\n",
       "    :return: Connection Object or None\n",
       "    '''\n",
       "    conn=None\n",
       "    try:\n",
       "        conn=sqlite3.connect(db_file)\n",
       "        return conn\n",
       "    except Error as e:\n",
       "        print(e)\n",
       "        \n",
       "def create_table(conn, create_table_sql):\n",
       "    '''\n",
       "    create a table from the create_table_sql statement\n",
       "    :param conn: Connection object\n",
       "    :param create_table_sql: a CREATE TABLE statement\n",
       "    :return:\n",
       "    '''\n",
       "    try:\n",
       "        c=conn.cursor()\n",
       "        c.execute(create_table_sql)\n",
       "    except Error as e:\n",
       "        print(e)\n",
       "        \n",
       "def execute_create_dsdadb(conn, stationName):\n",
       "    sql_create_table=\"\"\"CREATE TABLE IF NOT EXISTS {}(currentdate text UNIQUE, data text)\"\"\".format(stationName)\n",
       "    \n",
       "    #create tables\n",
       "    if conn is not None:\n",
       "        # create projects table\n",
       "        create_table(conn, sql_create_table)\n",
       "        print(\"Success\")\n",
       "    else:\n",
       "        print(\"Error! cannot create the database conenction.\")\n",
       "        \n",
       "def insert_dsdadb(conn, table, datevalues, datavalues):\n",
       "    '''\n",
       "    Insert new datevalues to dsda table\n",
       "    :param conn:\n",
       "    :param datevalues: text in sqlite datetime format\n",
       "    '''\n",
       "    sql='''insert into {}(currentdate, data) values(\"{}\",\"{}\")'''.format(table,datevalues,datavalues)\n",
       "    cur=conn.cursor()\n",
       "    try:\n",
       "        cur.execute(sql,)\n",
       "        conn.commit()\n",
       "        return \"Data insert success\"\n",
       "    except Error:\n",
       "        return \"Data insert Error - Already inserted\"\n",
       "        \n",
       "\n",
       "def currentStartEndDate(stationCode, yearList, totalData, currentLoop):\n",
       "    '''\n",
       "    Return a pair of tuple data containing start and end date for data crawling.\n",
       "    Also, by passing stationCode (unique), this function will store current loop performed on the disk\n",
       "    '''\n",
       "    \n",
       "    \n",
       "    #print(currentLoop)\n",
       "\n",
       "    # from 2000 to 2021\n",
       "    #yearList=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021']\n",
       "\n",
       "    # month : 01 - 12\n",
       "    monthList=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
       "\n",
       "    # daylist : \n",
       "    startDay='01'\n",
       "    endDayListKabisat=['31','29','31','30','31','30','31','31','30','31','30','31']\n",
       "    endDayListN=['31','28','31','30','31','30','31','31','30','31','30','31']\n",
       "\n",
       "\n",
       "    #currentLoop=0\n",
       "    #totalData=len(yearList)*len(monthList)\n",
       "    monthIndex=currentLoop%12\n",
       "    yearIndex=currentLoop//12\n",
       "    if monthIndex == 12:\n",
       "        monthIndex=0\n",
       "        yearIndex=yearIndex+1   \n",
       "\n",
       "\n",
       "    if currentLoop < totalData:\n",
       "        '''\n",
       "        Performing loop in current Year list to a 1-month-basis for crawling purpose\n",
       "        '''\n",
       "        # START DATE\n",
       "        # assign data section\n",
       "        year=yearList[yearIndex]\n",
       "        month=monthList[monthIndex]\n",
       "\n",
       "        # get day data based on month and 'kabisat' year identification\n",
       "        checkRemainder=int(year)%4\n",
       "        if checkRemainder != 0:\n",
       "            day=endDayListN[monthIndex]\n",
       "        else:\n",
       "            day=endDayListKabisat[monthIndex]\n",
       "\n",
       "        # increment section\n",
       "        if monthIndex < 11:\n",
       "            monthIndex+=1\n",
       "        else:\n",
       "            monthIndex=0\n",
       "            yearIndex+=1\n",
       "        currentLoop+=1\n",
       "        \n",
       "        # first tuple index is for identification, 0 if the loop is ongoing, 1 if we are already on the end of the loop\n",
       "        return ('0','{}'.format(currentLoop),'{}/{}/{}'.format(startDay,month,year), '{}/{}/{}'.format(day,month,year))\n",
       "        \n",
       "    else: # else we already on the end of the loop\n",
       "        return ('1','0','0','0') \n",
       "        \n",
       "def dsdaRequestData(currentStation, yearList, totalData, currentLoop):\n",
       "    '''\n",
       "    Return dictionary of data from dsda website for current loop and station\n",
       "    '''\n",
       "    check, currentLoopStatus, startDate, endDate = currentStartEndDate(currentStation, yearList, totalData, currentLoop)\n",
       "    print('{}/{} : {}-{}'.format(currentLoop+1, totalData, startDate, endDate))\n",
       "    if check=='0':\n",
       "        url='http://poskobanjirdsda.jakarta.go.id/Pages/GenerateDataTinggiAir.aspx?IdPintuAir={}&StartDate={}&EndDate={}'.format(currentStation,startDate,endDate)\n",
       "        # test URL down below (worked!)\n",
       "        #url='http://poskobanjirdsda.jakarta.go.id/Pages/GenerateDataTinggiAir.aspx?IdPintuAir=122&StartDate=02/03/2021&EndDate=03/03/2021'\n",
       "        \n",
       "        try:\n",
       "            response=requests.get(url)\n",
       "\n",
       "                \n",
       "            # start processing response content\n",
       "            # beautify text got from the response content\n",
       "            soup = bs4.BeautifulSoup(response.content)\n",
       "            textOnly=soup.text\n",
       "            # split with \"|\"\n",
       "            twoKindData=textOnly.split(\"|\")\n",
       "            mainData=twoKindData[0]\n",
       "            # split again with ';'\n",
       "            mainDataLists=mainData.split(';')\n",
       "            # delete last item in list, because contained empty data\n",
       "            mainDataLists.pop(len(mainDataLists)-1)\n",
       "            dataDictionary={}\n",
       "            for data in mainDataLists:\n",
       "                # last split with ','\n",
       "                dateText, dataText=data.split(',')\n",
       "                dateFormatted='{}:{}'.format(dateText[:13], dateText[14:16])\n",
       "                dataDictionary[dateFormatted]=dataText\n",
       "                \n",
       "            # update current loop on file\n",
       "            with open('dcl-{}.cache'.format(currentStation),'w') as dcl:\n",
       "                dcl.write(str(currentLoopStatus))\n",
       "\n",
       "            return dataDictionary\n",
       "\n",
       "        except:\n",
       "            # update current loop on file with last failed fetch batch\n",
       "            with open('dcl-{}.cache'.format(currentStation),'w') as dcl:\n",
       "                dcl.write(str(currentLoopStatus-1))\n",
       "\n",
       "            return 0\n",
       "    \n",
       "        \n",
       "    else:\n",
       "        # update current loop on file with 0 again (meaning that we are ready start again from zero)\n",
       "        with open('dcl-{}.cache'.format(stationCode),'w') as dcl:\n",
       "            dcl.write('0')\n",
       "        \n",
       "    \n",
       "\n",
       "    \n",
       "def executeBatchPastDsdaData(currentStation, stationName, yearList):\n",
       "    '''\n",
       "    yearList=['2000','2001','2002','2003','2004','2005',\n",
       "    '2006','2007','2008','2009','2010','2011','2012','2013',\n",
       "    '2014','2015','2016','2017','2018','2019','2020','2021']\n",
       "    \n",
       "    '''\n",
       "    # check stored current loop\n",
       "    try:\n",
       "        with open('dcl-{}.cache'.format(currentStation),'r') as dcl:\n",
       "            fetchCurrentLoop=dcl.read()\n",
       "            currentLoop=int(fetchCurrentLoop)\n",
       "    except:\n",
       "        currentLoop=0\n",
       "        with open('dcl-{}.cache'.format(currentStation),'w') as dcl:\n",
       "            dcl.write(str(currentLoop))\n",
       "    \n",
       "    totalData=len(yearList)*12\n",
       "    \n",
       "    while currentLoop < totalData:\n",
       "        print('Start fetching data')\n",
       "        start_time=time.time()\n",
       "        conn=create_connection(dbDsda())        \n",
       "        datas=dsdaRequestData(currentStation, yearList, totalData, currentLoop)\n",
       "        \n",
       "        # if error occured, continue on the same batch of data after sleep\n",
       "        if datas == 0:\n",
       "            print(\"Failed to reach server. Trying again in 30 seconds\")\n",
       "            time.sleep(30)\n",
       "            continue\n",
       "        \n",
       "        # refetching current station progress\n",
       "        try:\n",
       "            with open('dcl-{}.cache'.format(currentStation),'r') as dcl:\n",
       "                fetchCurrentLoop=dcl.read()\n",
       "                currentLoop=int(fetchCurrentLoop)\n",
       "        except:\n",
       "            currentLoop=0\n",
       "            with open('dcl-{}.cache'.format(currentStation),'w') as dcl:\n",
       "                dcl.write(str(currentLoop))\n",
       "        \n",
       "        \n",
       "        \n",
       "        execute_create_dsdadb(conn, stationName)\n",
       "        for data in datas:\n",
       "            insertData=insert_dsdadb(conn, stationName, data, datas[data])\n",
       "            \n",
       "        # performing sleep for every fetch loop to prevent abusive behaviour\n",
       "        finish_time=time.time()\n",
       "        print('Finish data processing in {} secs, sleep for 10s'.format(finish_time-start_time))\n",
       "        time.sleep(10)\n",
       "\n",
       "\n",
       "crawledStation={'114':'krukut', #2014\n",
       "               '144':'cideng', #2015\n",
       "               '143':'karet', #2015\n",
       "               '140':'marinaancol', #2015\n",
       "               '117':'pasarikan', #2014\n",
       "               '141':'pluit', #2015\n",
       "               '108':'pesanggrahan', #2013\n",
       "               '106':'angkehulu',\n",
       "               '135':'sunterhulu',\n",
       "               '126':'pulogadung',\n",
       "               '142':'yossudarso1',\n",
       "               '103':'cipinanghulu',\n",
       "               '145':'kalijodo',\n",
       "               '148':'istiqlal',\n",
       "               '147':'jembatanmerah',\n",
       "               '146':'flusingancol',\n",
       "               '149':'hek'}\n",
       "\n",
       "\n",
       "yearList=['2013','2014','2015','2016','2017','2018','2019','2020','2021']\n",
       "for station in crawledStation:\n",
       "    print('Crawling {} started'.format(crawledStation[station]))\n",
       "    executeBatchPastDsdaData(station, crawledStation[station], yearList)\n",
       "    print('Finish crawling {}!'.format(crawledStation[station]))\n",
       "    \n",
       "```\n",
       "\n",
       "## Sadewa General Error and Non Standard Dimension Handling\n",
       "\n",
       "\n",
       "```python\n",
       "# resize function for wind data\n",
       "def correctingWindData():\n",
       "    dataset = ('winu', 'wn10', 'wind')\n",
       "    paths = {}\n",
       "    for data in dataset:\n",
       "        paths[data] = os.listdir(folderPath.format(data))\n",
       "\n",
       "    for path in paths:\n",
       "        print('Processing {} data'.format(path))\n",
       "        for filename in paths[path]:\n",
       "            # check if readable\n",
       "            if filename in readError:\n",
       "                # use previous data\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), prevImg)\n",
       "            # check if in correct dimension\n",
       "            elif filename in nonStdDim:\n",
       "                # use previous data\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), prevImg)\n",
       "            else:\n",
       "                img = mpimg.imread('../mining_sadewa/sadewa/{}/{}'.format(path, filename))\n",
       "\n",
       "                # resize image to correct dimension(s)\n",
       "                resized = skimage.transform.resize(img, (400,1000))\n",
       "                plt.imsave('../mining_sadewa/sadewa/{}_r/{}'.format(path, filename), resized)\n",
       "\n",
       "                prevImg = copy.deepcopy(resized)\n",
       "\n",
       "def checkDataError(datasetList, stdDimension):\n",
       "    '''\n",
       "    Check for read and dimension error in dataset\n",
       "    Input datasetList : array like list of data\n",
       "    stdDimension : a tuple containing standard dimension (and color channel(s)) of image\n",
       "    Returning 2 list : readError and nonStdDim\n",
       "    '''\n",
       "    paths = {}\n",
       "    for data in dataset:\n",
       "        paths[data] = os.listdir(folderPath.format(data))\n",
       "\n",
       "    # read test\n",
       "    readError = []\n",
       "    nonStdDim = []\n",
       "    for path in paths:\n",
       "        for filename in paths[path]:\n",
       "            try :\n",
       "                img = mpimg.imread('../mining_sadewa/sadewa/{}/{}'.format(path, filename))\n",
       "                if img.shape != stdDimension:\n",
       "                    print('Non standard dimensions : {}'.format(filename))\n",
       "                    nonStdDim.append(filename)\n",
       "            except Exception:\n",
       "                print('Error occured : {}'.format(filename))\n",
       "                readError.append(filename)\n",
       "\n",
       "    return readError, nonStdDim\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "correctingWindData()\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "predData = ('cloud','psf','qvapor','rain','sst')\n",
       "pdReadError, pdNonStdDim = checkDataError(predData, (400,1000,4))\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "pdNonStdDim\n",
       "```\n",
       "\n",
       "## Creating Deep Neural Network Input Datasets\n",
       "\n",
       "\n",
       "```python\n",
       "# import modules\n",
       "import sqlite3\n",
       "from sqlite3 import Error\n",
       "import os\n",
       "import matplotlib.pyplot as plt\n",
       "import matplotlib.image as mpimg\n",
       "import datetime\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import random\n",
       "import h5py\n",
       "import time\n",
       "from skimage import color\n",
       "import copy\n",
       "\n",
       "def create_connection(db_file):\n",
       "    '''\n",
       "    create a database connection to a SQLite database\n",
       "    specified by db_file\n",
       "    :param db_file : database file\n",
       "    :return: Connection Object or None\n",
       "    '''\n",
       "    conn=None\n",
       "    try:\n",
       "        conn=sqlite3.connect(db_file)\n",
       "        return conn\n",
       "    except Error as e:\n",
       "        print(e)  \n",
       "\n",
       "def manggaraiFullData():\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM manggarai', conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "    \n",
       "    return manggaraiConv\n",
       "\n",
       "def manggaraiDataList(maxData=True, hourOffset=0, wlstation='manggarai'):\n",
       "    '''\n",
       "    Returning a tuple of list (date, data) of manggarai TMA data with 10-minutes-interval from DSDA dataset in year 2020\n",
       "    '''\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM {}'.format(wlstation), conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "\n",
       "    # slicing data to 2020 timeframe\n",
       "    #mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00') & (manggaraiConv['currentdate'] <= '2021-04-03 23:50')\n",
       "    mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00')\n",
       "    manggaraiSlice2020=manggaraiConv.loc[mask]\n",
       "\n",
       "    # converting 10-minute-data to hourly data\n",
       "    startDate=datetime.datetime(2019,2,1)\n",
       "    minutes=[x*10 for x in range(6)]\n",
       "    hours=[x for x in range(24)]\n",
       "    days=[x for x in range(780)]\n",
       "\n",
       "    dateListHourly=[]\n",
       "    dataListHourly=[]\n",
       "    for day in days:\n",
       "        for hour in hours:\n",
       "            hourlyData=[]\n",
       "\n",
       "            # set error indicator back to false\n",
       "            error=False\n",
       "\n",
       "            for minute in minutes:\n",
       "                # perform data fetch, add to list, and get max value\n",
       "                dateLoop=startDate+datetime.timedelta(days=day, hours=hour+hourOffset, minutes=minute)\n",
       "                rowFetch=manggaraiSlice2020.loc[(manggaraiSlice2020['currentdate'] == dateLoop)]\n",
       "                #print(rowFetch)\n",
       "\n",
       "                # try to fetch if the result is not zero\n",
       "                try:\n",
       "                    dataFetch=rowFetch['data'].item()\n",
       "                    hourlyData.append(dataFetch)\n",
       "                except ValueError:\n",
       "                    error=True\n",
       "\n",
       "            # insert data if error indicator is False\n",
       "            if not error:\n",
       "                # make hourly date using timedelta\n",
       "                hourlyDate=startDate+datetime.timedelta(days=day, hours=hour)\n",
       "                \n",
       "                if maxData:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=max(hourlyData)\n",
       "                else:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=hourlyData.mean()\n",
       "\n",
       "                # insert value to global list\n",
       "                dateListHourly.append(hourlyDate)\n",
       "                dataListHourly.append(maxDataHourly)\n",
       "            else: # if error occured during data fetch (null or something else)\n",
       "                continue # to next loop\n",
       "    return dateListHourly, dataListHourly\n",
       "\n",
       "def getHimawariFilename():\n",
       "    '''\n",
       "    Return dictionary of available himawari data based on filename inside\n",
       "    folder as a key\n",
       "    '''\n",
       "    himawariPath='../mining_sadewa/sadewa/'\n",
       "    # load folder name\n",
       "    directory=[directory for directory in os.listdir(himawariPath)]\n",
       "\n",
       "    # store fileame\n",
       "    himawari={}\n",
       "\n",
       "    # load all filename stored on disk to dictionary with each folder name as keys\n",
       "    for direct in directory:\n",
       "        fpath='{}{}'.format(himawariPath, direct)\n",
       "        himawari[direct]=[fname for fname in os.listdir(fpath)]\n",
       "        \n",
       "    return himawari\n",
       "\n",
       "def extractHimawariDatetime():\n",
       "    '''\n",
       "    Extract every filename in sadewa-himawari data to datetime object for easier handling\n",
       "    \n",
       "    Returns :\n",
       "    extractedDate -- dictionary containing list of datetime object for each filename inside dictionary keys for every data\n",
       "    '''\n",
       "    himawari=getHimawariFilename()\n",
       "\n",
       "    # extract date for each himawari data type to datetime.datetime object\n",
       "    observations=['CCLD','B04','IR1','IR3','VIS']\n",
       "    extractedDate={}\n",
       "    for obs in observations:\n",
       "        extractedDate[obs]=[datetime.datetime.strptime(x.replace('H89_{}_'.format(obs),'').replace('.png',''), '%Y%m%d%H%M') for x in himawari[obs]]\n",
       "\n",
       "    predictions=['cloud','psf','qvapor','rain','sst','wind','winu','wn10']\n",
       "    for pred in predictions:\n",
       "        extractedDate[pred]=[datetime.datetime.strptime(x.replace('{}_'.format(pred),'').replace('.png','').replace('_','')+'00', '%Y%m%d%H%M') for x in himawari[pred]]\n",
       "        \n",
       "    return extractedDate\n",
       "\n",
       "def getAvailableSlicedData(maxData=True, hourOffset=0, dataScope='combination', wlstation='manggarai', flagged=False):\n",
       "    '''\n",
       "    check through all available dataset, including manggarai TMA, sadewa-himawari IR1, IR3, VIS, B04, and CCLD\n",
       "    and return a tuple containing datetime object and manggarai hourly TMA data that are synced through all available dataset\n",
       "    \n",
       "    This function doesn't return sadewa-himawari data, because using the datetime format and the sadewa-himawari data types,\n",
       "    the full name of the file required can be constructed.\n",
       "    \n",
       "    return : (slicedDate, slicedData) # both are lists inside a tuple\n",
       "    '''\n",
       "    extractedDate = extractHimawariDatetime()\n",
       "        \n",
       "    # getting date-data slice from himawari and manggarai TMA data\n",
       "\n",
       "    # using function to get manggarai available date-data\n",
       "    dateListHourly, dataListHourly = manggaraiDataList(maxData, hourOffset, wlstation=wlstation)\n",
       "    \n",
       "    # check if the data is flagged above the mean or not\n",
       "    if flagged:\n",
       "        dateListHourly, dataListHourly = flagData(dateListHourly, dataListHourly)\n",
       "\n",
       "    # loop to every data\n",
       "    # check algorithm : manggarai checked against every himawari data, and if all true, date is inserted to sliced data\n",
       "    slicedDate=[]\n",
       "    slicedData=[]\n",
       "    for i in range(len(dateListHourly)):\n",
       "        \n",
       "        if dataScope == 'combination':\n",
       "            usedData=['CCLD','B04','IR1','IR3','VIS','rain','cloud','psf','qvapor','sst']\n",
       "        elif dataScope == 'prediction':\n",
       "            usedData=('cloud','psf','qvapor','rain','sst','wind','winu','wn10')\n",
       "\n",
       "        # defining control mechanism\n",
       "        checked=True\n",
       "\n",
       "        # loop through every himawari data\n",
       "        for used in usedData:\n",
       "            if dateListHourly[i] not in extractedDate[used]:\n",
       "                checked=False # set checked to False if there are no complementary data found in another dataset\n",
       "\n",
       "        # input data if all checked\n",
       "        if checked:\n",
       "            slicedDate.append(dateListHourly[i])\n",
       "            slicedData.append(dataListHourly[i])\n",
       "    return slicedDate, slicedData\n",
       "\n",
       "def flagData(adte, adta):\n",
       "    '''\n",
       "    Filter date and data above the mean\n",
       "    '''\n",
       "    adtaDF = pd.DataFrame(adta).astype('int32')\n",
       "    adteDF = pd.DataFrame(adte)\n",
       "    flaggedAdta = adtaDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    flaggedAdte = adteDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    return list(flaggedAdte[0].dt.to_pydatetime()), list(flaggedAdta[0].astype('object'))\n",
       "\n",
       "def cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath, predData=False):\n",
       "    '''\n",
       "    Crop image data based on defined crop bound in horizontal (x) and vertical (y) direction,\n",
       "    and append the cropped data to nd numpy array with format : (m datas, datatypes, imgdim1, imgdim2, number of channels)\n",
       "    \n",
       "    Parameters :\n",
       "    imgCropX -- list of start and end bound of horizontal slice index image numpy array\n",
       "    imgCropY -- list of start and end bound of horizontal slice index image numpy array\n",
       "    adte -- list of available date in datetime object\n",
       "    usedDatas -- list of want-to-crop data\n",
       "    imgPath -- complete image path with string format placeholder relative from current working directory\n",
       "    datef -- main date formatted to inserted into placeholder in imgPath\n",
       "    dateh -- optional date format for prediction data\n",
       "    \n",
       "    Returns :\n",
       "    croppedData -- numpy array of cropped data with format : (m datas, datatypes, imgdim1, imgdim2, number of channels)\n",
       "    '''\n",
       "    # loop conditional\n",
       "    firstColumn=True\n",
       "    i=0\n",
       "    for date in adte:\n",
       "        # loop conditional\n",
       "        firstRow=True\n",
       "        for data in usedDatas:\n",
       "            if predData:\n",
       "                datef = date.strftime('%Y%m%d')\n",
       "                dateh = date.strftime('%H')\n",
       "            else:\n",
       "                datef = date.strftime('%Y%m%d%H%M')\n",
       "                dateh = None\n",
       "\n",
       "            imgPathF=imgPath.format(data, data, datef, dateh)\n",
       "\n",
       "            # fetching image data\n",
       "            try:\n",
       "                image=mpimg.imread(imgPathF)\n",
       "                pathPrev=imgPathF\n",
       "            except:\n",
       "                print(imgPathF)\n",
       "                image=mpimg.imread(pathPrev)\n",
       "            \n",
       "            # cropping image to defined dimension(s)\n",
       "            image=image[imgCropX[0]:imgCropX[1], imgCropY[0]:imgCropY[1]]\n",
       "            \n",
       "            image=image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
       "            \n",
       "            # check for first loop \n",
       "            if firstRow:\n",
       "                sameDate=np.copy(image)\n",
       "                firstRow=False\n",
       "            else:\n",
       "                sameDate=np.vstack((sameDate, image))\n",
       "        \n",
       "        # reshaping numpy array\n",
       "        sameDate=sameDate.reshape(1, sameDate.shape[0], sameDate.shape[1], sameDate.shape[2], sameDate.shape[3])\n",
       "        \n",
       "        # check for first loop\n",
       "        if firstColumn:\n",
       "            croppedData=np.copy(sameDate)\n",
       "            firstColumn=False\n",
       "        else:\n",
       "            croppedData=np.vstack((croppedData, sameDate))\n",
       "        if i%100 == 0:\n",
       "            print(croppedData.shape)\n",
       "        i+=1\n",
       "            \n",
       "    return croppedData \n",
       "    \n",
       "\n",
       "def cropImagePredictionData(dim, usedDatas=['rain','cloud','psf','qvapor','sst'], hourOffset=0, wlstation='manggarai', flagged=False, nativeSadewa=False, recurrentCount=0):\n",
       "    '''\n",
       "    Returning numpy array with dimension of (m training data, nodes), that nodes = (rain, cloud, psf, qvapor, sst) cropped data\n",
       "    based on defined dimensions : 100 (10x10), 196 (14x14), 400 (20x20)\n",
       "    '''\n",
       "    if dim == 72:\n",
       "        imgCropX=[324, 336] # 12x6\n",
       "        imgCropY=[234, 240] # 12x6\n",
       "    elif dim == 100:\n",
       "        imgCropX=[323,333] # 10x10\n",
       "        imgCropY=[233,243] # 10x10\n",
       "    elif dim == 196:\n",
       "        imgCropX=[320,334] # 14x14\n",
       "        imgCropY=[230,244] # 14x14\n",
       "    elif dim == 240:\n",
       "        imgCropX=[318, 338] # 20x12\n",
       "        imgCropY=[231, 243] # 20x12  \n",
       "    #elif dim == 400:\n",
       "        #imgCropX=[317,337] # 20x20\n",
       "        #imgCropY=[227,247] # 20x20\n",
       "    #elif dim == 400:\n",
       "        #imgCropX=[318, 338] # 20x20v2 shifted down 1 cell\n",
       "        #imgCropY=[227, 247] # 20x20v2 shifted down 1 cell\n",
       "    # Katulampa crop extent\n",
       "    elif dim == 16: # Katulampa 4x4 input cell\n",
       "        imgCropX=[332, 336]\n",
       "        imgCropY=[236, 240]\n",
       "    elif dim == 64: # Katulampa 8x8 input cell\n",
       "        imgCropX=[330, 338]\n",
       "        imgCropY=[234, 242]\n",
       "    elif dim == 144: # Katulampa 12x12 input cell\n",
       "        imgCropX=[328, 340]\n",
       "        imgCropY=[232, 244]\n",
       "    elif dim == 256: # Katulampa 16x16 input cell\n",
       "        imgCropX=[326, 342]\n",
       "        imgCropY=[230, 246]\n",
       "    elif dim == 400: # Katulampa 20x20 input cell\n",
       "        imgCropX=[324, 344]\n",
       "        imgCropY=[228, 248]\n",
       "    elif dim == 576: # Katulampa 24x24 input cell\n",
       "        imgCropX=[322, 346]\n",
       "        imgCropY=[226, 250]\n",
       "    elif dim == 784: # Katulampa 28x28 input cell\n",
       "        imgCropX=[320, 348]\n",
       "        imgCropY=[224, 252]\n",
       "    elif dim == 1024: # Katulampa 32x32 input cell\n",
       "        imgCropX=[318, 350]\n",
       "        imgCropY=[222, 254]\n",
       "    elif dim == 1296: # Katulampa 36x36 input cell\n",
       "        imgCropX=[316, 352]\n",
       "        imgCropY=[220, 256]\n",
       "    elif dim == 1600: # Katulampa 36x36 input cell\n",
       "        imgCropX=[314, 354]\n",
       "        imgCropY=[218, 258]\n",
       "\n",
       "    # fetch data\n",
       "    adte, adta = getAvailableSlicedData(dataScope='prediction', hourOffset=hourOffset, wlstation=wlstation, flagged=flagged)\n",
       "    recurrentIndexList, availableRecurrentDate, availableRecurrentLabel, sadewaDateFiltered = generateRNNInputv2(adte, adta, recurrentCount=recurrentCount, offset=hourOffset)\n",
       "    \n",
       "    if nativeSadewa:\n",
       "        # for the sake of image crop, assign sadewaDateFiltered as adte\n",
       "        adte = sadewaDateFiltered\n",
       "\n",
       "    else:\n",
       "        # passthrough availableRecurrentDate as adte (the value doesn't change if the recurrentCount and hourOffset set to 0)\n",
       "        adte = availableRecurrentDate\n",
       "\n",
       "    # save another data to file with format : WLSTATION_R[#]_F[#]_VARNAME\n",
       "    # recurrentIndexList\n",
       "    with h5py.File(f'./dataset/prequisites/{wlstation}_R{recurrentCount}_O{hourOffset}_recurrentIndexList.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=recurrentIndexList, compression='gzip', compression_opts=7)\n",
       "    # availableRecurrentDate\n",
       "    ardDF = pd.DataFrame(availableRecurrentDate)\n",
       "    ardDF.to_csv(f'./dataset/prequisites/{wlstation}_R{recurrentCount}_O{hourOffset}_availableRecurrentDate.csv')\n",
       "    # availableRecurrentLabel\n",
       "    arl = np.array(availableRecurrentLabel).astype('int16')\n",
       "    with h5py.File(f'./dataset/prequisites/{wlstation}_R{recurrentCount}_O{hourOffset}_availableRecurrentLabel.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=arl, compression='gzip', compression_opts=7)\n",
       "    # sadewaDateFiltered\n",
       "    sdfDF = pd.DataFrame(sadewaDateFiltered)\n",
       "    sdfDF.to_csv(f'./dataset/prequisites/{wlstation}_R{recurrentCount}_O{hourOffset}_sadewaDateFiltered.csv')\n",
       "\n",
       "    imgPath = '../mining_sadewa/sadewa/{}/{}_{}_{}.png'\n",
       "\n",
       "    return cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath, predData=True)\n",
       "\n",
       "def cropImageObservationData(dim, usedDatas=['IR1','IR3','B04','VIS','CCLD']):\n",
       "    '''\n",
       "    Returning 3 dimensions numpy array with (m training data, nodes), that nodes = (IR1, IR3, B04, VIS, CCLD) cropped data\n",
       "    based on defined dimensions : 100 (10x10), 196 (14x14), 400 (20x20)\n",
       "    '''\n",
       "    if dim == 45:\n",
       "        imgCropX=[932,941]\n",
       "        imgCropY=[517,522]\n",
       "    elif dim == 100:\n",
       "        imgCropX=[910,920]\n",
       "        imgCropY=[405,415]\n",
       "    elif dim == 196:\n",
       "        imgCropX=[908,922]\n",
       "        imgCropY=[403,417]\n",
       "    elif dim == 198: #v2\n",
       "        imgCropX=[928,942]\n",
       "        imgCropY=[512,526]\n",
       "    elif dim == 400:\n",
       "        imgCropX=[905,925]\n",
       "        imgCropY=[400,420]\n",
       "       \n",
       "    adte, adta = getAvailableSlicedData()\n",
       "    imgPath = '../mining_sadewa/sadewa/{}/H89_{}_{}.png'\n",
       "\n",
       "    return cropImageData(imgCropX, imgCropY, adte, usedDatas, imgPath)\n",
       "\n",
       "# prediction data only\n",
       "def preparePrediction(pred, grayscale=False):\n",
       "    # loop through all available data\n",
       "    firstData = True\n",
       "    for i in range(len(pred)):\n",
       "        # loop through dataset\n",
       "        firstDataset = True\n",
       "        for j in range(len(pred[i])):\n",
       "            if False:\n",
       "                continue\n",
       "            else :\n",
       "                # check if grayscale or not\n",
       "                if grayscale:\n",
       "                    img = color.rgb2gray(color.rgba2rgb(pred[i][j]))\n",
       "                    flat = img.reshape(pred[i][j].shape[0]*pred[i][j].shape[1])\n",
       "                else:\n",
       "                    img = pred[i][j]\n",
       "                    flat = pred[i][j].reshape(pred[i][j].shape[0]*pred[i][j].shape[1]*pred[i][j].shape[2])\n",
       "                \n",
       "                \n",
       "                if firstDataset:\n",
       "                    flattened = flat.copy()\n",
       "                    firstDataset = False\n",
       "                else :\n",
       "                    flattened = np.hstack((flattened, flat))\n",
       "        if firstData:\n",
       "            data = flattened.copy()\n",
       "            data = data.reshape(1, data.shape[0])\n",
       "            firstData = False\n",
       "        else :\n",
       "            flattened = flattened.reshape(1, flattened.shape[0])\n",
       "            data = np.vstack((data, flattened))\n",
       "    return data\n",
       "\n",
       "def generateRNNInputv2(adte, adta, recurrentCount=0, offset=0):\n",
       "    '''\n",
       "    Second version, upgraded algorithm, use sadewa date to check for RNN, and then pair last date of sadewa to WL data.\n",
       "    This version prevent steep decline* of paired data, as seen on the 1st version.\n",
       "    *In Katulampa, the paired data only diminish about ~10% in 36 paired data, compared to ~99.9% in the 1st version\n",
       "    '''\n",
       "    # fetch sadewa data\n",
       "    sadewaDate = extractHimawariDatetime()\n",
       "\n",
       "    # combined Sadewa Date\n",
       "    combinedSadewa = []\n",
       "    keys = ('cloud','psf','qvapor','rain','sst','wind','winu','wn10')\n",
       "    for firstDate in sadewaDate[keys[0]]:\n",
       "        passed = True\n",
       "        for key in keys[1:]:\n",
       "            if firstDate not in sadewaDate[key]:\n",
       "                passed = False\n",
       "        if passed:\n",
       "            combinedSadewa.append(firstDate)    \n",
       "    \n",
       "    # filter sadewa date, only from adte and above to prevent remaking base dataset\n",
       "    sadewaToDF = pd.DataFrame(combinedSadewa)\n",
       "    filtered = sadewaToDF[sadewaToDF[0] >= adte[0]]\n",
       "    sadewaDateFiltered = list(filtered[0].dt.to_pydatetime())\n",
       "\n",
       "    # filter adte date to prevent error in indexing by filtering in the end of the list\n",
       "    assert(offset >= 0)\n",
       "    if offset > 0:\n",
       "        adteDF = pd.DataFrame(adte)\n",
       "        filteredAdte = adteDF[:-offset]\n",
       "        filteredAdteList = list(filteredAdte[0].dt.to_pydatetime())\n",
       "    else:\n",
       "        filteredAdteList = copy.deepcopy(adte)\n",
       "\n",
       "    # check RNN Sequence on input section (Sadewa)\n",
       "    recurrentIndexList = [] # for restacking purpose\n",
       "    availableRecurrentLabel = [] # for training purpose\n",
       "    availableRecurrentDate = [] # for analysis purpose\n",
       "    for idx in range(len(sadewaDateFiltered[recurrentCount:])):\n",
       "        # check sequence\n",
       "        checkSeq = [sadewaDateFiltered[idx+recurrentCount]+datetime.timedelta(hours=-recurrentCount)+datetime.timedelta(hours=x) for x in range(recurrentCount+1)]\n",
       "        realSeq = [sadewaDateFiltered[idx+x] for x in range(recurrentCount+1)]\n",
       "        if checkSeq != realSeq: # if sequence is not complete, skip to next date\n",
       "            continue \n",
       "        else: # if sequence check complete, check last sadewa date to WL date\n",
       "            sadewaPairDate = sadewaDateFiltered[idx+recurrentCount]+datetime.timedelta(hours=offset)\n",
       "            if sadewaPairDate in filteredAdteList:\n",
       "                # search for Label index\n",
       "                labelIndex = filteredAdteList.index(sadewaPairDate)\n",
       "                availableRecurrentLabel.append(adta[labelIndex])\n",
       "                availableRecurrentDate.append(sadewaPairDate)\n",
       "\n",
       "                # make a recurrent count for stacking\n",
       "                recurrentIndexList.append([idx+x for x in range(recurrentCount+1)])  \n",
       "                \n",
       "    return recurrentIndexList, availableRecurrentDate, availableRecurrentLabel, sadewaDateFiltered\n",
       "\n",
       "def performIndividualCropPredictionData(dim, sequence, combination=2):\n",
       "    '''\n",
       "    Perform individual database creation of prediction data\n",
       "    '''\n",
       "    # initializing individual variables\n",
       "    #\n",
       "    if combination == 1:\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 2:\n",
       "        usedDatas = [('cloud', 'psf'), ('cloud', 'qvapor'), ('cloud', 'rain'), ('cloud', 'sst'), ('cloud', 'wind'), ('psf', 'qvapor'), ('psf', 'rain'), ('psf', 'sst'), ('psf', 'wind'), ('qvapor', 'rain'), ('qvapor', 'sst'), ('qvapor', 'wind'), ('rain', 'sst'), ('rain', 'wind'), ('sst', 'wind')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:8]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[8:12]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[12:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}{}.hdf5'.format(usedData[0], usedData[1], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 3:\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor'), ('cloud', 'psf', 'rain'), ('cloud', 'psf', 'sst'), ('cloud', 'psf', 'wind'), ('cloud', 'qvapor', 'rain'), ('cloud', 'qvapor', 'sst'), ('cloud', 'qvapor', 'wind'), ('cloud', 'rain', 'sst'), ('cloud', 'rain', 'wind'), ('cloud', 'sst', 'wind'), ('psf', 'qvapor', 'rain'), ('psf', 'qvapor', 'sst'), ('psf', 'qvapor', 'wind'), ('psf', 'rain', 'sst'), ('psf', 'rain', 'wind'), ('psf', 'sst', 'wind'), ('qvapor', 'rain', 'sst'), ('qvapor', 'rain', 'wind'), ('qvapor', 'sst', 'wind'), ('rain', 'sst', 'wind')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:8]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[8:12]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[12:16]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[16:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 4:\n",
       "        usedDatas = [['winu'],['wn10'],['wind']]\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 5:\n",
       "        usedDatas = [('cloud', 'winu'), ('cloud', 'wn10'), ('psf', 'winu'), ('psf', 'wn10'), ('qvapor', 'winu'), ('qvapor', 'wn10'), ('rain', 'winu'), ('rain', 'wn10'), ('sst', 'winu'), ('sst', 'wn10'), ('wind', 'winu'), ('wind', 'wn10'), ('winu', 'wn10'), ('cloud', 'wind'), ('psf', 'wind'), ('qvapor', 'wind'), ('rain', 'wind'), ('sst', 'wind')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:8]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[8:12]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[12:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}{}.hdf5'.format(usedData[0], usedData[1], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 6:\n",
       "        usedDatas = [('cloud', 'psf', 'winu'), ('cloud', 'psf', 'wn10'), ('cloud', 'qvapor', 'winu'), ('cloud', 'qvapor', 'wn10'), ('cloud', 'rain', 'winu'), ('cloud', 'rain', 'wn10'), ('cloud', 'sst', 'winu'), ('cloud', 'sst', 'wn10'), ('cloud', 'wind', 'winu'), ('cloud', 'wind', 'wn10'), ('cloud', 'winu', 'wn10'), ('psf', 'qvapor', 'winu'), ('psf', 'qvapor', 'wn10'), ('psf', 'rain', 'winu'), ('psf', 'rain', 'wn10'), ('psf', 'sst', 'winu'), ('psf', 'sst', 'wn10'), ('psf', 'wind', 'winu'), ('psf', 'wind', 'wn10'), ('psf', 'winu', 'wn10'), ('qvapor', 'rain', 'winu'), ('qvapor', 'rain', 'wn10'), ('qvapor', 'sst', 'winu'), ('qvapor', 'sst', 'wn10'), ('qvapor', 'wind', 'winu'), ('qvapor', 'wind', 'wn10'), ('qvapor', 'winu', 'wn10'), ('rain', 'sst', 'winu'), ('rain', 'sst', 'wn10'), ('rain', 'wind', 'winu'), ('rain', 'wind', 'wn10'), ('rain', 'winu', 'wn10'), ('sst', 'wind', 'winu'), ('sst', 'wind', 'wn10'), ('sst', 'winu', 'wn10'), ('wind', 'winu', 'wn10'), ('cloud', 'psf', 'wind'), ('cloud', 'qvapor', 'wind'), ('cloud', 'rain', 'wind'), ('cloud', 'sst', 'wind'), ('psf', 'qvapor', 'wind'), ('psf', 'rain', 'wind'), ('psf', 'sst', 'wind'), ('qvapor', 'rain', 'wind'), ('qvapor', 'sst', 'wind'), ('rain', 'sst', 'wind')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:10]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[10:20]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[20:30]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[30:40]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[40:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 7:\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor'), ('psf', 'qvapor', 'sst')]\n",
       "        offsets = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:4]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[4:8]\n",
       "        elif sequence == 2:\n",
       "            offsets = offsets[8:12]\n",
       "        elif sequence == 3:\n",
       "            offsets = offsets[12:16]\n",
       "        elif sequence == 4:\n",
       "            offsets = offsets[16:20]\n",
       "        elif sequence == 5:\n",
       "            offsets = offsets[20:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}{}{}-{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 8:\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor'), ('psf', 'qvapor', 'sst')]\n",
       "        offsets = [25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:6]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[6:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}{}{}-{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 9:\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "        offsets = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:18]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[18:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}-{}.hdf5'.format(usedData[0], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 10:\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "        offsets = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:4]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[4:8]\n",
       "        elif sequence == 2:\n",
       "            offsets = offsets[8:12]\n",
       "        elif sequence == 3:\n",
       "            offsets = offsets[12:16]\n",
       "        elif sequence == 4:\n",
       "            offsets = offsets[16:20]\n",
       "        elif sequence == 5:\n",
       "            offsets = offsets[20:24]\n",
       "        elif sequence == 6:\n",
       "            offsets = offsets[24:28]\n",
       "        elif sequence == 7:\n",
       "            offsets = offsets[28:32]\n",
       "        elif sequence == 8:\n",
       "            offsets = offsets[32:36]\n",
       "            \n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}-{}.hdf5'.format(usedData[0], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 11:\n",
       "        usedDatas = [('qvapor', 'rain', 'sst')]\n",
       "        offsets = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:4]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[4:8]\n",
       "        elif sequence == 2:\n",
       "            offsets = offsets[8:12]\n",
       "        elif sequence == 3:\n",
       "            offsets = offsets[12:16]\n",
       "        elif sequence == 4:\n",
       "            offsets = offsets[16:20]\n",
       "        elif sequence == 5:\n",
       "            offsets = offsets[20:24]\n",
       "        elif sequence == 6:\n",
       "            offsets = offsets[24:28]\n",
       "        elif sequence == 7:\n",
       "            offsets = offsets[28:32]\n",
       "        elif sequence == 8:\n",
       "            offsets = offsets[32:36]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}{}{}-{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 12:\n",
       "        usedDatas = [('psf', 'qvapor', 'sst')]\n",
       "        offsets = [25,26,27,28,29,30,31,32,33,34,35,36]\n",
       "\n",
       "        if sequence == 0:\n",
       "            offsets = offsets[:6]\n",
       "        elif sequence == 1:\n",
       "            offsets = offsets[6:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            for offset in offsets:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, hourOffset=offset)\n",
       "                with h5py.File('dataset/{}{}{}{}-{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim, offset), 'w') as f:\n",
       "                    f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 13:\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain'), ('cloud', 'psf', 'qvapor', 'sst'), ('cloud', 'psf', 'qvapor', 'wind'), ('cloud', 'psf', 'rain', 'sst'), ('cloud', 'psf', 'rain', 'wind'), ('cloud', 'psf', 'sst', 'wind'), ('cloud', 'qvapor', 'rain', 'sst'), ('cloud', 'qvapor', 'rain', 'wind'), ('cloud', 'qvapor', 'sst', 'wind'), ('cloud', 'rain', 'sst', 'wind'), ('psf', 'qvapor', 'rain', 'sst'), ('psf', 'qvapor', 'rain', 'wind'), ('psf', 'qvapor', 'sst', 'wind'), ('psf', 'rain', 'sst', 'wind'), ('qvapor', 'rain', 'sst', 'wind')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:3]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[3:6]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[6:9]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[9:12]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[12:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 14: # Katulampa single data\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData)\n",
       "            with h5py.File('dataset/{}{}.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 14: # Katulampa single data\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            with h5py.File('dataset/{}{}.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 15: # Katulampa double data\n",
       "        usedDatas = [('cloud', 'psf'), ('cloud', 'qvapor'), ('cloud', 'rain'), ('cloud', 'sst'), ('cloud', 'wind'), ('cloud', 'winu'), ('cloud', 'wn10'), ('psf', 'qvapor'), ('psf', 'rain'), ('psf', 'sst'), ('psf', 'wind'), ('psf', 'winu'), ('psf', 'wn10'), ('qvapor', 'rain'), ('qvapor', 'sst'), ('qvapor', 'wind'), ('qvapor', 'winu'), ('qvapor', 'wn10'), ('rain', 'sst'), ('rain', 'wind'), ('rain', 'winu'), ('rain', 'wn10'), ('sst', 'wind'), ('sst', 'winu'), ('sst', 'wn10'), ('wind', 'winu'), ('wind', 'wn10'), ('winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            with h5py.File('dataset/{}{}{}.hdf5'.format(usedData[0], usedData[1],dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 16: # Katulampa triple data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor'), ('cloud', 'psf', 'rain'), ('cloud', 'psf', 'sst'), ('cloud', 'psf', 'wind'), ('cloud', 'psf', 'winu'), ('cloud', 'psf', 'wn10'), ('cloud', 'qvapor', 'rain'), ('cloud', 'qvapor', 'sst'), ('cloud', 'qvapor', 'wind'), ('cloud', 'qvapor', 'winu'), ('cloud', 'qvapor', 'wn10'), ('cloud', 'rain', 'sst'), ('cloud', 'rain', 'wind'), ('cloud', 'rain', 'winu'), ('cloud', 'rain', 'wn10'), ('cloud', 'sst', 'wind'), ('cloud', 'sst', 'winu'), ('cloud', 'sst', 'wn10'), ('cloud', 'wind', 'winu'), ('cloud', 'wind', 'wn10'), ('cloud', 'winu', 'wn10'), ('psf', 'qvapor', 'rain'), ('psf', 'qvapor', 'sst'), ('psf', 'qvapor', 'wind'), ('psf', 'qvapor', 'winu'), ('psf', 'qvapor', 'wn10'), ('psf', 'rain', 'sst'), ('psf', 'rain', 'wind'), ('psf', 'rain', 'winu'), ('psf', 'rain', 'wn10'), ('psf', 'sst', 'wind'), ('psf', 'sst', 'winu'), ('psf', 'sst', 'wn10'), ('psf', 'wind', 'winu'), ('psf', 'wind', 'wn10'), ('psf', 'winu', 'wn10'), ('qvapor', 'rain', 'sst'), ('qvapor', 'rain', 'wind'), ('qvapor', 'rain', 'winu'), ('qvapor', 'rain', 'wn10'), ('qvapor', 'sst', 'wind'), ('qvapor', 'sst', 'winu'), ('qvapor', 'sst', 'wn10'), ('qvapor', 'wind', 'winu'), ('qvapor', 'wind', 'wn10'), ('qvapor', 'winu', 'wn10'), ('rain', 'sst', 'wind'), ('rain', 'sst', 'winu'), ('rain', 'sst', 'wn10'), ('rain', 'wind', 'winu'), ('rain', 'wind', 'wn10'), ('rain', 'winu', 'wn10'), ('sst', 'wind', 'winu'), ('sst', 'wind', 'wn10'), ('sst', 'winu', 'wn10'), ('wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            with h5py.File('dataset/{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 17: # Katulampa tetra data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain'), ('cloud', 'psf', 'qvapor', 'sst'), ('cloud', 'psf', 'qvapor', 'wind'), ('cloud', 'psf', 'qvapor', 'winu'), ('cloud', 'psf', 'qvapor', 'wn10'), ('cloud', 'psf', 'rain', 'sst'), ('cloud', 'psf', 'rain', 'wind'), ('cloud', 'psf', 'rain', 'winu'), ('cloud', 'psf', 'rain', 'wn10'), ('cloud', 'psf', 'sst', 'wind'), ('cloud', 'psf', 'sst', 'winu'), ('cloud', 'psf', 'sst', 'wn10'), ('cloud', 'psf', 'wind', 'winu'), ('cloud', 'psf', 'wind', 'wn10'), ('cloud', 'psf', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst'), ('cloud', 'qvapor', 'rain', 'wind'), ('cloud', 'qvapor', 'rain', 'winu'), ('cloud', 'qvapor', 'rain', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind'), ('cloud', 'qvapor', 'sst', 'winu'), ('cloud', 'qvapor', 'sst', 'wn10'), ('cloud', 'qvapor', 'wind', 'winu'), ('cloud', 'qvapor', 'wind', 'wn10'), ('cloud', 'qvapor', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind'), ('cloud', 'rain', 'sst', 'winu'), ('cloud', 'rain', 'sst', 'wn10'), ('cloud', 'rain', 'wind', 'winu'), ('cloud', 'rain', 'wind', 'wn10'), ('cloud', 'rain', 'winu', 'wn10'), ('cloud', 'sst', 'wind', 'winu'), ('cloud', 'sst', 'wind', 'wn10'), ('cloud', 'sst', 'winu', 'wn10'), ('cloud', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst'), ('psf', 'qvapor', 'rain', 'wind'), ('psf', 'qvapor', 'rain', 'winu'), ('psf', 'qvapor', 'rain', 'wn10'), ('psf', 'qvapor', 'sst', 'wind'), ('psf', 'qvapor', 'sst', 'winu'), ('psf', 'qvapor', 'sst', 'wn10'), ('psf', 'qvapor', 'wind', 'winu'), ('psf', 'qvapor', 'wind', 'wn10'), ('psf', 'qvapor', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind'), ('psf', 'rain', 'sst', 'winu'), ('psf', 'rain', 'sst', 'wn10'), ('psf', 'rain', 'wind', 'winu'), ('psf', 'rain', 'wind', 'wn10'), ('psf', 'rain', 'winu', 'wn10'), ('psf', 'sst', 'wind', 'winu'), ('psf', 'sst', 'wind', 'wn10'), ('psf', 'sst', 'winu', 'wn10'), ('psf', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind'), ('qvapor', 'rain', 'sst', 'winu'), ('qvapor', 'rain', 'sst', 'wn10'), ('qvapor', 'rain', 'wind', 'winu'), ('qvapor', 'rain', 'wind', 'wn10'), ('qvapor', 'rain', 'winu', 'wn10'), ('qvapor', 'sst', 'wind', 'winu'), ('qvapor', 'sst', 'wind', 'wn10'), ('qvapor', 'sst', 'winu', 'wn10'), ('qvapor', 'wind', 'winu', 'wn10'), ('rain', 'sst', 'wind', 'winu'), ('rain', 'sst', 'wind', 'wn10'), ('rain', 'sst', 'winu', 'wn10'), ('rain', 'wind', 'winu', 'wn10'), ('sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:56]\n",
       "        elif sequence == 8:\n",
       "            usedDatas = usedDatas[56:63]\n",
       "        elif sequence == 9:\n",
       "            usedDatas = usedDatas[63:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            with h5py.File('dataset/{}{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 18: # Katulampa penta data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst'), ('cloud', 'psf', 'qvapor', 'rain', 'wind'), ('cloud', 'psf', 'qvapor', 'rain', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind'), ('cloud', 'psf', 'qvapor', 'sst', 'winu'), ('cloud', 'psf', 'qvapor', 'sst', 'wn10'), ('cloud', 'psf', 'qvapor', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind'), ('cloud', 'psf', 'rain', 'sst', 'winu'), ('cloud', 'psf', 'rain', 'sst', 'wn10'), ('cloud', 'psf', 'rain', 'wind', 'winu'), ('cloud', 'psf', 'rain', 'wind', 'wn10'), ('cloud', 'psf', 'rain', 'winu', 'wn10'), ('cloud', 'psf', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind'), ('cloud', 'qvapor', 'rain', 'sst', 'winu'), ('cloud', 'qvapor', 'rain', 'sst', 'wn10'), ('cloud', 'qvapor', 'rain', 'wind', 'winu'), ('cloud', 'qvapor', 'rain', 'wind', 'wn10'), ('cloud', 'qvapor', 'rain', 'winu', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind', 'winu'), ('cloud', 'qvapor', 'sst', 'wind', 'wn10'), ('cloud', 'qvapor', 'sst', 'winu', 'wn10'), ('cloud', 'qvapor', 'wind', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind'), ('psf', 'qvapor', 'rain', 'sst', 'winu'), ('psf', 'qvapor', 'rain', 'sst', 'wn10'), ('psf', 'qvapor', 'rain', 'wind', 'winu'), ('psf', 'qvapor', 'rain', 'wind', 'wn10'), ('psf', 'qvapor', 'rain', 'winu', 'wn10'), ('psf', 'qvapor', 'sst', 'wind', 'winu'), ('psf', 'qvapor', 'sst', 'wind', 'wn10'), ('psf', 'qvapor', 'sst', 'winu', 'wn10'), ('psf', 'qvapor', 'wind', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind', 'winu'), ('psf', 'rain', 'sst', 'wind', 'wn10'), ('psf', 'rain', 'sst', 'winu', 'wn10'), ('psf', 'rain', 'wind', 'winu', 'wn10'), ('psf', 'sst', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind', 'winu'), ('qvapor', 'rain', 'sst', 'wind', 'wn10'), ('qvapor', 'rain', 'sst', 'winu', 'wn10'), ('qvapor', 'rain', 'wind', 'winu', 'wn10'), ('qvapor', 'sst', 'wind', 'winu', 'wn10'), ('rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:]\n",
       "        elif sequence == 699:\n",
       "            usedDatas = [('psf', 'rain', 'sst', 'winu', 'wn10'), ('psf', 'rain', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened)\n",
       "\n",
       "    elif combination == 19: # recreating specific manggarai data for RNN\n",
       "        usedDatas = [('qvapor','sst'),('psf','qvapor'),('psf','qvapor','sst'),('qvapor','rain','sst'),('cloud','qvapor','sst')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            if len(usedData) == 2:\n",
       "                fileName = 'dataset/manggaraiRNN/{}{}{}.hdf5'.format(usedData[0], usedData[1], dim)\n",
       "            elif len(usedData) == 3:\n",
       "                fileName = 'dataset/manggaraiRNN/{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], dim)\n",
       "            with h5py.File(fileName, 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "    elif combination == 20: # Katulampa hexa data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened)\n",
       "\n",
       "    elif combination == 21: # Katulampa hepta data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened)\n",
       "\n",
       "    elif combination == 22: # Katulampa octa data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], usedData[7], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened)\n",
       "\n",
       "    elif combination == 23: # Recreating missing katulampa data\n",
       "        usedDatas = [('cloud','psf','qvapor','sst'),]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa')\n",
       "            with h5py.File('dataset/katulampaRNN/{}{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=crop)\n",
       "\n",
       "\n",
       "    elif combination == 24: # Manggarai double data\n",
       "        usedDatas = [('cloud', 'psf'), ('cloud', 'qvapor'), ('cloud', 'rain'), ('cloud', 'sst'), ('cloud', 'wind'), ('cloud', 'winu'), ('cloud', 'wn10'), ('psf', 'qvapor'), ('psf', 'rain'), ('psf', 'sst'), ('psf', 'wind'), ('psf', 'winu'), ('psf', 'wn10'), ('qvapor', 'rain'), ('qvapor', 'sst'), ('qvapor', 'wind'), ('qvapor', 'winu'), ('qvapor', 'wn10'), ('rain', 'sst'), ('rain', 'wind'), ('rain', 'winu'), ('rain', 'wn10'), ('sst', 'wind'), ('sst', 'winu'), ('sst', 'wn10'), ('wind', 'winu'), ('wind', 'wn10'), ('winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}f.hdf5'.format(usedData[0], usedData[1],dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 25: # Manggarai triple data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor'), ('cloud', 'psf', 'rain'), ('cloud', 'psf', 'sst'), ('cloud', 'psf', 'wind'), ('cloud', 'psf', 'winu'), ('cloud', 'psf', 'wn10'), ('cloud', 'qvapor', 'rain'), ('cloud', 'qvapor', 'sst'), ('cloud', 'qvapor', 'wind'), ('cloud', 'qvapor', 'winu'), ('cloud', 'qvapor', 'wn10'), ('cloud', 'rain', 'sst'), ('cloud', 'rain', 'wind'), ('cloud', 'rain', 'winu'), ('cloud', 'rain', 'wn10'), ('cloud', 'sst', 'wind'), ('cloud', 'sst', 'winu'), ('cloud', 'sst', 'wn10'), ('cloud', 'wind', 'winu'), ('cloud', 'wind', 'wn10'), ('cloud', 'winu', 'wn10'), ('psf', 'qvapor', 'rain'), ('psf', 'qvapor', 'sst'), ('psf', 'qvapor', 'wind'), ('psf', 'qvapor', 'winu'), ('psf', 'qvapor', 'wn10'), ('psf', 'rain', 'sst'), ('psf', 'rain', 'wind'), ('psf', 'rain', 'winu'), ('psf', 'rain', 'wn10'), ('psf', 'sst', 'wind'), ('psf', 'sst', 'winu'), ('psf', 'sst', 'wn10'), ('psf', 'wind', 'winu'), ('psf', 'wind', 'wn10'), ('psf', 'winu', 'wn10'), ('qvapor', 'rain', 'sst'), ('qvapor', 'rain', 'wind'), ('qvapor', 'rain', 'winu'), ('qvapor', 'rain', 'wn10'), ('qvapor', 'sst', 'wind'), ('qvapor', 'sst', 'winu'), ('qvapor', 'sst', 'wn10'), ('qvapor', 'wind', 'winu'), ('qvapor', 'wind', 'wn10'), ('qvapor', 'winu', 'wn10'), ('rain', 'sst', 'wind'), ('rain', 'sst', 'winu'), ('rain', 'sst', 'wn10'), ('rain', 'wind', 'winu'), ('rain', 'wind', 'wn10'), ('rain', 'winu', 'wn10'), ('sst', 'wind', 'winu'), ('sst', 'wind', 'wn10'), ('sst', 'winu', 'wn10'), ('wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 26: # Manggarai tetra data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain'), ('cloud', 'psf', 'qvapor', 'sst'), ('cloud', 'psf', 'qvapor', 'wind'), ('cloud', 'psf', 'qvapor', 'winu'), ('cloud', 'psf', 'qvapor', 'wn10'), ('cloud', 'psf', 'rain', 'sst'), ('cloud', 'psf', 'rain', 'wind'), ('cloud', 'psf', 'rain', 'winu'), ('cloud', 'psf', 'rain', 'wn10'), ('cloud', 'psf', 'sst', 'wind'), ('cloud', 'psf', 'sst', 'winu'), ('cloud', 'psf', 'sst', 'wn10'), ('cloud', 'psf', 'wind', 'winu'), ('cloud', 'psf', 'wind', 'wn10'), ('cloud', 'psf', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst'), ('cloud', 'qvapor', 'rain', 'wind'), ('cloud', 'qvapor', 'rain', 'winu'), ('cloud', 'qvapor', 'rain', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind'), ('cloud', 'qvapor', 'sst', 'winu'), ('cloud', 'qvapor', 'sst', 'wn10'), ('cloud', 'qvapor', 'wind', 'winu'), ('cloud', 'qvapor', 'wind', 'wn10'), ('cloud', 'qvapor', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind'), ('cloud', 'rain', 'sst', 'winu'), ('cloud', 'rain', 'sst', 'wn10'), ('cloud', 'rain', 'wind', 'winu'), ('cloud', 'rain', 'wind', 'wn10'), ('cloud', 'rain', 'winu', 'wn10'), ('cloud', 'sst', 'wind', 'winu'), ('cloud', 'sst', 'wind', 'wn10'), ('cloud', 'sst', 'winu', 'wn10'), ('cloud', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst'), ('psf', 'qvapor', 'rain', 'wind'), ('psf', 'qvapor', 'rain', 'winu'), ('psf', 'qvapor', 'rain', 'wn10'), ('psf', 'qvapor', 'sst', 'wind'), ('psf', 'qvapor', 'sst', 'winu'), ('psf', 'qvapor', 'sst', 'wn10'), ('psf', 'qvapor', 'wind', 'winu'), ('psf', 'qvapor', 'wind', 'wn10'), ('psf', 'qvapor', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind'), ('psf', 'rain', 'sst', 'winu'), ('psf', 'rain', 'sst', 'wn10'), ('psf', 'rain', 'wind', 'winu'), ('psf', 'rain', 'wind', 'wn10'), ('psf', 'rain', 'winu', 'wn10'), ('psf', 'sst', 'wind', 'winu'), ('psf', 'sst', 'wind', 'wn10'), ('psf', 'sst', 'winu', 'wn10'), ('psf', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind'), ('qvapor', 'rain', 'sst', 'winu'), ('qvapor', 'rain', 'sst', 'wn10'), ('qvapor', 'rain', 'wind', 'winu'), ('qvapor', 'rain', 'wind', 'wn10'), ('qvapor', 'rain', 'winu', 'wn10'), ('qvapor', 'sst', 'wind', 'winu'), ('qvapor', 'sst', 'wind', 'wn10'), ('qvapor', 'sst', 'winu', 'wn10'), ('qvapor', 'wind', 'winu', 'wn10'), ('rain', 'sst', 'wind', 'winu'), ('rain', 'sst', 'wind', 'wn10'), ('rain', 'sst', 'winu', 'wn10'), ('rain', 'wind', 'winu', 'wn10'), ('sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:56]\n",
       "        elif sequence == 8:\n",
       "            usedDatas = usedDatas[56:63]\n",
       "        elif sequence == 9:\n",
       "            usedDatas = usedDatas[63:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            try:\n",
       "                try:\n",
       "                    with h5py.File('dataset/{}{}{}{}{}.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'r') as f:\n",
       "                        tryToRead = f['datas'][()]\n",
       "                        print('Already exists, shape : {}'.format(tryToRead.shape))\n",
       "                except Exception:\n",
       "                    with h5py.File('dataset/{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'r') as f:\n",
       "                        tryToRead = f['datas'][()]\n",
       "                        print('Already exists, shape : {}'.format(tryToRead.shape))\n",
       "            except Exception:\n",
       "                crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "                flattened = preparePrediction(crop, grayscale=True)\n",
       "                with h5py.File('dataset/{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim), 'w') as f:\n",
       "                    f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 27: # Manggarai penta data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst'), ('cloud', 'psf', 'qvapor', 'rain', 'wind'), ('cloud', 'psf', 'qvapor', 'rain', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind'), ('cloud', 'psf', 'qvapor', 'sst', 'winu'), ('cloud', 'psf', 'qvapor', 'sst', 'wn10'), ('cloud', 'psf', 'qvapor', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind'), ('cloud', 'psf', 'rain', 'sst', 'winu'), ('cloud', 'psf', 'rain', 'sst', 'wn10'), ('cloud', 'psf', 'rain', 'wind', 'winu'), ('cloud', 'psf', 'rain', 'wind', 'wn10'), ('cloud', 'psf', 'rain', 'winu', 'wn10'), ('cloud', 'psf', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind'), ('cloud', 'qvapor', 'rain', 'sst', 'winu'), ('cloud', 'qvapor', 'rain', 'sst', 'wn10'), ('cloud', 'qvapor', 'rain', 'wind', 'winu'), ('cloud', 'qvapor', 'rain', 'wind', 'wn10'), ('cloud', 'qvapor', 'rain', 'winu', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind', 'winu'), ('cloud', 'qvapor', 'sst', 'wind', 'wn10'), ('cloud', 'qvapor', 'sst', 'winu', 'wn10'), ('cloud', 'qvapor', 'wind', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind'), ('psf', 'qvapor', 'rain', 'sst', 'winu'), ('psf', 'qvapor', 'rain', 'sst', 'wn10'), ('psf', 'qvapor', 'rain', 'wind', 'winu'), ('psf', 'qvapor', 'rain', 'wind', 'wn10'), ('psf', 'qvapor', 'rain', 'winu', 'wn10'), ('psf', 'qvapor', 'sst', 'wind', 'winu'), ('psf', 'qvapor', 'sst', 'wind', 'wn10'), ('psf', 'qvapor', 'sst', 'winu', 'wn10'), ('psf', 'qvapor', 'wind', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind', 'winu'), ('psf', 'rain', 'sst', 'wind', 'wn10'), ('psf', 'rain', 'sst', 'winu', 'wn10'), ('psf', 'rain', 'wind', 'winu', 'wn10'), ('psf', 'sst', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind', 'winu'), ('qvapor', 'rain', 'sst', 'wind', 'wn10'), ('qvapor', 'rain', 'sst', 'winu', 'wn10'), ('qvapor', 'rain', 'wind', 'winu', 'wn10'), ('qvapor', 'sst', 'wind', 'winu', 'wn10'), ('rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:28]\n",
       "        elif sequence == 4:\n",
       "            usedDatas = usedDatas[28:35]\n",
       "        elif sequence == 5:\n",
       "            usedDatas = usedDatas[35:42]\n",
       "        elif sequence == 6:\n",
       "            usedDatas = usedDatas[42:49]\n",
       "        elif sequence == 7:\n",
       "            usedDatas = usedDatas[49:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 28: # Manggarai hexa data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:7]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[7:14]\n",
       "        elif sequence == 2:\n",
       "            usedDatas = usedDatas[14:21]\n",
       "        elif sequence == 3:\n",
       "            usedDatas = usedDatas[21:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 29: # Manggarai hepta data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'rain', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'qvapor', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'psf', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('cloud', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'), ('psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10')]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    elif combination == 30: # Manggarai octa data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='manggarai')\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}{}{}f.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], usedData[7], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    # FLAGGED DATA\n",
       "    elif combination == 31: # Katulampa octa data\n",
       "        usedDatas = [('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa', flagged=True)\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/{}{}{}{}{}{}{}{}{}f!.hdf5'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], usedData[7], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    # Native sadewa Data\n",
       "    elif combination == 32: # Native sadewa data, save to individual dataset\n",
       "        usedDatas = [['cloud'],['psf'],['qvapor'],['rain'],['sst'],['wind'],['winu'],['wn10']]\n",
       "\n",
       "        if sequence == 0:\n",
       "            usedDatas = usedDatas[:4]\n",
       "        elif sequence == 1:\n",
       "            usedDatas = usedDatas[4:]\n",
       "\n",
       "        for usedData in usedDatas:\n",
       "            crop = cropImagePredictionData(dim, usedDatas=usedData, wlstation='katulampa', flagged=False, nativeSadewa=True)\n",
       "            flattened = preparePrediction(crop, grayscale=True)\n",
       "            with h5py.File('dataset/master_{}{}f.hdf5'.format(usedData[0], dim), 'w') as f:\n",
       "                f.create_dataset('datas', data=flattened, compression='gzip', compression_opts=7)\n",
       "\n",
       "    \n",
       "\n",
       "'''\n",
       "Available input :\n",
       "o45\n",
       "o100\n",
       "o196\n",
       "o198\n",
       "o400\n",
       "p72\n",
       "p100\n",
       "p196\n",
       "p400\n",
       "p402\n",
       "indvPred72\n",
       "predComb2-72-0\n",
       "predComb2-72-1\n",
       "predComb2-72-2\n",
       "predComb2-72-3\n",
       "predComb3-72-0\n",
       "predComb3-72-1\n",
       "predComb3-72-2\n",
       "predComb3-72-3\n",
       "predComb3-72-4\n",
       "wind72\n",
       "\n",
       "as string\n",
       "'''\n",
       "\n",
       "opt = input('Enter data fetch option : ')\n",
       "\n",
       "if opt == 'o45':\n",
       "    crop45 = cropImageObservationData(45)\n",
       "    with h5py.File('observation45.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop45)\n",
       "elif opt == 'o100':\n",
       "    crop100 = cropImageObservationData(100)\n",
       "    with h5py.File('observation100.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop100)\n",
       "elif opt == 'o196':\n",
       "    crop196 = cropImageObservationData(196)\n",
       "    with h5py.File('observation196.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop196)\n",
       "elif opt == 'o198':\n",
       "    crop198 = cropImageObservationData(198)\n",
       "    with h5py.File('observation196v2.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop198)\n",
       "elif opt == 'o400':\n",
       "    crop400 = cropImageObservationData(400)\n",
       "    with h5py.File('observation400.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop400)\n",
       "elif opt == 'p72':\n",
       "    crop72 = cropImagePredictionData(72)\n",
       "    with h5py.File('prediction72.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop72)\n",
       "elif opt == 'p100':\n",
       "    crop100 = cropImagePredictionData(100)\n",
       "    with h5py.File('prediction100.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop100)\n",
       "elif opt == 'p196':\n",
       "    crop196 = cropImagePredictionData(196)\n",
       "    with h5py.File('prediction196.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop196)\n",
       "elif opt == 'p400':\n",
       "    crop400 = cropImagePredictionData(400)\n",
       "    with h5py.File('prediction400.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop400)\n",
       "elif opt == 'p402':\n",
       "    crop402 = cropImagePredictionData(402)\n",
       "    with h5py.File('prediction400v2.hdf5', 'w') as f:\n",
       "        f.create_dataset('datas', data=crop402)\n",
       "elif opt == 'indvPred72':\n",
       "    performIndividualCropPredictionData(72)\n",
       "elif opt == 'predComb2-72-0':\n",
       "    performIndividualCropPredictionData(72, 0)\n",
       "elif opt == 'predComb2-72-1':\n",
       "    performIndividualCropPredictionData(72, 1)\n",
       "elif opt == 'predComb2-72-2':\n",
       "    performIndividualCropPredictionData(72, 2)\n",
       "elif opt == 'predComb2-72-3':\n",
       "    performIndividualCropPredictionData(72, 3)\n",
       "elif opt == 'predComb3-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 3)\n",
       "elif opt == 'predComb3-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 3)\n",
       "elif opt == 'predComb3-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 3)\n",
       "elif opt == 'predComb3-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 3)\n",
       "elif opt == 'predComb3-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 3)\n",
       "elif opt == 'wind72':\n",
       "    performIndividualCropPredictionData(72, None, 4)\n",
       "\n",
       "elif opt == 'predComb5-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 5)\n",
       "elif opt == 'predComb5-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 5)\n",
       "elif opt == 'predComb5-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 5)\n",
       "elif opt == 'predComb5-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 5)   \n",
       "\n",
       "elif opt == 'predComb6-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 6)\n",
       "elif opt == 'predComb6-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 6)\n",
       "elif opt == 'predComb6-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 6)\n",
       "elif opt == 'predComb6-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 6)   \n",
       "elif opt == 'predComb6-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 6)  \n",
       "\n",
       "elif opt == 'predComb7-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 7)\n",
       "elif opt == 'predComb7-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 7)\n",
       "elif opt == 'predComb7-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 7)\n",
       "elif opt == 'predComb7-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 7)   \n",
       "elif opt == 'predComb7-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 7)  \n",
       "elif opt == 'predComb7-72-5':\n",
       "    performIndividualCropPredictionData(72, 5, 7)  \n",
       "\n",
       "elif opt == 'predComb8-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 8)  \n",
       "elif opt == 'predComb8-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 8)  \n",
       "\n",
       "elif opt == 'predComb9-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 9)  \n",
       "elif opt == 'predComb9-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 9)  \n",
       "\n",
       "elif opt == 'predComb10-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 10)  \n",
       "elif opt == 'predComb10-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 10)  \n",
       "elif opt == 'predComb10-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 10)  \n",
       "elif opt == 'predComb10-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 10)  \n",
       "elif opt == 'predComb10-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 10)  \n",
       "elif opt == 'predComb10-240-5':\n",
       "    performIndividualCropPredictionData(240, 5, 10)  \n",
       "elif opt == 'predComb10-240-6':\n",
       "    performIndividualCropPredictionData(240, 6, 10)  \n",
       "elif opt == 'predComb10-240-7':\n",
       "    performIndividualCropPredictionData(240, 7, 10)  \n",
       "elif opt == 'predComb10-240-8':\n",
       "    performIndividualCropPredictionData(240, 8, 10)  \n",
       "\n",
       "elif opt == 'predComb6-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 6)\n",
       "elif opt == 'predComb6-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 6)\n",
       "elif opt == 'predComb6-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 6)\n",
       "elif opt == 'predComb6-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 6)   \n",
       "elif opt == 'predComb6-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 6)  \n",
       "\n",
       "elif opt == 'predComb5-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 5)\n",
       "elif opt == 'predComb5-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 5)\n",
       "elif opt == 'predComb5-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 5)\n",
       "elif opt == 'predComb5-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 5) \n",
       "\n",
       "elif opt == 'predComb2-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 2)\n",
       "elif opt == 'predComb2-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 2)\n",
       "elif opt == 'predComb2-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 2)\n",
       "elif opt == 'predComb2-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 2)\n",
       "\n",
       "elif opt == 'predComb3-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 3)\n",
       "elif opt == 'predComb3-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 3)\n",
       "elif opt == 'predComb3-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 3)\n",
       "elif opt == 'predComb3-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 3)\n",
       "elif opt == 'predComb3-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 3)\n",
       "\n",
       "elif opt == 'indvPred240':\n",
       "    performIndividualCropPredictionData(240, None, 1)\n",
       "\n",
       "elif opt == 'predCom7-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 7)\n",
       "elif opt == 'predComb7-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 7)\n",
       "elif opt == 'predComb7-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 7)\n",
       "elif opt == 'predComb7-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 7)   \n",
       "elif opt == 'predComb7-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 7)  \n",
       "elif opt == 'predComb7-240-5':\n",
       "    performIndividualCropPredictionData(240, 5, 7)  \n",
       "\n",
       "elif opt == 'predComb8-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 8)  \n",
       "elif opt == 'predComb8-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 8) \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "elif opt == 'predComb2-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 2)\n",
       "elif opt == 'predComb2-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 2)\n",
       "elif opt == 'predComb2-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 2)\n",
       "elif opt == 'predComb2-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 2)\n",
       "\n",
       "elif opt == 'predComb3-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 3)\n",
       "elif opt == 'predComb3-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 3)\n",
       "elif opt == 'predComb3-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 3)\n",
       "elif opt == 'predComb3-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 3)\n",
       "elif opt == 'predComb3-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 3)\n",
       "\n",
       "elif opt == 'indvPred400':\n",
       "    performIndividualCropPredictionData(400, None, 1)\n",
       "\n",
       "elif opt == 'predCom7-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 7)\n",
       "elif opt == 'predComb7-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 7)\n",
       "elif opt == 'predComb7-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 7)\n",
       "elif opt == 'predComb7-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 7)   \n",
       "elif opt == 'predComb7-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 7)  \n",
       "elif opt == 'predComb7-400-5':\n",
       "    performIndividualCropPredictionData(400, 5, 7)  \n",
       "\n",
       "elif opt == 'predComb8-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 8)  \n",
       "elif opt == 'predComb8-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 8) \n",
       "\n",
       "\n",
       "elif opt == 'predComb11-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 11)  \n",
       "elif opt == 'predComb11-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 11)  \n",
       "elif opt == 'predComb11-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 11)  \n",
       "elif opt == 'predComb11-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 11)  \n",
       "elif opt == 'predComb11-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 11)  \n",
       "elif opt == 'predComb11-400-5':\n",
       "    performIndividualCropPredictionData(400, 5, 11)  \n",
       "elif opt == 'predComb11-400-6':\n",
       "    performIndividualCropPredictionData(400, 6, 11)  \n",
       "elif opt == 'predComb11-400-7':\n",
       "    performIndividualCropPredictionData(400, 7, 11)  \n",
       "elif opt == 'predComb11-400-8':\n",
       "    performIndividualCropPredictionData(400, 8, 11) \n",
       "\n",
       "\n",
       "elif opt == 'predComb12-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 12)  \n",
       "elif opt == 'predComb12-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 12) \n",
       "\n",
       "elif opt == 'predComb13-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 13)\n",
       "elif opt == 'predComb13-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 13)\n",
       "elif opt == 'predComb13-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 13)\n",
       "elif opt == 'predComb13-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 13)\n",
       "elif opt == 'predComb13-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 13)\n",
       "\n",
       "elif opt == 'predComb13-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 13)\n",
       "elif opt == 'predComb13-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 13)\n",
       "elif opt == 'predComb13-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 13)\n",
       "elif opt == 'predComb13-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 13)\n",
       "elif opt == 'predComb13-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 13)\n",
       "\n",
       "# single data\n",
       "elif opt == 'predComb14-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 14)\n",
       "elif opt == 'predComb14-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 14)\n",
       "# double data\n",
       "elif opt == 'predComb15-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 15)\n",
       "elif opt == 'predComb15-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 15)\n",
       "elif opt == 'predComb15-16-2':\n",
       "    performIndividualCropPredictionData(16, 2, 15)\n",
       "elif opt == 'predComb15-16-3':\n",
       "    performIndividualCropPredictionData(16, 3, 15)\n",
       "# triple data\n",
       "elif opt == 'predComb16-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 16)\n",
       "elif opt == 'predComb16-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 16)\n",
       "elif opt == 'predComb16-16-2':\n",
       "    performIndividualCropPredictionData(16, 2, 16)\n",
       "elif opt == 'predComb16-16-3':\n",
       "    performIndividualCropPredictionData(16, 3, 16)\n",
       "elif opt == 'predComb16-16-4':\n",
       "    performIndividualCropPredictionData(16, 4, 16)\n",
       "elif opt == 'predComb16-16-5':\n",
       "    performIndividualCropPredictionData(16, 5, 16)\n",
       "elif opt == 'predComb16-16-6':\n",
       "    performIndividualCropPredictionData(16, 6, 16)\n",
       "elif opt == 'predComb16-16-7':\n",
       "    performIndividualCropPredictionData(16, 7, 16)\n",
       "# tetra data\n",
       "elif opt == 'predComb17-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 17)\n",
       "elif opt == 'predComb17-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 17)\n",
       "elif opt == 'predComb17-16-2':\n",
       "    performIndividualCropPredictionData(16, 2, 17)\n",
       "elif opt == 'predComb17-16-3':\n",
       "    performIndividualCropPredictionData(16, 3, 17)\n",
       "elif opt == 'predComb17-16-4':\n",
       "    performIndividualCropPredictionData(16, 4, 17)\n",
       "elif opt == 'predComb17-16-5':\n",
       "    performIndividualCropPredictionData(16, 5, 17)\n",
       "elif opt == 'predComb17-16-6':\n",
       "    performIndividualCropPredictionData(16, 6, 17)\n",
       "elif opt == 'predComb17-16-7':\n",
       "    performIndividualCropPredictionData(16, 7, 17)\n",
       "elif opt == 'predComb17-16-8':\n",
       "    performIndividualCropPredictionData(16, 8, 17)\n",
       "elif opt == 'predComb17-16-9':\n",
       "    performIndividualCropPredictionData(16, 9, 17)\n",
       "# penta data\n",
       "elif opt == 'predComb18-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 18)\n",
       "elif opt == 'predComb18-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 18)\n",
       "elif opt == 'predComb18-16-2':\n",
       "    performIndividualCropPredictionData(16, 2, 18)\n",
       "elif opt == 'predComb18-16-3':\n",
       "    performIndividualCropPredictionData(16, 3, 18)\n",
       "elif opt == 'predComb18-16-4':\n",
       "    performIndividualCropPredictionData(16, 4, 18)\n",
       "elif opt == 'predComb18-16-5':\n",
       "    performIndividualCropPredictionData(16, 5, 18)\n",
       "elif opt == 'predComb18-16-6':\n",
       "    performIndividualCropPredictionData(16, 6, 18)\n",
       "elif opt == 'predComb18-16-7':\n",
       "    performIndividualCropPredictionData(16, 7, 18)\n",
       "# hexa data\n",
       "elif opt == 'predComb20-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 20)\n",
       "elif opt == 'predComb20-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 20)\n",
       "elif opt == 'predComb20-16-2':\n",
       "    performIndividualCropPredictionData(16, 2, 20)\n",
       "elif opt == 'predComb20-16-3':\n",
       "    performIndividualCropPredictionData(16, 3, 20)\n",
       "# hepta data\n",
       "elif opt == 'predComb21-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 21)\n",
       "elif opt == 'predComb21-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 21)\n",
       "# octa data\n",
       "elif opt == 'predComb22-16':\n",
       "    performIndividualCropPredictionData(16, None, 22)\n",
       "# missing katulampa data for RNN restacking\n",
       "elif opt == 'predComb23-16':\n",
       "    performIndividualCropPredictionData(16, None, 23)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "elif opt == 'predComb14-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 14)\n",
       "elif opt == 'predComb14-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 14)\n",
       "# double\n",
       "elif opt == 'predComb15-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 15)\n",
       "elif opt == 'predComb15-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 15)\n",
       "elif opt == 'predComb15-784-2':\n",
       "    performIndividualCropPredictionData(784, 2, 15)\n",
       "elif opt == 'predComb15-784-3':\n",
       "    performIndividualCropPredictionData(784, 3, 15)\n",
       "# triple data\n",
       "elif opt == 'predComb16-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 16)\n",
       "elif opt == 'predComb16-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 16)\n",
       "elif opt == 'predComb16-784-2':\n",
       "    performIndividualCropPredictionData(784, 2, 16)\n",
       "elif opt == 'predComb16-784-3':\n",
       "    performIndividualCropPredictionData(784, 3, 16)\n",
       "elif opt == 'predComb16-784-4':\n",
       "    performIndividualCropPredictionData(784, 4, 16)\n",
       "elif opt == 'predComb16-784-5':\n",
       "    performIndividualCropPredictionData(784, 5, 16)\n",
       "elif opt == 'predComb16-784-6':\n",
       "    performIndividualCropPredictionData(784, 6, 16)\n",
       "elif opt == 'predComb16-784-7':\n",
       "    performIndividualCropPredictionData(784, 7, 16)\n",
       "# tetra data\n",
       "elif opt == 'predComb17-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 17)\n",
       "elif opt == 'predComb17-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 17)\n",
       "elif opt == 'predComb17-784-2':\n",
       "    performIndividualCropPredictionData(784, 2, 17)\n",
       "elif opt == 'predComb17-784-3':\n",
       "    performIndividualCropPredictionData(784, 3, 17)\n",
       "elif opt == 'predComb17-784-4':\n",
       "    performIndividualCropPredictionData(784, 4, 17)\n",
       "elif opt == 'predComb17-784-5':\n",
       "    performIndividualCropPredictionData(784, 5, 17)\n",
       "elif opt == 'predComb17-784-6':\n",
       "    performIndividualCropPredictionData(784, 6, 17)\n",
       "elif opt == 'predComb17-784-7':\n",
       "    performIndividualCropPredictionData(784, 7, 17)\n",
       "elif opt == 'predComb17-784-8':\n",
       "    performIndividualCropPredictionData(784, 8, 17)\n",
       "elif opt == 'predComb17-784-9':\n",
       "    performIndividualCropPredictionData(784, 9, 17)\n",
       "# penta data\n",
       "elif opt == 'predComb18-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 18)\n",
       "elif opt == 'predComb18-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 18)\n",
       "elif opt == 'predComb18-784-2':\n",
       "    performIndividualCropPredictionData(784, 2, 18)\n",
       "elif opt == 'predComb18-784-3':\n",
       "    performIndividualCropPredictionData(784, 3, 18)\n",
       "elif opt == 'predComb18-784-4':\n",
       "    performIndividualCropPredictionData(784, 4, 18)\n",
       "elif opt == 'predComb18-784-5':\n",
       "    performIndividualCropPredictionData(784, 5, 18)\n",
       "elif opt == 'predComb18-784-6':\n",
       "    performIndividualCropPredictionData(784, 6, 18)\n",
       "elif opt == 'predComb18-784-7':\n",
       "    performIndividualCropPredictionData(784, 7, 18)\n",
       "elif opt == 'predComb18-784-699':\n",
       "    performIndividualCropPredictionData(784, 699, 18)\n",
       "# hexa data\n",
       "elif opt == 'predComb20-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 20)\n",
       "elif opt == 'predComb20-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 20)\n",
       "elif opt == 'predComb20-784-2':\n",
       "    performIndividualCropPredictionData(784, 2, 20)\n",
       "elif opt == 'predComb20-784-3':\n",
       "    performIndividualCropPredictionData(784, 3, 20)\n",
       "# hepta data\n",
       "elif opt == 'predComb21-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 21)\n",
       "elif opt == 'predComb21-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 21)\n",
       "# octa data\n",
       "elif opt == 'predComb22-784':\n",
       "    performIndividualCropPredictionData(784, None, 22)\n",
       "\n",
       "# RNN \n",
       "elif opt == 'rnn19-72':\n",
       "    performIndividualCropPredictionData(72, 0, 19)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "#Manggarai recreating double-octa data from 72,240,to 400 dimensions\n",
       "# 72 DIMENSIONS\n",
       "# double\n",
       "elif opt == '24-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 24)\n",
       "elif opt == '24-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 24)\n",
       "elif opt == '24-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 24)\n",
       "elif opt == '24-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 24)\n",
       "# triple data\n",
       "elif opt == '25-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 25)\n",
       "elif opt == '25-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 25)\n",
       "elif opt == '25-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 25)\n",
       "elif opt == '25-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 25)\n",
       "elif opt == '25-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 25)\n",
       "elif opt == '25-72-5':\n",
       "    performIndividualCropPredictionData(72, 5, 25)\n",
       "elif opt == '25-72-6':\n",
       "    performIndividualCropPredictionData(72, 6, 25)\n",
       "elif opt == '25-72-7':\n",
       "    performIndividualCropPredictionData(72, 7, 25)\n",
       "# tetra data\n",
       "elif opt == '26-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 26)\n",
       "elif opt == '26-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 26)\n",
       "elif opt == '26-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 26)\n",
       "elif opt == '26-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 26)\n",
       "elif opt == '26-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 26)\n",
       "elif opt == '26-72-5':\n",
       "    performIndividualCropPredictionData(72, 5, 26)\n",
       "elif opt == '26-72-6':\n",
       "    performIndividualCropPredictionData(72, 6, 26)\n",
       "elif opt == '26-72-7':\n",
       "    performIndividualCropPredictionData(72, 7, 26)\n",
       "elif opt == '26-72-8':\n",
       "    performIndividualCropPredictionData(72, 8, 26)\n",
       "elif opt == '26-72-9':\n",
       "    performIndividualCropPredictionData(72, 9, 26)\n",
       "# penta data\n",
       "elif opt == '27-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 27)\n",
       "elif opt == '27-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 27)\n",
       "elif opt == '27-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 27)\n",
       "elif opt == '27-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 27)\n",
       "elif opt == '27-72-4':\n",
       "    performIndividualCropPredictionData(72, 4, 27)\n",
       "elif opt == '27-72-5':\n",
       "    performIndividualCropPredictionData(72, 5, 27)\n",
       "elif opt == '27-72-6':\n",
       "    performIndividualCropPredictionData(72, 6, 27)\n",
       "elif opt == '27-72-7':\n",
       "    performIndividualCropPredictionData(72, 7, 27)\n",
       "# hexa data\n",
       "elif opt == '28-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 28)\n",
       "elif opt == '28-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 28)\n",
       "elif opt == '28-72-2':\n",
       "    performIndividualCropPredictionData(72, 2, 28)\n",
       "elif opt == '28-72-3':\n",
       "    performIndividualCropPredictionData(72, 3, 28)\n",
       "# hepta data\n",
       "elif opt == '29-72-0':\n",
       "    performIndividualCropPredictionData(72, 0, 29)\n",
       "elif opt == '29-72-1':\n",
       "    performIndividualCropPredictionData(72, 1, 29)\n",
       "# octa data\n",
       "elif opt == '30-72':\n",
       "    performIndividualCropPredictionData(72, None, 30)\n",
       "\n",
       "# 240 DIMENSIONS\n",
       "# double\n",
       "elif opt == '24-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 24)\n",
       "elif opt == '24-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 24)\n",
       "elif opt == '24-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 24)\n",
       "elif opt == '24-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 24)\n",
       "# triple data\n",
       "elif opt == '25-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 25)\n",
       "elif opt == '25-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 25)\n",
       "elif opt == '25-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 25)\n",
       "elif opt == '25-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 25)\n",
       "elif opt == '25-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 25)\n",
       "elif opt == '25-240-5':\n",
       "    performIndividualCropPredictionData(240, 5, 25)\n",
       "elif opt == '25-240-6':\n",
       "    performIndividualCropPredictionData(240, 6, 25)\n",
       "elif opt == '25-240-7':\n",
       "    performIndividualCropPredictionData(240, 7, 25)\n",
       "# tetra data\n",
       "elif opt == '26-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 26)\n",
       "elif opt == '26-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 26)\n",
       "elif opt == '26-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 26)\n",
       "elif opt == '26-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 26)\n",
       "elif opt == '26-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 26)\n",
       "elif opt == '26-240-5':\n",
       "    performIndividualCropPredictionData(240, 5, 26)\n",
       "elif opt == '26-240-6':\n",
       "    performIndividualCropPredictionData(240, 6, 26)\n",
       "elif opt == '26-240-7':\n",
       "    performIndividualCropPredictionData(240, 7, 26)\n",
       "elif opt == '26-240-8':\n",
       "    performIndividualCropPredictionData(240, 8, 26)\n",
       "elif opt == '26-240-9':\n",
       "    performIndividualCropPredictionData(240, 9, 26)\n",
       "# penta data\n",
       "elif opt == '27-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 27)\n",
       "elif opt == '27-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 27)\n",
       "elif opt == '27-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 27)\n",
       "elif opt == '27-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 27)\n",
       "elif opt == '27-240-4':\n",
       "    performIndividualCropPredictionData(240, 4, 27)\n",
       "elif opt == '27-240-5':\n",
       "    performIndividualCropPredictionData(240, 5, 27)\n",
       "elif opt == '27-240-6':\n",
       "    performIndividualCropPredictionData(240, 6, 27)\n",
       "elif opt == '27-240-7':\n",
       "    performIndividualCropPredictionData(240, 7, 27)\n",
       "# hexa data\n",
       "elif opt == '28-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 28)\n",
       "elif opt == '28-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 28)\n",
       "elif opt == '28-240-2':\n",
       "    performIndividualCropPredictionData(240, 2, 28)\n",
       "elif opt == '28-240-3':\n",
       "    performIndividualCropPredictionData(240, 3, 28)\n",
       "# hepta data\n",
       "elif opt == '29-240-0':\n",
       "    performIndividualCropPredictionData(240, 0, 29)\n",
       "elif opt == '29-240-1':\n",
       "    performIndividualCropPredictionData(240, 1, 29)\n",
       "# octa data\n",
       "elif opt == '30-240':\n",
       "    performIndividualCropPredictionData(240, None, 30)\n",
       "\n",
       "# 400 DIMENSIONS\n",
       "# double\n",
       "elif opt == '24-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 24)\n",
       "elif opt == '24-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 24)\n",
       "elif opt == '24-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 24)\n",
       "elif opt == '24-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 24)\n",
       "# triple data\n",
       "elif opt == '25-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 25)\n",
       "elif opt == '25-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 25)\n",
       "elif opt == '25-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 25)\n",
       "elif opt == '25-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 25)\n",
       "elif opt == '25-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 25)\n",
       "elif opt == '25-400-5':\n",
       "    performIndividualCropPredictionData(400, 5, 25)\n",
       "elif opt == '25-400-6':\n",
       "    performIndividualCropPredictionData(400, 6, 25)\n",
       "elif opt == '25-400-7':\n",
       "    performIndividualCropPredictionData(400, 7, 25)\n",
       "# tetra data\n",
       "elif opt == '26-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 26)\n",
       "elif opt == '26-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 26)\n",
       "elif opt == '26-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 26)\n",
       "elif opt == '26-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 26)\n",
       "elif opt == '26-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 26)\n",
       "elif opt == '26-400-5':\n",
       "    performIndividualCropPredictionData(400, 5, 26)\n",
       "elif opt == '26-400-6':\n",
       "    performIndividualCropPredictionData(400, 6, 26)\n",
       "elif opt == '26-400-7':\n",
       "    performIndividualCropPredictionData(400, 7, 26)\n",
       "elif opt == '26-400-8':\n",
       "    performIndividualCropPredictionData(400, 8, 26)\n",
       "elif opt == '26-400-9':\n",
       "    performIndividualCropPredictionData(400, 9, 26)\n",
       "# penta data\n",
       "elif opt == '27-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 27)\n",
       "elif opt == '27-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 27)\n",
       "elif opt == '27-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 27)\n",
       "elif opt == '27-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 27)\n",
       "elif opt == '27-400-4':\n",
       "    performIndividualCropPredictionData(400, 4, 27)\n",
       "elif opt == '27-400-5':\n",
       "    performIndividualCropPredictionData(400, 5, 27)\n",
       "elif opt == '27-400-6':\n",
       "    performIndividualCropPredictionData(400, 6, 27)\n",
       "elif opt == '27-400-7':\n",
       "    performIndividualCropPredictionData(400, 7, 27)\n",
       "# hexa data\n",
       "elif opt == '28-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 28)\n",
       "elif opt == '28-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 28)\n",
       "elif opt == '28-400-2':\n",
       "    performIndividualCropPredictionData(400, 2, 28)\n",
       "elif opt == '28-400-3':\n",
       "    performIndividualCropPredictionData(400, 3, 28)\n",
       "# hepta data\n",
       "elif opt == '29-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 29)\n",
       "elif opt == '29-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 29)\n",
       "# octa data\n",
       "elif opt == '30-400':\n",
       "    performIndividualCropPredictionData(400, None, 30)\n",
       "\n",
       "\n",
       "\n",
       "# Katulampa Flagged Data\n",
       "# octa data\n",
       "elif opt == 'predComb31-784':\n",
       "    performIndividualCropPredictionData(784, None, 31)\n",
       "elif opt == 'predComb31-16':\n",
       "    performIndividualCropPredictionData(16, None, 31)\n",
       "\n",
       "\n",
       "# Native Sadewa\n",
       "elif opt == '32-16-0':\n",
       "    performIndividualCropPredictionData(16, 0, 32)\n",
       "elif opt == '32-16-1':\n",
       "    performIndividualCropPredictionData(16, 1, 32)\n",
       "\n",
       "elif opt == '32-64-0':\n",
       "    performIndividualCropPredictionData(64, 0, 32)\n",
       "elif opt == '32-64-1':\n",
       "    performIndividualCropPredictionData(64, 1, 32)\n",
       "\n",
       "elif opt == '32-144-0':\n",
       "    performIndividualCropPredictionData(144, 0, 32)\n",
       "elif opt == '32-144-1':\n",
       "    performIndividualCropPredictionData(144, 1, 32)\n",
       "\n",
       "elif opt == '32-256-0':\n",
       "    performIndividualCropPredictionData(256, 0, 32)\n",
       "elif opt == '32-256-1':\n",
       "    performIndividualCropPredictionData(256, 1, 32)\n",
       "\n",
       "elif opt == '32-400-0':\n",
       "    performIndividualCropPredictionData(400, 0, 32)\n",
       "elif opt == '32-400-1':\n",
       "    performIndividualCropPredictionData(400, 1, 32)\n",
       "\n",
       "elif opt == '32-576-0':\n",
       "    performIndividualCropPredictionData(576, 0, 32)\n",
       "elif opt == '32-576-1':\n",
       "    performIndividualCropPredictionData(576, 1, 32)\n",
       "\n",
       "elif opt == '32-784-0':\n",
       "    performIndividualCropPredictionData(784, 0, 32)\n",
       "elif opt == '32-784-1':\n",
       "    performIndividualCropPredictionData(784, 1, 32)\n",
       "\n",
       "elif opt == '32-1024-0':\n",
       "    performIndividualCropPredictionData(1024, 0, 32)\n",
       "elif opt == '32-1024-1':\n",
       "    performIndividualCropPredictionData(1024, 1, 32)\n",
       "\n",
       "elif opt == '32-1296-0':\n",
       "    performIndividualCropPredictionData(1296, 0, 32)\n",
       "elif opt == '32-1296-1':\n",
       "    performIndividualCropPredictionData(1296, 1, 32)\n",
       "\n",
       "elif opt == '32-1600-0':\n",
       "    performIndividualCropPredictionData(1600, 0, 32)\n",
       "elif opt == '32-1600-1':\n",
       "    performIndividualCropPredictionData(1600, 1, 32)\n",
       "```\n",
       "\n",
       "## Creating Master Raw-unstacked Datasets for RNN\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "## Restacking Simple RNN Input Datasets\n",
       "\n",
       "\n",
       "```python\n",
       "# import modules\n",
       "import os\n",
       "import sqlite3\n",
       "from sqlite3 import Error\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from skimage import color\n",
       "import h5py\n",
       "import time\n",
       "import datetime\n",
       "import copy\n",
       "\n",
       "\n",
       "def create_connection(db_file):\n",
       "    '''\n",
       "    create a database connection to a SQLite database\n",
       "    specified by db_file\n",
       "    :param db_file : database file\n",
       "    :return: Connection Object or None\n",
       "    '''\n",
       "    conn=None\n",
       "    try:\n",
       "        conn=sqlite3.connect(db_file)\n",
       "        return conn\n",
       "    except Error as e:\n",
       "        print(e)  \n",
       "\n",
       "def manggaraiDataList(maxData=True, hourOffset=0, wlstation='manggarai'):\n",
       "    '''\n",
       "    Returning a tuple of list (date, data) of manggarai TMA data with 10-minutes-interval from DSDA dataset in year 2020\n",
       "    '''\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM {}'.format(wlstation), conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "\n",
       "    # slicing data to 2020 timeframe\n",
       "    #mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00') & (manggaraiConv['currentdate'] <= '2021-04-03 23:50')\n",
       "    mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00')\n",
       "    manggaraiSlice2020=manggaraiConv.loc[mask]\n",
       "\n",
       "    # converting 10-minute-data to hourly data\n",
       "    startDate=datetime.datetime(2019,2,1)\n",
       "    minutes=[x*10 for x in range(6)]\n",
       "    hours=[x for x in range(24)]\n",
       "    days=[x for x in range(780)]\n",
       "\n",
       "    dateListHourly=[]\n",
       "    dataListHourly=[]\n",
       "    for day in days:\n",
       "        for hour in hours:\n",
       "            hourlyData=[]\n",
       "\n",
       "            # set error indicator back to false\n",
       "            error=False\n",
       "\n",
       "            for minute in minutes:\n",
       "                # perform data fetch, add to list, and get max value\n",
       "                dateLoop=startDate+datetime.timedelta(days=day, hours=hour+hourOffset, minutes=minute)\n",
       "                rowFetch=manggaraiSlice2020.loc[(manggaraiSlice2020['currentdate'] == dateLoop)]\n",
       "                #print(rowFetch)\n",
       "\n",
       "                # try to fetch if the result is not zero\n",
       "                try:\n",
       "                    dataFetch=rowFetch['data'].item()\n",
       "                    hourlyData.append(dataFetch)\n",
       "                except ValueError:\n",
       "                    error=True\n",
       "\n",
       "            # insert data if error indicator is False\n",
       "            if not error:\n",
       "                # make hourly date using timedelta\n",
       "                hourlyDate=startDate+datetime.timedelta(days=day, hours=hour)\n",
       "                \n",
       "                if maxData:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=max(hourlyData)\n",
       "                else:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=hourlyData.mean()\n",
       "\n",
       "                # insert value to global list\n",
       "                dateListHourly.append(hourlyDate)\n",
       "                dataListHourly.append(maxDataHourly)\n",
       "            else: # if error occured during data fetch (null or something else)\n",
       "                continue # to next loop\n",
       "    return dateListHourly, dataListHourly\n",
       "\n",
       "def getHimawariFilename():\n",
       "    '''\n",
       "    Return dictionary of available himawari data based on filename inside\n",
       "    folder as a key\n",
       "    '''\n",
       "    himawariPath='../mining_sadewa/sadewa/'\n",
       "    # load folder name\n",
       "    directory=[directory for directory in os.listdir(himawariPath)]\n",
       "\n",
       "    # store fileame\n",
       "    himawari={}\n",
       "\n",
       "    # load all filename stored on disk to dictionary with each folder name as keys\n",
       "    for direct in directory:\n",
       "        fpath='{}{}'.format(himawariPath, direct)\n",
       "        himawari[direct]=[fname for fname in os.listdir(fpath)]\n",
       "        \n",
       "    return himawari\n",
       "\n",
       "def extractHimawariDatetime():\n",
       "    '''\n",
       "    Extract every filename in sadewa-himawari data to datetime object for easier handling\n",
       "    \n",
       "    Returns :\n",
       "    extractedDate -- dictionary containing list of datetime object for each filename inside dictionary keys for every data\n",
       "    '''\n",
       "    himawari=getHimawariFilename()\n",
       "\n",
       "    # extract date for each himawari data type to datetime.datetime object\n",
       "    observations=['CCLD','B04','IR1','IR3','VIS']\n",
       "    extractedDate={}\n",
       "    for obs in observations:\n",
       "        extractedDate[obs]=[datetime.datetime.strptime(x.replace('H89_{}_'.format(obs),'').replace('.png',''), '%Y%m%d%H%M') for x in himawari[obs]]\n",
       "\n",
       "    predictions=['cloud','psf','qvapor','rain','sst','wind','winu','wn10']\n",
       "    for pred in predictions:\n",
       "        extractedDate[pred]=[datetime.datetime.strptime(x.replace('{}_'.format(pred),'').replace('.png','').replace('_','')+'00', '%Y%m%d%H%M') for x in himawari[pred]]\n",
       "        \n",
       "    return extractedDate\n",
       "\n",
       "def getAvailableSlicedData(maxData=True, hourOffset=0, dataScope='combination', wlstation='manggarai'):\n",
       "    '''\n",
       "    check through all available dataset, including manggarai TMA, sadewa-himawari IR1, IR3, VIS, B04, and CCLD\n",
       "    and return a tuple containing datetime object and manggarai hourly TMA data that are synced through all available dataset\n",
       "    \n",
       "    This function doesn't return sadewa-himawari data, because using the datetime format and the sadewa-himawari data types,\n",
       "    the full name of the file required can be constructed.\n",
       "    \n",
       "    return : (slicedDate, slicedData) # both are lists inside a tuple\n",
       "    '''\n",
       "    extractedDate = extractHimawariDatetime()\n",
       "        \n",
       "    # getting date-data slice from himawari and manggarai TMA data\n",
       "\n",
       "    # using function to get manggarai available date-data\n",
       "    dateListHourly, dataListHourly = manggaraiDataList(maxData, hourOffset, wlstation=wlstation)\n",
       "\n",
       "    # loop to every data\n",
       "    # check algorithm : manggarai checked against every himawari data, and if all true, date is inserted to sliced data\n",
       "    slicedDate=[]\n",
       "    slicedData=[]\n",
       "    for i in range(len(dateListHourly)):\n",
       "        \n",
       "        if dataScope == 'combination':\n",
       "            usedData=['CCLD','B04','IR1','IR3','VIS','rain','cloud','psf','qvapor','sst']\n",
       "        elif dataScope == 'prediction':\n",
       "            usedData=('cloud','psf','qvapor','rain','sst','wind','winu','wn10')\n",
       "\n",
       "        # defining control mechanism\n",
       "        checked=True\n",
       "\n",
       "        # loop through every himawari data\n",
       "        for used in usedData:\n",
       "            if dateListHourly[i] not in extractedDate[used]:\n",
       "                checked=False # set checked to False if there are no complementary data found in another dataset\n",
       "\n",
       "        # input data if all checked\n",
       "        if checked:\n",
       "            slicedDate.append(dateListHourly[i])\n",
       "            slicedData.append(dataListHourly[i])\n",
       "    return slicedDate, slicedData\n",
       "\n",
       "def preparePrediction(pred, grayscale=False):\n",
       "    # loop through all available data\n",
       "    firstData = True\n",
       "    for i in range(len(pred)):\n",
       "        # loop through dataset\n",
       "        firstDataset = True\n",
       "        for j in range(len(pred[i])):\n",
       "            if False:\n",
       "                continue\n",
       "            else :\n",
       "                # check if grayscale or not\n",
       "                if grayscale:\n",
       "                    img = color.rgb2gray(color.rgba2rgb(pred[i][j]))\n",
       "                    flat = img.reshape(pred[i][j].shape[0]*pred[i][j].shape[1])\n",
       "                else:\n",
       "                    img = pred[i][j]\n",
       "                    flat = pred[i][j].reshape(pred[i][j].shape[0]*pred[i][j].shape[1]*pred[i][j].shape[2])\n",
       "                \n",
       "                \n",
       "                if firstDataset:\n",
       "                    flattened = flat.copy()\n",
       "                    firstDataset = False\n",
       "                else :\n",
       "                    flattened = np.hstack((flattened, flat))\n",
       "        if firstData:\n",
       "            data = flattened.copy()\n",
       "            data = data.reshape(1, data.shape[0])\n",
       "            firstData = False\n",
       "        else :\n",
       "            flattened = flattened.reshape(1, flattened.shape[0])\n",
       "            data = np.vstack((data, flattened))\n",
       "    return data\n",
       "\n",
       "def generateRNNInput(adte, adta, recurrentCount=1):\n",
       "    '''\n",
       "    Check and return a tuple of date containing available data for recurrent configuration\n",
       "    \n",
       "    This is a sub-function to restack current cropped data into rnn enabled data based on recurrentCount number\n",
       "    \n",
       "    Return:\n",
       "    recurrentIndexList = [(index-2, index-1, index+0), (index-1, index+0, index+1), (index-recurrentCount+index, index-recurrentCount+1+index, index-recurrentCount+2+index), ...]\n",
       "    availableRecurrentDate = array like containing available date in recurrent configuration (in t=0)\n",
       "    availableRecurrentLabel = array like containing available data label in recurrent configuration\n",
       "    '''\n",
       "    \n",
       "    # defining start index\n",
       "    # defining list to store the recurrent index\n",
       "    recurrentIndexList = []\n",
       "    availableRecurrentDate = []\n",
       "    availableRecurrentLabel = []\n",
       "    for idx in range(len(adte[recurrentCount:])):\n",
       "        # check sequence\n",
       "        checkSeq = [adte[idx+recurrentCount]+datetime.timedelta(hours=-recurrentCount)+datetime.timedelta(hours=x) for x in range(recurrentCount+1)]\n",
       "        realSeq = [adte[idx+x] for x in range(recurrentCount+1)]\n",
       "        if checkSeq != realSeq:\n",
       "            continue\n",
       "        else:\n",
       "            recurrentIndexList.append([idx+x for x in range(recurrentCount+1)])\n",
       "            availableRecurrentDate.append(adte[idx+recurrentCount])\n",
       "            availableRecurrentLabel.append(adta[idx+recurrentCount])\n",
       "    \n",
       "    return recurrentIndexList, availableRecurrentDate, availableRecurrentLabel\n",
       "\n",
       "\n",
       "def restackRNNInput(recurrentIndexList, dataset, flattened=False, grayscale=True):\n",
       "    '''\n",
       "    Create a new datasets in rnn mode by passing recurrentIndexList and dataset that want to be restacked\n",
       "    \n",
       "    Input:\n",
       "    flattened : False(default)/True\n",
       "    \n",
       "    Output :\n",
       "    restacked dataset (flattened / not flattened)\n",
       "    '''\n",
       "    firstData = True\n",
       "    for sequences in recurrentIndexList:\n",
       "        first = True\n",
       "        for sequence in sequences:\n",
       "            if first:\n",
       "                stacked = copy.deepcopy(dataset[sequence])\n",
       "                first = False\n",
       "            else:\n",
       "                stacked = np.vstack((stacked, dataset[sequence]))\n",
       "        # reshape stacked data\n",
       "        # try if it already flattened or not\n",
       "        try:\n",
       "            stacked = stacked.reshape(1, stacked.shape[0], stacked.shape[1], stacked.shape[2], stacked.shape[3])\n",
       "            alreadyFlattened = False\n",
       "        except Exception:\n",
       "            stacked = stacked.reshape(1, stacked.shape[0]*stacked.shape[1])\n",
       "            alreadyFlattened = True\n",
       "        if firstData:\n",
       "            allStacked = copy.deepcopy(stacked)\n",
       "            firstData = False\n",
       "        else:\n",
       "            allStacked = np.vstack((allStacked, stacked))\n",
       "    \n",
       "    if flattened and not alreadyFlattened:\n",
       "        print(allStacked.shape)\n",
       "        return preparePrediction(allStacked, grayscale=grayscale)\n",
       "    elif flattened and alreadyFlattened:\n",
       "        return allStacked\n",
       "    else:\n",
       "        return allStacked\n",
       "\n",
       "    \n",
       "def performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True):\n",
       "    '''\n",
       "    Performing RNN Data Creation by passing data combination that want to be recreated as RNN sequence and list of number that acting as\n",
       "    how much sequence that want to be added before the t+0 data. For ex if the recurrentLists[0] says 2, it means that there will be 3 stacked data,\n",
       "    t-2, t-1, t+0.\n",
       "    \n",
       "    This function can process from 1 to 6 data combination(s)\n",
       "    \n",
       "    Input:\n",
       "    -usedDatas : array like of array of data combination(s) (up to 6) in sequence with dims\n",
       "    -dims : array like of dimensions, in squence with usedDatas\n",
       "    -recurrentLists : array like of lists of number that acting as how much sequence that want to be added before the t+0 data (>=1)\n",
       "    \n",
       "    '''\n",
       "\n",
       "    adte, adta = getAvailableSlicedData(maxData=True, hourOffset=0, dataScope=dataScope, wlstation=wlstation)\n",
       "    recurrentIndexLists=[]\n",
       "    for recurrentList in recurrentLists:\n",
       "        recurrentIndexList, availableRecurrentDate, availableRecurrentLabel = generateRNNInput(adte, adta, recurrentCount=recurrentList)\n",
       "        recurrentIndexLists.append(recurrentIndexList)\n",
       "\n",
       "    for j in range(len(usedDatas)):\n",
       "        usedData = usedDatas[j]\n",
       "        dim = dims[j]\n",
       "        # define the length of data\n",
       "        dataLength = len(usedData)\n",
       "        # read stored data\n",
       "        if dataLength == 1:\n",
       "            fileName = '{}{}'.format(usedData[0], dim)\n",
       "        elif dataLength == 2:\n",
       "            fileName = '{}{}{}'.format(usedData[0], usedData[1], dim)\n",
       "        elif dataLength == 3:\n",
       "            fileName = '{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], dim)\n",
       "        elif dataLength == 4:\n",
       "            fileName = '{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], dim)\n",
       "        elif dataLength == 5:\n",
       "            fileName = '{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], dim)\n",
       "        elif dataLength == 6:\n",
       "            fileName = '{}{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], dim)\n",
       "        elif dataLength == 7:\n",
       "            fileName = '{}{}{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], dim)\n",
       "        elif dataLength == 8:\n",
       "            fileName = '{}{}{}{}{}{}{}{}{}'.format(usedData[0], usedData[1], usedData[2], usedData[3], usedData[4], usedData[5], usedData[6], usedData[7], dim)\n",
       "        print(fileName)\n",
       "        try: # try if the default dataset is already stacked or not\n",
       "            fpath = 'dataset/{}RNN/{}.hdf5'.format(wlstation, fileName)\n",
       "            with h5py.File(fpath,'r') as f:\n",
       "                data = f['datas'][()]\n",
       "        except Exception:\n",
       "            fpath = 'dataset/{}RNN/{}f.hdf5'.format(wlstation, fileName)\n",
       "            with h5py.File(fpath,'r') as f:\n",
       "                data = f['datas'][()]\n",
       "            \n",
       "        for i in range(len(recurrentLists)):\n",
       "            print('{}-{}-{}'.format(fileName, dim, recurrentLists[i]))\n",
       "            # restacking the data\n",
       "            allStacked = restackRNNInput(recurrentIndexLists[i], data, flattened=flattened)\n",
       "            \n",
       "            # save restacked data to file\n",
       "            with h5py.File('dataset/{}RNN/{}r{}f.hdf5'.format(wlstation, fileName, recurrentLists[i]), 'w') as f:\n",
       "                f.create_dataset('datas', data=allStacked)\n",
       "\n",
       "# Option 1 : 17 best data combination of manggarai\n",
       "usedDatas=[('qvapor','sst'),('psf','qvapor'),('psf','qvapor','sst'),('qvapor','rain','sst'),('cloud','qvapor','sst'),('psf','qvapor','sst','wind'),('cloud','psf','qvapor','sst'),('qvapor','sst'),('psf','qvapor'),('psf','qvapor','sst'),('cloud','qvapor','sst'),('qvapor','rain','sst'),('qvapor','sst'),('psf','qvapor'),('psf','qvapor','sst'),('qvapor','rain','sst'),('cloud','qvapor','sst')]\n",
       "dims=(72,72,72,72,72,72,72,240,240,240,240,240,400,400,400,400,400)\n",
       "recurrentLists = (1,2,3,4,5,6,7,8,9,10,11)\n",
       "\n",
       "# Option 2 : best 5 of 1st manggarai RNN simulation with 1-11 Recurrent data, to simulate again with higher value of recurrent data (up to t-35 hours : 1.5 days)\n",
       "usedDatas2 = [[['cloud','psf','qvapor','sst'],],[['psf','qvapor'],],[['psf','qvapor'],],[['qvapor','sst'],],[['psf','qvapor','sst'],]]\n",
       "dims2 = [[72,],[240,],[72,],[240,],[72,]]\n",
       "recurrentLists2 = (12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35)\n",
       "\n",
       "# Option 3 : best 10 of katulampa DNN simulation\n",
       "usedDatas3 = [('cloud','psf','qvapor','rain','sst'),('psf','qvapor','sst'),('cloud','psf','qvapor','rain','sst','wind'),('cloud','psf','qvapor','sst','wind'),('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),('cloud','psf','qvapor','sst'),('cloud','psf','qvapor','rain','sst','wn10'),('cloud','psf','qvapor','sst','wn10'),('psf','qvapor','sst','wn10'),('qvapor','sst')]\n",
       "dims3 = (16,16,16,16,16,16,16,16,16,16)\n",
       "recurrentLists3 = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35)\n",
       "\n",
       "# Option 4 : best 5 of 1st manggarai RNN simulation. How much is too much recurrent data since in r29 we still get descent improvement? (up to t-95 hours : 4 days)\n",
       "usedDatas4 = [[['cloud','psf','qvapor','sst'],],[['psf','qvapor'],],[['psf','qvapor'],],[['qvapor','sst'],],[['psf','qvapor','sst'],]]\n",
       "dims4 = [[72,],[240,],[72,],[240,],[72,]]\n",
       "recurrentLists4 = (36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95)\n",
       "\n",
       "# Option 5 : Since the manggarai RNN producing very interesting result in high recurrent data, better to prepare for katulampa data too \n",
       "usedDatas5 = [('cloud','psf','qvapor','rain','sst'),('psf','qvapor','sst'),('cloud','psf','qvapor','rain','sst','wind'),('cloud','psf','qvapor','sst','wind'),('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),('cloud','psf','qvapor','sst'),('cloud','psf','qvapor','rain','sst','wn10'),('cloud','psf','qvapor','sst','wn10'),('psf','qvapor','sst','wn10'),('qvapor','sst')]\n",
       "dims5 = (16,16,16,16,16,16,16,16,16,16)\n",
       "recurrentLists5 = (36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95)\n",
       "\n",
       "# Option 6 : Katulampa 784 DNN Best 5\n",
       "usedDatas6 =[[('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10'),],[('psf','qvapor','rain','sst','wind','winu','wn10'),],[('cloud','psf','qvapor','rain','sst','wind','winu'),],[('cloud','psf','qvapor','rain','sst','wind','wn10'),],[('psf','qvapor','rain','sst','wind','winu'),]]\n",
       "dims6 = ([784,],[784,],[784,],[784,],[784,])\n",
       "recurrentLists6 = (12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39)\n",
       "\n",
       "# Option 7 : Katulampa 784 2 and 3 combinations\n",
       "usedDatas7 = [[('psf','qvapor'),],[('psf','qvapor','sst'),]]\n",
       "dims7 = ([784,],[784,])\n",
       "recurrentLists7 = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39)\n",
       "\n",
       "# Option 8 : Manggarai 240 & 400 to streamline comparison : 240 and 400\n",
       "usedDatas8 = [(('psf','qvapor','sst'),),(('psf','qvapor','sst'),)]\n",
       "dims8 = ((240,),(400,))\n",
       "recurrentLists8 = ((1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),(19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35))\n",
       "\n",
       "\n",
       "option = input('Enter sequence (0-3 for 1st Option, 4-8 for 2nd Option, or 9-13 for 3nd option) : ')\n",
       "# Option 1 : Manggarai RNN Part 1\n",
       "if option == '0':\n",
       "    usedDatas = usedDatas[:4]\n",
       "    dims = dims[:4]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '1':\n",
       "    usedDatas = usedDatas[4:8]\n",
       "    dims = dims[4:8]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '2':\n",
       "    usedDatas = usedDatas[8:12]\n",
       "    dims = dims[8:12]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '3':\n",
       "    usedDatas = usedDatas[12:]\n",
       "    dims = dims[12:]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "# Option 2 : Manggarai RNN part 2\n",
       "elif option == '4':\n",
       "    usedDatas = usedDatas2[0]\n",
       "    dims = dims2[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists2, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '5':\n",
       "    usedDatas = usedDatas2[1]\n",
       "    dims = dims2[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists2, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '6':\n",
       "    usedDatas = usedDatas2[2]\n",
       "    dims = dims2[2]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists2, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '7':\n",
       "    usedDatas = usedDatas2[3]\n",
       "    dims = dims2[3]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists2, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '8':\n",
       "    usedDatas = usedDatas2[4]\n",
       "    dims = dims2[4]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists2, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "# Option 3 : Katulampa RNN\n",
       "elif option == '9':\n",
       "    usedDatas = usedDatas3[:2]\n",
       "    dims = dims3[:2]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists3, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '10':\n",
       "    usedDatas = usedDatas3[2:4]\n",
       "    dims = dims3[2:4]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists3, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '11':\n",
       "    usedDatas = usedDatas3[4:6]\n",
       "    dims = dims3[4:6]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists3, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '12':\n",
       "    usedDatas = usedDatas3[6:8]\n",
       "    dims = dims3[6:8]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists3, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '13':\n",
       "    usedDatas = usedDatas3[8:]\n",
       "    dims = dims3[8:]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists3, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "# Option 4 : Manggarai RNN part 3\n",
       "elif option == '14':\n",
       "    usedDatas = usedDatas4[0]\n",
       "    dims = dims4[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists4, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '15':\n",
       "    usedDatas = usedDatas4[1]\n",
       "    dims = dims4[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists4, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '16':\n",
       "    usedDatas = usedDatas4[2]\n",
       "    dims = dims4[2]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists4, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '17':\n",
       "    usedDatas = usedDatas4[3]\n",
       "    dims = dims4[3]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists4, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '18':\n",
       "    usedDatas = usedDatas4[4]\n",
       "    dims = dims4[4]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists4, dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "# Option 5 : Katulampa RNN following descent manggarai RNN performance\n",
       "elif option == '19':\n",
       "    usedDatas = usedDatas5[1:3]\n",
       "    dims = dims5[1:3]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists5, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '20':\n",
       "    usedDatas = usedDatas5[3:5]\n",
       "    dims = dims5[3:5]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists5, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '21':\n",
       "    usedDatas = usedDatas5[5:7]\n",
       "    dims = dims5[5:7]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists5, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '22':\n",
       "    usedDatas = usedDatas5[7:9]\n",
       "    dims = dims5[7:9]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists5, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '23':\n",
       "    usedDatas = usedDatas5[9:]\n",
       "    dims = dims5[9:]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists5, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "# Option 6 : Katulampa RNN 784 in 1-11 dimension(s) for better insight\n",
       "elif option == '24':\n",
       "    usedDatas = usedDatas6[0]\n",
       "    dims = dims6[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists6, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '25':\n",
       "    usedDatas = usedDatas6[1]\n",
       "    dims = dims6[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists6, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '26':\n",
       "    usedDatas = usedDatas6[2]\n",
       "    dims = dims6[2]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists6, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '27':\n",
       "    usedDatas = usedDatas6[3]\n",
       "    dims = dims6[3]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists6, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '28':\n",
       "    usedDatas = usedDatas6[4]\n",
       "    dims = dims6[4]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists6, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "# Option 7 : Katulampa RNN 784 with 2 and 3 data combinations\n",
       "elif option == '29':\n",
       "    usedDatas = usedDatas7[0]\n",
       "    dims = dims7[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists7, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "elif option == '30':\n",
       "    usedDatas = usedDatas7[1]\n",
       "    dims = dims7[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists7, dataScope='prediction', wlstation='katulampa', flattened=True)\n",
       "# Option 8 : Manggarai RNN 240 & 400 \n",
       "elif option == '31':\n",
       "    usedDatas = usedDatas8[0]\n",
       "    dims = dims8[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists8[0], dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '32':\n",
       "    usedDatas = usedDatas8[0]\n",
       "    dims = dims8[0]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists8[1], dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '33':\n",
       "    usedDatas = usedDatas8[1]\n",
       "    dims = dims8[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists8[0], dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "elif option == '34':\n",
       "    usedDatas = usedDatas8[1]\n",
       "    dims = dims8[1]\n",
       "    performRNNDatasetCreation(usedDatas, dims, recurrentLists8[1], dataScope='prediction', wlstation='manggarai', flattened=True)\n",
       "\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "## Restacking LSTM RNN Input Datasets\n",
       "\n",
       "\n",
       "```python\n",
       "# restacking following input format requirement\n",
       "\n",
       "# defining variables\n",
       "R_COMBINATIONS = (3,)\n",
       "ROOT_PATH = './'\n",
       "DATASET_PATH = 'dataset/'\n",
       "RECURRENT_INDEX_PATH = 'dataset/prequisites/'\n",
       "RECURRENT_OFFSET_PATH = 'dataset/recurrent_offset/'\n",
       "WLSTATION = 'katulampa'\n",
       "current_data_name = 'sstqvaporpsfraincloud'\n",
       "current_data = ('sst','qvapor','psf','rain','cloud')\n",
       "\n",
       "# defining array of combination(s)\n",
       "O_COMBINATIONS = np.arange(0,1,1)\n",
       "DIMENSIONS = (1296,1600)\n",
       "\n",
       "# defining phase split of combination(s)\n",
       "# DEFINED BY R_COMBINATIONS\n",
       "\n",
       "# loop by O_COMBINATIONS\n",
       "HOTSTART = 0\n",
       "count = 0\n",
       "\n",
       "# loop by used data\n",
       "# loop by DIMENSIONS\n",
       "for dim in DIMENSIONS:\n",
       "    for R_COMBINATION in R_COMBINATIONS:\n",
       "        # restacking\n",
       "        for offset in O_COMBINATIONS:\n",
       "            # fetch recurrent index list\n",
       "            with h5py.File(f'{ROOT_PATH}{RECURRENT_INDEX_PATH}{WLSTATION}_R{R_COMBINATION}_O{offset}_recurrentIndexList.hdf5', 'r') as f:\n",
       "                recurrentIndexList = f['datas'][()]\n",
       "\n",
       "            ### CONTROLLED SECTION ###\n",
       "            tick = datetime.datetime.now()\n",
       "            if count < HOTSTART:\n",
       "                count+=1\n",
       "                continue\n",
       "\n",
       "            # fetch dataset\n",
       "            stored_dataset = []\n",
       "            for cd in current_data:\n",
       "                with h5py.File(f'{ROOT_PATH}{DATASET_PATH}master_{cd}{dim}f.hdf5', 'r') as f:\n",
       "                    stored_dataset.append(f['datas'][()])\n",
       "                # loop by recurrentIndexList, define temporary array, stack horizontally\n",
       "                first_sequences = True\n",
       "                for sequences in recurrentIndexList: # loop vertically\n",
       "                    first_sequence = True\n",
       "                    for sequence in sequences: # loop horizontally\n",
       "                      # sequence is index of dataset that need to be stacked\n",
       "                        first_dataset = True\n",
       "                        for idx, dataset in enumerate(stored_dataset): # loop by used data, because they need to be merged first (by the index)\n",
       "                            if first_dataset:\n",
       "                                first_dataset = False\n",
       "                                stacked_dataset = copy.deepcopy(dataset[sequence])\n",
       "                            else:\n",
       "                                stacked_dataset = np.hstack((stacked_dataset, dataset[sequence]))\n",
       "\n",
       "                        # for RNN / LSTM SEQUENCE, the format will be (batch, time, features)\n",
       "                        stacked_dataset = stacked_dataset.reshape(1, stacked_dataset.shape[0])\n",
       "\n",
       "                        if first_sequence:\n",
       "                            stacked_sequence = copy.deepcopy(stacked_dataset)\n",
       "                            first_sequence = False\n",
       "                        else:\n",
       "                            stacked_sequence = np.vstack((stacked_sequence, stacked_dataset))\n",
       "\n",
       "                    # reshape the array\n",
       "                    stacked_sequence = stacked_sequence.reshape(1, stacked_sequence.shape[0], stacked_sequence.shape[1])\n",
       "\n",
       "                    if first_sequences:\n",
       "                        stacked_sequences = copy.deepcopy(stacked_sequence)\n",
       "                        first_sequences = False\n",
       "                    else:\n",
       "                        stacked_sequences = np.vstack((stacked_sequences, stacked_sequence))\n",
       "\n",
       "            # store stacked sequences to file\n",
       "            with h5py.File(f'{ROOT_PATH}{RECURRENT_OFFSET_PATH}{current_data_name}{dim}_R{R_COMBINATION}_O{offset}_btf.hdf5', 'w') as f:\n",
       "                f.create_dataset('datas', data=stacked_sequences, compression='gzip', compression_opts=9)\n",
       "\n",
       "            tock = datetime.datetime.now()\n",
       "\n",
       "            # print control mechanism\n",
       "            print(f'{count} - DATA {current_data_name} - DIM {dim} - RECURRENT {R_COMBINATION} - OFFSET {offset} - time {tock-tick}')\n",
       "\n",
       "            count+=1\n",
       "            ### END OF CONTROLLED SECTION ###\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "## Restacking Flagged LSTM RNN Input Datasets\n",
       "\n",
       "\n",
       "```python\n",
       "# restacking following input format requirement\n",
       "\n",
       "# defining variables\n",
       "R_COMBINATIONS = (36,48)\n",
       "ROOT_PATH = './'\n",
       "DATASET_PATH = 'dataset/'\n",
       "RECURRENT_INDEX_PATH = 'dataset/prequisites/'\n",
       "RECURRENT_OFFSET_PATH = 'dataset/recurrent_offset/'\n",
       "WLSTATION = 'katulampa'\n",
       "current_data_name = 'sstqvaporpsfraincloud'\n",
       "current_data = ('sst','qvapor','psf','rain','cloud')\n",
       "\n",
       "# defining array of combination(s)\n",
       "O_COMBINATIONS = (0,)\n",
       "DIMENSIONS = (64,)\n",
       "\n",
       "# defining phase split of combination(s)\n",
       "# DEFINED BY R_COMBINATIONS\n",
       "\n",
       "# loop by O_COMBINATIONS\n",
       "HOTSTART = 0\n",
       "count = 0\n",
       "\n",
       "# loop by used data\n",
       "# loop by DIMENSIONS\n",
       "for dim in DIMENSIONS:\n",
       "    for R_COMBINATION in R_COMBINATIONS:\n",
       "        # restacking\n",
       "        for offset in O_COMBINATIONS:\n",
       "            # fetch recurrent index list\n",
       "            with h5py.File(f'{ROOT_PATH}{RECURRENT_INDEX_PATH}{WLSTATION}_R{R_COMBINATION}_O{offset}_recurrentIndexList!.hdf5', 'r') as f:\n",
       "                recurrentIndexList = f['datas'][()]\n",
       "\n",
       "            ### CONTROLLED SECTION ###\n",
       "            tick = datetime.datetime.now()\n",
       "            if count < HOTSTART:\n",
       "                count+=1\n",
       "                continue\n",
       "\n",
       "            # fetch dataset\n",
       "            stored_dataset = []\n",
       "            for cd in current_data:\n",
       "                with h5py.File(f'{ROOT_PATH}{DATASET_PATH}master_{cd}{dim}f.hdf5', 'r') as f:\n",
       "                    stored_dataset.append(f['datas'][()])\n",
       "                # loop by recurrentIndexList, define temporary array, stack horizontally\n",
       "                first_sequences = True\n",
       "                for sequences in recurrentIndexList: # loop vertically\n",
       "                    first_sequence = True\n",
       "                    for sequence in sequences: # loop horizontally\n",
       "                      # sequence is index of dataset that need to be stacked\n",
       "                        first_dataset = True\n",
       "                        for idx, dataset in enumerate(stored_dataset): # loop by used data, because they need to be merged first (by the index)\n",
       "                            if first_dataset:\n",
       "                                first_dataset = False\n",
       "                                stacked_dataset = copy.deepcopy(dataset[sequence])\n",
       "                            else:\n",
       "                                stacked_dataset = np.hstack((stacked_dataset, dataset[sequence]))\n",
       "\n",
       "                        # for RNN / LSTM SEQUENCE, the format will be (batch, time, features)\n",
       "                        stacked_dataset = stacked_dataset.reshape(1, stacked_dataset.shape[0])\n",
       "\n",
       "                        if first_sequence:\n",
       "                            stacked_sequence = copy.deepcopy(stacked_dataset)\n",
       "                            first_sequence = False\n",
       "                        else:\n",
       "                            stacked_sequence = np.vstack((stacked_sequence, stacked_dataset))\n",
       "\n",
       "                    # reshape the array\n",
       "                    stacked_sequence = stacked_sequence.reshape(1, stacked_sequence.shape[0], stacked_sequence.shape[1])\n",
       "\n",
       "                    if first_sequences:\n",
       "                        stacked_sequences = copy.deepcopy(stacked_sequence)\n",
       "                        first_sequences = False\n",
       "                    else:\n",
       "                        stacked_sequences = np.vstack((stacked_sequences, stacked_sequence))\n",
       "\n",
       "            # store stacked sequences to file\n",
       "            with h5py.File(f'{ROOT_PATH}{RECURRENT_OFFSET_PATH}{current_data_name}{dim}_R{R_COMBINATION}_O{offset}_btf!.hdf5', 'w') as f:\n",
       "                f.create_dataset('datas', data=stacked_sequences, compression='gzip', compression_opts=9)\n",
       "\n",
       "            tock = datetime.datetime.now()\n",
       "\n",
       "            # print control mechanism\n",
       "            print(f'{count} - DATA! {current_data_name} - DIM {dim} - RECURRENT {R_COMBINATION} - OFFSET {offset} - time {tock-tick}')\n",
       "\n",
       "            count+=1\n",
       "            ### END OF CONTROLLED SECTION ###\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "# Machine Learning Models\n",
       "\n",
       "## Deep Neural Network Model\n",
       "\n",
       "\n",
       "```python\n",
       "# importing modules\n",
       "import h5py, random, datetime, tensorflow as tf, numpy as np\n",
       "from sklearn.metrics import r2_score\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# run important functions\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "  \n",
       "def model6(flayer, slayer, epoch):\n",
       "    tf.keras.backend.clear_session()\n",
       "    initializer = tf.keras.initializers.GlorotNormal()\n",
       "    model = tf.keras.Sequential([\n",
       "        tf.keras.layers.Flatten(input_shape=(trainData.shape[1],)),\n",
       "        tf.keras.layers.Dense(flayer, activation='relu', kernel_initializer=initializer),\n",
       "        tf.keras.layers.Dense(slayer, activation='relu', kernel_initializer=initializer),\n",
       "        tf.keras.layers.Dense(1)\n",
       "    ])\n",
       "\n",
       "    # compiling model\n",
       "    opt = tf.keras.optimizers.Adam()\n",
       "    model.compile(optimizer='adam',\n",
       "                    loss=tf.keras.losses.MeanSquaredError(),\n",
       "                    metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
       "\n",
       "    # feed and train the model\n",
       "    model.fit(trainData, trainLabel, epochs=epoch, verbose=0)\n",
       "    return model\n",
       "\n",
       "def rmse(yreal, ypred):\n",
       "    return np.sqrt(np.mean(np.square(yreal-ypred)))\n",
       "\n",
       "def nse(yreal, ypred):\n",
       "    a = np.sum(np.square(ypred-yreal))\n",
       "    b = np.sum(np.square(yreal-np.mean(yreal)))\n",
       "    return 1-(a/b)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "### DEFINE THIS FIRST ###\n",
       "OFFSET = 0\n",
       "ITERATION = '78'\n",
       "DATASETSNAME = 'sst'\n",
       "DIMENSION = 16\n",
       "#########################\n",
       "\n",
       "ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "WLSTATION = 'katulampa'\n",
       "RECURRENTS = np.arange(0,7,1)\n",
       "FLAYERS = (4,8,12,16)\n",
       "SLAYERS = (1,2,3,4,5,6)\n",
       "EPOCHS = (50,100,250,500)\n",
       "\n",
       "loops = 0\n",
       "HOTSTART = 0\n",
       "for recurrent in RECURRENTS:\n",
       "    # load katulampa / manggarai adta\n",
       "    with h5py.File(f'{ROOT_PATH}dataset/prequisites/{WLSTATION}_R{recurrent}_O{OFFSET}_availableRecurrentLabel.hdf5', 'r') as f:\n",
       "        adta = f['datas'][()]\n",
       "    normalizedLabel, minStat, maxStat = normalizingLabels(adta)\n",
       "\n",
       "    # Load dataset\n",
       "    with h5py.File(f'{ROOT_PATH}dataset/recurrent_offset/{DATASETSNAME}{DIMENSION}_R{recurrent}_O{OFFSET}.hdf5', 'r') as f:\n",
       "        data = f['datas'][()]\n",
       "\n",
       "    # split train and test set\n",
       "    (trainData, trainLabel), (testData, testLabel) = splitTrainTest(data, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "\n",
       "    for flayer in FLAYERS:\n",
       "        for slayer in SLAYERS:\n",
       "            for epoch in EPOCHS:\n",
       "                # hotstart\n",
       "                if loops < HOTSTART:\n",
       "                loops+=1\n",
       "                continue\n",
       "\n",
       "                tick = datetime.datetime.now()\n",
       "                model = model6(flayer, slayer, epoch)\n",
       "\n",
       "                # evaluating model accuracy\n",
       "                prediction_model = tf.keras.Sequential([model,\n",
       "                                                        tf.keras.layers.ReLU()])\n",
       "                testPredictions = prediction_model.predict(testData)\n",
       "                trainPredictions = prediction_model.predict(trainData)\n",
       "\n",
       "                # make predictions\n",
       "                testPredictions = testPredictions*(maxStat-minStat)+minStat\n",
       "                trainPredictions = trainPredictions*(maxStat-minStat)+minStat\n",
       "                realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "                realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "                # Mean Squared Error : \n",
       "                mse = tf.keras.losses.MeanSquaredError()\n",
       "                mseTestError = mse(realTestLabel, testPredictions).numpy()\n",
       "                mseTrainError = mse(realTrainLabel, trainPredictions).numpy()\n",
       "\n",
       "                # RMSE\n",
       "                rmseTest = rmse(np.squeeze(testPredictions), realTestLabel)\n",
       "                rmseTrain = rmse(np.squeeze(trainPredictions), realTrainLabel)\n",
       "\n",
       "                # NSE\n",
       "                nseTest = nse(realTestLabel, np.squeeze(testPredictions))\n",
       "                nseTrain = nse(realTrainLabel, np.squeeze(trainPredictions))\n",
       "\n",
       "                # R^2\n",
       "                r2Test = r2_score(realTestLabel, testPredictions)\n",
       "                r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "                # save statistics to csv\n",
       "                statistics = '{},{},{},{},{},{},{},{},{},{},{}\\n'.format(flayer, slayer, epoch, mseTrainError, mseTestError, rmseTrain, rmseTest, r2Train, r2Test, nseTrain, nseTest)\n",
       "                with open('{}models_statistics/{}_GS_{}_R{}_O{}.csv'.format(ROOT_PATH, ITERATION, DATASETSNAME, recurrent, OFFSET), 'a') as stat:\n",
       "                    stat.write(statistics)\n",
       "\n",
       "                # save model to drive\n",
       "                model.save('{}models/{}/{}_R{}_O{}__GS_{}_{}_{}.h5'.format(ROOT_PATH, ITERATION, DATASETSNAME, recurrent, OFFSET, flayer, slayer, epoch))\n",
       "\n",
       "                # loop identifier :\n",
       "                tock = datetime.datetime.now()\n",
       "                print('{} : {} - R{} - O{} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{DATASETSNAME}{DIMENSION}', recurrent, OFFSET, flayer, slayer, epoch, tock-tick, r2Train, r2Test))\n",
       "                loops+=1\n",
       "      \n",
       "```\n",
       "\n",
       "## Deep Neural Network Flagged Model\n",
       "\n",
       "\n",
       "```python\n",
       "# run important functions\n",
       "def create_connection(db_file):\n",
       "    '''\n",
       "    create a database connection to a SQLite database\n",
       "    specified by db_file\n",
       "    :param db_file : database file\n",
       "    :return: Connection Object or None\n",
       "    '''\n",
       "    conn=None\n",
       "    try:\n",
       "        conn=sqlite3.connect(db_file)\n",
       "        return conn\n",
       "    except Error as e:\n",
       "        print(e)  \n",
       "\n",
       "def manggaraiFullData():\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM manggarai', conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "    \n",
       "    return manggaraiConv\n",
       "\n",
       "def manggaraiDataList(maxData=True, hourOffset=0, wlstation='manggarai'):\n",
       "    '''\n",
       "    Returning a tuple of list (date, data) of manggarai TMA data with 10-minutes-interval from DSDA dataset in year 2020\n",
       "    '''\n",
       "    # read and fetch database data to pandas dataframe\n",
       "    dsdaPath='../mining_dsda/dsda.db'\n",
       "    conn=create_connection(dsdaPath)\n",
       "    manggarai=pd.read_sql_query('SELECT * FROM {}'.format(wlstation), conn)\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggarai.set_index('currentdate')\n",
       "\n",
       "    # convert data type from object to string\n",
       "    manggaraiConv=manggarai.convert_dtypes()\n",
       "\n",
       "    # set main index to currentdate\n",
       "    manggaraiConv.set_index('currentdate')\n",
       "\n",
       "    # convert date datatype to datetime64[ns]\n",
       "    manggaraiConv['currentdate']=manggaraiConv['currentdate'].astype('datetime64[ns]')\n",
       "\n",
       "    # slicing data to 2020 timeframe\n",
       "    #mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00') & (manggaraiConv['currentdate'] <= '2021-04-03 23:50')\n",
       "    mask = (manggaraiConv['currentdate'] >= '2019-02-01 00:00')\n",
       "    manggaraiSlice2020=manggaraiConv.loc[mask]\n",
       "\n",
       "    # converting 10-minute-data to hourly data\n",
       "    startDate=datetime.datetime(2019,2,1)\n",
       "    minutes=[x*10 for x in range(6)]\n",
       "    hours=[x for x in range(24)]\n",
       "    days=[x for x in range(780)]\n",
       "\n",
       "    dateListHourly=[]\n",
       "    dataListHourly=[]\n",
       "    for day in days:\n",
       "        for hour in hours:\n",
       "            hourlyData=[]\n",
       "\n",
       "            # set error indicator back to false\n",
       "            error=False\n",
       "\n",
       "            for minute in minutes:\n",
       "                # perform data fetch, add to list, and get max value\n",
       "                dateLoop=startDate+datetime.timedelta(days=day, hours=hour+hourOffset, minutes=minute)\n",
       "                rowFetch=manggaraiSlice2020.loc[(manggaraiSlice2020['currentdate'] == dateLoop)]\n",
       "                #print(rowFetch)\n",
       "\n",
       "                # try to fetch if the result is not zero\n",
       "                try:\n",
       "                    dataFetch=rowFetch['data'].item()\n",
       "                    hourlyData.append(dataFetch)\n",
       "                except ValueError:\n",
       "                    error=True\n",
       "\n",
       "            # insert data if error indicator is False\n",
       "            if not error:\n",
       "                # make hourly date using timedelta\n",
       "                hourlyDate=startDate+datetime.timedelta(days=day, hours=hour)\n",
       "                \n",
       "                if maxData:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=max(hourlyData)\n",
       "                else:\n",
       "                    # get maximum value of hourly data\n",
       "                    maxDataHourly=hourlyData.mean()\n",
       "\n",
       "                # insert value to global list\n",
       "                dateListHourly.append(hourlyDate)\n",
       "                dataListHourly.append(maxDataHourly)\n",
       "            else: # if error occured during data fetch (null or something else)\n",
       "                continue # to next loop\n",
       "    return dateListHourly, dataListHourly\n",
       "\n",
       "def getHimawariFilename():\n",
       "    '''\n",
       "    Return dictionary of available himawari data based on filename inside\n",
       "    folder as a key\n",
       "    '''\n",
       "    himawariPath='../mining_sadewa/sadewa/'\n",
       "    # load folder name\n",
       "    directory=[directory for directory in os.listdir(himawariPath)]\n",
       "\n",
       "    # store fileame\n",
       "    himawari={}\n",
       "\n",
       "    # load all filename stored on disk to dictionary with each folder name as keys\n",
       "    for direct in directory:\n",
       "        fpath='{}{}'.format(himawariPath, direct)\n",
       "        himawari[direct]=[fname for fname in os.listdir(fpath)]\n",
       "        \n",
       "    return himawari\n",
       "\n",
       "def extractHimawariDatetime():\n",
       "    '''\n",
       "    Extract every filename in sadewa-himawari data to datetime object for easier handling\n",
       "    \n",
       "    Returns :\n",
       "    extractedDate -- dictionary containing list of datetime object for each filename inside dictionary keys for every data\n",
       "    '''\n",
       "    himawari=getHimawariFilename()\n",
       "\n",
       "    # extract date for each himawari data type to datetime.datetime object\n",
       "    observations=['CCLD','B04','IR1','IR3','VIS']\n",
       "    extractedDate={}\n",
       "    for obs in observations:\n",
       "        extractedDate[obs]=[datetime.datetime.strptime(x.replace('H89_{}_'.format(obs),'').replace('.png',''), '%Y%m%d%H%M') for x in himawari[obs]]\n",
       "\n",
       "    predictions=['cloud','psf','qvapor','rain','sst','wind','winu','wn10']\n",
       "    for pred in predictions:\n",
       "        extractedDate[pred]=[datetime.datetime.strptime(x.replace('{}_'.format(pred),'').replace('.png','').replace('_','')+'00', '%Y%m%d%H%M') for x in himawari[pred]]\n",
       "        \n",
       "    return extractedDate\n",
       "\n",
       "def getAvailableSlicedData(maxData=True, hourOffset=0, dataScope='combination', wlstation='manggarai', flagged=False):\n",
       "    '''\n",
       "    check through all available dataset, including manggarai TMA, sadewa-himawari IR1, IR3, VIS, B04, and CCLD\n",
       "    and return a tuple containing datetime object and manggarai hourly TMA data that are synced through all available dataset\n",
       "    \n",
       "    This function doesn't return sadewa-himawari data, because using the datetime format and the sadewa-himawari data types,\n",
       "    the full name of the file required can be constructed.\n",
       "    \n",
       "    return : (slicedDate, slicedData) # both are lists inside a tuple\n",
       "    '''\n",
       "    extractedDate = extractHimawariDatetime()\n",
       "        \n",
       "    # getting date-data slice from himawari and manggarai TMA data\n",
       "\n",
       "    # using function to get manggarai available date-data\n",
       "    dateListHourly, dataListHourly = manggaraiDataList(maxData, hourOffset, wlstation=wlstation)\n",
       "    \n",
       "    # check if the data is flagged above the mean or not\n",
       "    if flagged:\n",
       "        dateListHourly, dataListHourly = flagData(dateListHourly, dataListHourly)\n",
       "\n",
       "    # loop to every data\n",
       "    # check algorithm : manggarai checked against every himawari data, and if all true, date is inserted to sliced data\n",
       "    slicedDate=[]\n",
       "    slicedData=[]\n",
       "    for i in range(len(dateListHourly)):\n",
       "        \n",
       "        if dataScope == 'combination':\n",
       "            usedData=['CCLD','B04','IR1','IR3','VIS','rain','cloud','psf','qvapor','sst']\n",
       "        elif dataScope == 'prediction':\n",
       "            usedData=('cloud','psf','qvapor','rain','sst','wind','winu','wn10')\n",
       "\n",
       "        # defining control mechanism\n",
       "        checked=True\n",
       "\n",
       "        # loop through every himawari data\n",
       "        for used in usedData:\n",
       "            if dateListHourly[i] not in extractedDate[used]:\n",
       "                checked=False # set checked to False if there are no complementary data found in another dataset\n",
       "\n",
       "        # input data if all checked\n",
       "        if checked:\n",
       "            slicedDate.append(dateListHourly[i])\n",
       "            slicedData.append(dataListHourly[i])\n",
       "    return slicedDate, slicedData\n",
       "\n",
       "def flagData(adte, adta):\n",
       "    '''\n",
       "    Filter date and data above the mean\n",
       "    '''\n",
       "    adtaDF = pd.DataFrame(adta).astype('int32')\n",
       "    adteDF = pd.DataFrame(adte)\n",
       "    flaggedAdta = adtaDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    flaggedAdte = adteDF[adtaDF[0] > adtaDF.mean()[0]]\n",
       "    return list(flaggedAdte[0].dt.to_pydatetime()), list(flaggedAdta[0].astype('object'))\n",
       "\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "  \n",
       "def model6(flayer, slayer, epoch):\n",
       "    tf.keras.backend.clear_session()\n",
       "    initializer = tf.keras.initializers.GlorotNormal()\n",
       "    model = tf.keras.Sequential([\n",
       "      tf.keras.layers.Flatten(input_shape=(trainData.shape[1],)),\n",
       "      tf.keras.layers.Dense(flayer, activation='relu', kernel_initializer=initializer),\n",
       "      tf.keras.layers.Dense(slayer, activation='relu', kernel_initializer=initializer),\n",
       "      tf.keras.layers.Dense(1)\n",
       "    ])\n",
       "\n",
       "    # compiling model\n",
       "    opt = tf.keras.optimizers.Adam()\n",
       "    model.compile(optimizer='adam',\n",
       "                  loss=tf.keras.losses.MeanSquaredError(),\n",
       "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
       "\n",
       "    # feed and train the model\n",
       "    model.fit(trainData, trainLabel, epochs=epoch, verbose=0)\n",
       "    return model\n",
       "\n",
       "def rmse(yreal, ypred):\n",
       "    return np.sqrt(np.mean(np.square(yreal-ypred)))\n",
       "\n",
       "def nse(yreal, ypred):\n",
       "    a = np.sum(np.square(ypred-yreal))\n",
       "    b = np.sum(np.square(yreal-np.mean(yreal)))\n",
       "    return 1-(a/b)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# load input data\n",
       "USED_DATA = ('cloud', 'psf', 'qvapor', 'rain', 'sst', 'wind', 'winu', 'wn10')\n",
       "datasetname = ''.join(USED_DATA)\n",
       "dim = 16\n",
       "with h5py.File(f'./dataset/{datasetname}{dim}f!.hdf5', 'r') as f:\n",
       "    input_data = f['datas'][()]\n",
       "\n",
       "# fetch label\n",
       "WLSTATION = 'katulampa'\n",
       "FLAGGED = True\n",
       "adte, adta = getAvailableSlicedData(dataScope='prediction', hourOffset=0, wlstation=WLSTATION, flagged=FLAGGED)\n",
       "adta = np.array(adta).astype('int16')\n",
       "\n",
       "# normalize label\n",
       "normalizedLabel, minStat, maxStat = normalizingLabels(adta)\n",
       "\n",
       "# split train and test set\n",
       "(trainData, trainLabel), (testData, testLabel) = splitTrainTest(input_data, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# Load dataset\n",
       "# split train and test set\n",
       "# run the simulations\n",
       "# model variables\n",
       "\n",
       "ITERATION = '76'\n",
       "FLAYERS = (4,8,12,16)\n",
       "SLAYERS = (1,2,3,4,5,6)\n",
       "EPOCHS = (50,100,250,500)\n",
       "\n",
       "loops = 0\n",
       "HOTSTART = 0\n",
       "for flayer in FLAYERS:\n",
       "    for slayer in SLAYERS:\n",
       "        for epoch in EPOCHS:\n",
       "            # hotstart\n",
       "            if loops < HOTSTART:\n",
       "                loops+=1\n",
       "                continue\n",
       "\n",
       "            tick = datetime.datetime.now()\n",
       "            model = model6(flayer, slayer, epoch)\n",
       "\n",
       "            # evaluating model accuracy\n",
       "            prediction_model = tf.keras.Sequential([model,\n",
       "                                                    tf.keras.layers.ReLU()])\n",
       "            testPredictions = prediction_model.predict(testData)\n",
       "            trainPredictions = prediction_model.predict(trainData)\n",
       "\n",
       "            # make predictions\n",
       "            testPredictions = testPredictions*(maxStat-minStat)+minStat\n",
       "            trainPredictions = trainPredictions*(maxStat-minStat)+minStat\n",
       "            realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "            realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "            # Mean Squared Error : \n",
       "            mse = tf.keras.losses.MeanSquaredError()\n",
       "            mseTestError = mse(realTestLabel, testPredictions).numpy()\n",
       "            mseTrainError = mse(realTrainLabel, trainPredictions).numpy()\n",
       "\n",
       "            # RMSE\n",
       "            rmseTest = rmse(np.squeeze(testPredictions), realTestLabel)\n",
       "            rmseTrain = rmse(np.squeeze(trainPredictions), realTrainLabel)\n",
       "\n",
       "            # NSE\n",
       "            nseTest = nse(realTestLabel, np.squeeze(testPredictions))\n",
       "            nseTrain = nse(realTrainLabel, np.squeeze(trainPredictions))\n",
       "\n",
       "            # R^2\n",
       "            r2Test = r2_score(realTestLabel, testPredictions)\n",
       "            r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "            # save statistics to csv\n",
       "            statistics = '{},{},{},{},{},{},{},{},{},{},{}\\n'.format(flayer, slayer, epoch, mseTrainError, mseTestError, rmseTrain, rmseTest, r2Train, r2Test, nseTrain, nseTest)\n",
       "            with open('models_statistics/{}_GS_{}.csv'.format(ITERATION, datasetname), 'a') as stat:\n",
       "                stat.write(statistics)\n",
       "\n",
       "            # save model to drive\n",
       "            model.save('models/{}/{}_GS_{}_{}_{}.h5'.format(ITERATION, datasetname, flayer, slayer, epoch))\n",
       "\n",
       "            # loop identifier :\n",
       "            tock = datetime.datetime.now()\n",
       "            print('{} : {} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{datasetname}{dim}', flayer, slayer, epoch, tock-tick, r2Train, r2Test))\n",
       "            loops+=1\n",
       "      \n",
       "```\n",
       "\n",
       "## Simple Neural Network Model\n",
       "\n",
       "\n",
       "```python\n",
       "# importing modules\n",
       "import h5py, random, datetime, tensorflow as tf, numpy as np\n",
       "from sklearn.metrics import r2_score\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# run important functions\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "  \n",
       "def model6(flayer, slayer, epoch):\n",
       "    tf.keras.backend.clear_session()\n",
       "    initializer = tf.keras.initializers.GlorotNormal()\n",
       "    model = tf.keras.Sequential([\n",
       "        tf.keras.layers.Flatten(input_shape=(trainData.shape[1],)),\n",
       "        tf.keras.layers.Dense(flayer, activation='relu', kernel_initializer=initializer),\n",
       "        tf.keras.layers.Dense(slayer, activation='relu', kernel_initializer=initializer),\n",
       "        tf.keras.layers.Dense(1)\n",
       "    ])\n",
       "\n",
       "    # compiling model\n",
       "    opt = tf.keras.optimizers.Adam()\n",
       "    model.compile(optimizer='adam',\n",
       "                    loss=tf.keras.losses.MeanSquaredError(),\n",
       "                    metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
       "\n",
       "    # feed and train the model\n",
       "    model.fit(trainData, trainLabel, epochs=epoch, verbose=0)\n",
       "    return model\n",
       "\n",
       "def rmse(yreal, ypred):\n",
       "    return np.sqrt(np.mean(np.square(yreal-ypred)))\n",
       "\n",
       "def nse(yreal, ypred):\n",
       "    a = np.sum(np.square(ypred-yreal))\n",
       "    b = np.sum(np.square(yreal-np.mean(yreal)))\n",
       "    return 1-(a/b)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "### DEFINE THIS FIRST ###\n",
       "RECURRENT = 24\n",
       "OFFSET = 0\n",
       "#########################\n",
       "\n",
       "ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "WLSTATION = 'katulampa'\n",
       "DIMENSION = 16\n",
       "ITERATIONS = ('78','79','80','81','82','83','84','85')\n",
       "USED_DATA = ('sst', 'qvapor', 'psf', 'rain', 'cloud', 'wind', 'winu', 'wn10')\n",
       "FLAYERS = (4,8,12,16)\n",
       "SLAYERS = (1,2,3,4,5,6)\n",
       "EPOCHS = (50,100,250,500)\n",
       "\n",
       "loops = 0\n",
       "HOTSTART = 0\n",
       "for input_num, _ in enumerate(USED_DATA):\n",
       "    current_data = USED_DATA[0:input_num+1]\n",
       "    DATASETSNAME = ''.join(current_data)\n",
       "\n",
       "    # load katulampa / manggarai adta\n",
       "    with h5py.File(f'{ROOT_PATH}dataset/prequisites/{WLSTATION}_R{RECURRENT}_O{OFFSET}_availableRecurrentLabel.hdf5', 'r') as f:\n",
       "        adta = f['datas'][()]\n",
       "    normalizedLabel, minStat, maxStat = normalizingLabels(adta)\n",
       "\n",
       "    # Load dataset\n",
       "    with h5py.File(f'{ROOT_PATH}dataset/recurrent_offset/{DATASETSNAME}{DIMENSION}_R{RECURRENT}_O{OFFSET}.hdf5', 'r') as f:\n",
       "        data = f['datas'][()]\n",
       "\n",
       "    # split train and test set\n",
       "    (trainData, trainLabel), (testData, testLabel) = splitTrainTest(data, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "\n",
       "    for flayer in FLAYERS:\n",
       "        for slayer in SLAYERS:\n",
       "            for epoch in EPOCHS:\n",
       "                # hotstart\n",
       "                if loops < HOTSTART:\n",
       "                    loops+=1\n",
       "                    continue\n",
       "\n",
       "                tick = datetime.datetime.now()\n",
       "                model = model6(flayer, slayer, epoch)\n",
       "\n",
       "                # evaluating model accuracy\n",
       "                prediction_model = tf.keras.Sequential([model,\n",
       "                                                        tf.keras.layers.ReLU()])\n",
       "                testPredictions = prediction_model.predict(testData)\n",
       "                trainPredictions = prediction_model.predict(trainData)\n",
       "\n",
       "                # make predictions\n",
       "                testPredictions = testPredictions*(maxStat-minStat)+minStat\n",
       "                trainPredictions = trainPredictions*(maxStat-minStat)+minStat\n",
       "                realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "                realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "                # Mean Squared Error : \n",
       "                mse = tf.keras.losses.MeanSquaredError()\n",
       "                mseTestError = mse(realTestLabel, testPredictions).numpy()\n",
       "                mseTrainError = mse(realTrainLabel, trainPredictions).numpy()\n",
       "\n",
       "                # RMSE\n",
       "                rmseTest = rmse(np.squeeze(testPredictions), realTestLabel)\n",
       "                rmseTrain = rmse(np.squeeze(trainPredictions), realTrainLabel)\n",
       "\n",
       "                # NSE\n",
       "                nseTest = nse(realTestLabel, np.squeeze(testPredictions))\n",
       "                nseTrain = nse(realTrainLabel, np.squeeze(trainPredictions))\n",
       "\n",
       "                # R^2\n",
       "                r2Test = r2_score(realTestLabel, testPredictions)\n",
       "                r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "                # save statistics to csv\n",
       "                statistics = '{},{},{},{},{},{},{},{},{},{},{}\\n'.format(flayer, slayer, epoch, mseTrainError, mseTestError, rmseTrain, rmseTest, r2Train, r2Test, nseTrain, nseTest)\n",
       "                with open('{}models_statistics/{}_GS_{}_R{}_O{}.csv'.format(ROOT_PATH, ITERATIONS[input_num], DATASETSNAME, RECURRENT, OFFSET), 'a') as stat:\n",
       "                    stat.write(statistics)\n",
       "\n",
       "                # save model to drive\n",
       "                model.save('{}models/{}/{}_R{}_O{}__GS_{}_{}_{}.h5'.format(ROOT_PATH, ITERATIONS[input_num], DATASETSNAME, RECURRENT, OFFSET, flayer, slayer, epoch))\n",
       "\n",
       "                # loop identifier :\n",
       "                tock = datetime.datetime.now()\n",
       "                print('{} : {} - R{} - O{} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{DATASETSNAME}{DIMENSION}', RECURRENT, OFFSET, flayer, slayer, epoch, tock-tick, r2Train, r2Test))\n",
       "                loops+=1\n",
       "      \n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "## LSTM Recurrent Neural Network Model\n",
       "\n",
       "\n",
       "```python\n",
       "## DEFINE THIS FIRST ##\n",
       "ITERATION = '108'\n",
       "dim = 16\n",
       "R_COMBINATIONS = (24,)\n",
       "OFFSET = 24\n",
       "#######################\n",
       "\n",
       "# import modules\n",
       "import datetime, h5py, copy, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, datetime, random\n",
       "from sklearn.metrics import r2_score\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "\n",
       "# VARIABLES / COEFFICIENTS\n",
       "WLSTATION = 'katulampa'\n",
       "current_data_name = 'sstqvaporpsfraincloud'\n",
       "\n",
       "# MODELS\n",
       "EPOCHS = (144,288,480)\n",
       "LSTMUNITS = (80,)\n",
       "DLAYERS1 = (4,8,16)\n",
       "DLAYERS2 = (2,4,8)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# important function\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "\n",
       "def executeRNN_LSTM(HOTSTART, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2):\n",
       "    '''\n",
       "    Execute preconfigured RNN Models that print, save statistics, and models to Google Drive\n",
       "    '''\n",
       "    # structuring & fitting the model\n",
       "    loops = 0\n",
       "    #HOTSTART = 0\n",
       "    for R_COMBINATION in R_COMBINATIONS:\n",
       "        # open stacked input required\n",
       "        ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "        DATASET_PATH = 'dataset/'\n",
       "        RECURRENT_INDEX_PATH = 'dataset/prequisites/'\n",
       "        RECURRENT_OFFSET_PATH = 'dataset/recurrent_offset/'\n",
       "\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_OFFSET_PATH}{current_data_name}{dim}_R{R_COMBINATION}_O{OFFSET}_btf.hdf5', 'r') as f:\n",
       "            INPUT = f['datas'][()]\n",
       "\n",
       "        # creating label\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_INDEX_PATH}{WLSTATION}_R{R_COMBINATION}_O{OFFSET}_availableRecurrentLabel.hdf5', 'r') as f:\n",
       "            LABEL = f['datas'][()]\n",
       "            \n",
       "        normalizedLabel, minStat, maxStat = normalizingLabels(LABEL)\n",
       "        # split train and test\n",
       "        (trainData, trainLabel), (testData, testLabel) = splitTrainTest(INPUT, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "        \n",
       "        for LSTMUNIT in LSTMUNITS:\n",
       "            for DLAYER1 in DLAYERS1:\n",
       "                for DLAYER2 in DLAYERS2:\n",
       "                    for EPOCH in EPOCHS:\n",
       "                        # hotstart\n",
       "                        if loops < HOTSTART:\n",
       "                            loops+=1\n",
       "                            continue\n",
       "\n",
       "                        tick = datetime.datetime.now()\n",
       "\n",
       "                        initializer = tf.keras.initializers.HeNormal()\n",
       "                        lstm_model = tf.keras.Sequential([\n",
       "                            # Shape [batch, time, features]\n",
       "                            tf.keras.layers.LSTM(LSTMUNIT, return_sequences=False),\n",
       "                            # Output\n",
       "                            tf.keras.layers.Dense(DLAYER1, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(DLAYER2, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(1)\n",
       "                        ])\n",
       "\n",
       "                        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, mode='min')\n",
       "\n",
       "                        lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
       "                                      optimizer=tf.optimizers.Adam(),\n",
       "                                      metrics=[tf.metrics.RootMeanSquaredError()])\n",
       "\n",
       "                        lstm_model.fit(trainData, trainLabel, epochs=EPOCH, verbose=0)\n",
       "\n",
       "                        # evaluating model accuracy\n",
       "                        prediction_model = tf.keras.Sequential([lstm_model])\n",
       "                        testPredictionsN = prediction_model.predict(testData)\n",
       "                        trainPredictionsN = prediction_model.predict(trainData)\n",
       "\n",
       "                        # make predictions\n",
       "                        testPredictions = testPredictionsN*(maxStat-minStat)+minStat\n",
       "                        trainPredictions = trainPredictionsN*(maxStat-minStat)+minStat\n",
       "                        realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "                        realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "                        # R^2\n",
       "                        r2Test = r2_score(realTestLabel, testPredictions)\n",
       "                        r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "                        # save statistics to csv\n",
       "                        statistics = '{},{},{},{},{},{}\\n'.format(LSTMUNIT, DLAYER1, DLAYER2, EPOCH, r2Train, r2Test)\n",
       "                        with open('{}models_statistics/{}_{}_R{}_O{}.csv'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET), 'a') as stat:\n",
       "                            stat.write(statistics)\n",
       "\n",
       "                        # save model to drive\n",
       "                        lstm_model.save('{}models/{}/{}_R{}_O{}_LSTM_{}_{}_{}_{}.h5'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET, LSTMUNIT, DLAYER1, DLAYER2, EPOCH))\n",
       "\n",
       "                        tock = datetime.datetime.now()\n",
       "                        print('LSTM {} : {} - {} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{current_data_name}{dim}', LSTMUNIT, DLAYER1, DLAYER2, EPOCH, tock-tick, r2Train, r2Test))\n",
       "\n",
       "                        loops+=1\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "executeRNN_LSTM(0, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2)\n",
       "```\n",
       "\n",
       "## LSTM Recurrent Neural Network Flagged Model\n",
       "\n",
       "\n",
       "```python\n",
       "## DEFINE THIS FIRST ##\n",
       "ITERATION = '121'\n",
       "dim = 64\n",
       "R_COMBINATIONS = (24,)\n",
       "OFFSET = 0\n",
       "#######################\n",
       "\n",
       "# import modules\n",
       "import datetime, h5py, copy, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, datetime, random\n",
       "from sklearn.metrics import r2_score\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "\n",
       "# VARIABLES / COEFFICIENTS\n",
       "WLSTATION = 'katulampa'\n",
       "current_data_name = 'sstqvaporpsfraincloud'\n",
       "\n",
       "# MODELS\n",
       "EPOCHS = (144,360)\n",
       "LSTMUNITS = (120,140)\n",
       "DLAYERS1 = (16,8,4)\n",
       "DLAYERS2 = (8,4,2)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# important function\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "\n",
       "def executeRNN_LSTM_FLAGGED(HOTSTART, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2):\n",
       "    '''\n",
       "    Execute preconfigured RNN Models that print, save statistics, and models to Google Drive\n",
       "    '''\n",
       "    # structuring & fitting the model\n",
       "    loops = 0\n",
       "    #HOTSTART = 0\n",
       "    for R_COMBINATION in R_COMBINATIONS:\n",
       "        # open stacked input required\n",
       "        ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "        DATASET_PATH = 'dataset/'\n",
       "        RECURRENT_INDEX_PATH = 'dataset/prequisites/'\n",
       "        RECURRENT_OFFSET_PATH = 'dataset/recurrent_offset/'\n",
       "\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_OFFSET_PATH}{current_data_name}{dim}_R{R_COMBINATION}_O{OFFSET}_btf!.hdf5', 'r') as f:\n",
       "            INPUT = f['datas'][()]\n",
       "\n",
       "        # creating label\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_INDEX_PATH}{WLSTATION}_R{R_COMBINATION}_O{OFFSET}_availableRecurrentLabel!.hdf5', 'r') as f:\n",
       "            LABEL = f['datas'][()]\n",
       "            \n",
       "        normalizedLabel, minStat, maxStat = normalizingLabels(LABEL)\n",
       "        # split train and test\n",
       "        (trainData, trainLabel), (testData, testLabel) = splitTrainTest(INPUT, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "        \n",
       "        for LSTMUNIT in LSTMUNITS:\n",
       "            for DLAYER1 in DLAYERS1:\n",
       "                for DLAYER2 in DLAYERS2:\n",
       "                    for EPOCH in EPOCHS:\n",
       "                        # hotstart\n",
       "                        if loops < HOTSTART:\n",
       "                            loops+=1\n",
       "                            continue\n",
       "\n",
       "                        tick = datetime.datetime.now()\n",
       "\n",
       "                        initializer = tf.keras.initializers.HeNormal()\n",
       "                        lstm_model = tf.keras.Sequential([\n",
       "                            # Shape [batch, time, features]\n",
       "                            tf.keras.layers.LSTM(LSTMUNIT, return_sequences=False),\n",
       "                            # Output\n",
       "                            tf.keras.layers.Dense(DLAYER1, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(DLAYER2, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(1)\n",
       "                        ])\n",
       "\n",
       "                        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, mode='min')\n",
       "\n",
       "                        lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
       "                                      optimizer=tf.optimizers.Adam(),\n",
       "                                      metrics=[tf.metrics.RootMeanSquaredError()])\n",
       "\n",
       "                        lstm_model.fit(trainData, trainLabel, epochs=EPOCH, verbose=0)\n",
       "\n",
       "                        # evaluating model accuracy\n",
       "                        prediction_model = tf.keras.Sequential([lstm_model])\n",
       "                        testPredictionsN = prediction_model.predict(testData)\n",
       "                        trainPredictionsN = prediction_model.predict(trainData)\n",
       "\n",
       "                        # make predictions\n",
       "                        testPredictions = testPredictionsN*(maxStat-minStat)+minStat\n",
       "                        trainPredictions = trainPredictionsN*(maxStat-minStat)+minStat\n",
       "                        realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "                        realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "                        # R^2\n",
       "                        r2Test = r2_score(realTestLabel, testPredictions)\n",
       "                        r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "                        # save statistics to csv\n",
       "                        statistics = '{},{},{},{},{},{}\\n'.format(LSTMUNIT, DLAYER1, DLAYER2, EPOCH, r2Train, r2Test)\n",
       "                        with open('{}models_statistics/{}_{}_R{}_O{}.csv'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET), 'a') as stat:\n",
       "                            stat.write(statistics)\n",
       "\n",
       "                        # save model to drive\n",
       "                        lstm_model.save('{}models/{}/{}_R{}_O{}_LSTM_{}_{}_{}_{}.h5'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET, LSTMUNIT, DLAYER1, DLAYER2, EPOCH))\n",
       "\n",
       "                        tock = datetime.datetime.now()\n",
       "                        print('LSTM! {} : {} - {} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{current_data_name}{dim}', LSTMUNIT, DLAYER1, DLAYER2, EPOCH, tock-tick, r2Train, r2Test))\n",
       "\n",
       "                        loops+=1\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "executeRNN_LSTM_FLAGGED(0, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2)\n",
       "```\n",
       "\n",
       "## LSTM Recurrent Neural Network Unflagged 4K Data Model\n",
       "\n",
       "\n",
       "```python\n",
       "## DEFINE THIS FIRST ##\n",
       "ITERATION = '123'\n",
       "dim = 16\n",
       "R_COMBINATIONS = (24,)\n",
       "OFFSET = 0\n",
       "#######################\n",
       "\n",
       "# import modules\n",
       "import datetime, h5py, copy, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, datetime, random\n",
       "from sklearn.metrics import r2_score\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "\n",
       "# VARIABLES / COEFFICIENTS\n",
       "WLSTATION = 'katulampa'\n",
       "current_data_name = 'sstqvaporpsfraincloud'\n",
       "\n",
       "# MODELS\n",
       "EPOCHS = (144,288,480)\n",
       "LSTMUNITS = (80,)\n",
       "DLAYERS1 = (4,8,16)\n",
       "DLAYERS2 = (2,4,8)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# important function\n",
       "def normalizingLabels(adta):\n",
       "    '''\n",
       "    Return normalized input data from 0 to 1, min, max value to convert back to predicted label\n",
       "    '''\n",
       "    minStat = np.min(adta)\n",
       "    maxStat = np.max(adta)\n",
       "\n",
       "    norm = (adta - minStat)/(maxStat - minStat)\n",
       "\n",
       "    return norm, minStat, maxStat\n",
       "\n",
       "def splitTrainTest(data, label, startBound=None, endBound=None, split=0.8, shuffle=False, randomSeed=None):\n",
       "    if shuffle:\n",
       "        random.seed(randomSeed)\n",
       "        merge = list(zip(data, label))\n",
       "        try:\n",
       "            print(data.shape, label.shape)\n",
       "        except Exception:\n",
       "            pass\n",
       "        random.shuffle(merge)\n",
       "        data, label = zip(*merge)\n",
       "        data = np.array(data)\n",
       "        label = np.array(label)\n",
       "        #random.shuffle(data)\n",
       "        #random.shuffle(label)\n",
       "\n",
       "    boundData = data[startBound:endBound]\n",
       "    boundLabel = label[startBound:endBound]\n",
       "\n",
       "    splitBound = round(split*len(boundLabel))\n",
       "    trainData = boundData[:splitBound]\n",
       "    trainLabel = boundLabel[:splitBound]\n",
       "    testData = boundData[splitBound:]\n",
       "    testLabel = boundLabel[splitBound:]\n",
       "\n",
       "    return (trainData, trainLabel), (testData, testLabel)\n",
       "\n",
       "def executeRNN_LSTM_HALF(HOTSTART, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2):\n",
       "    '''\n",
       "    Execute preconfigured RNN Models that print, save statistics, and models to Google Drive\n",
       "    '''\n",
       "    # structuring & fitting the model\n",
       "    loops = 0\n",
       "    #HOTSTART = 0\n",
       "    for R_COMBINATION in R_COMBINATIONS:\n",
       "        # open stacked input required\n",
       "        ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "        DATASET_PATH = 'dataset/'\n",
       "        RECURRENT_INDEX_PATH = 'dataset/prequisites/'\n",
       "        RECURRENT_OFFSET_PATH = 'dataset/recurrent_offset/'\n",
       "\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_OFFSET_PATH}{current_data_name}{dim}_R{R_COMBINATION}_O{OFFSET}_btf.hdf5', 'r') as f:\n",
       "            INPUT = f['datas'][()]\n",
       "\n",
       "        # creating label\n",
       "        with h5py.File(f'{ROOT_PATH}{RECURRENT_INDEX_PATH}{WLSTATION}_R{R_COMBINATION}_O{OFFSET}_availableRecurrentLabel.hdf5', 'r') as f:\n",
       "            LABEL = f['datas'][()]\n",
       "            \n",
       "        normalizedLabel, minStat, maxStat = normalizingLabels(LABEL)\n",
       "\n",
       "        # cut paired data to half\n",
       "        (INPUT, normalizedLabel), (_, _) = splitTrainTest(INPUT, normalizedLabel, split=0.45, shuffle=True, randomSeed=10)\n",
       "\n",
       "        # split train and test\n",
       "        (trainData, trainLabel), (testData, testLabel) = splitTrainTest(INPUT, normalizedLabel, split=0.7, shuffle=True, randomSeed=10)\n",
       "        \n",
       "        for LSTMUNIT in LSTMUNITS:\n",
       "            for DLAYER1 in DLAYERS1:\n",
       "                for DLAYER2 in DLAYERS2:\n",
       "                    for EPOCH in EPOCHS:\n",
       "                        # hotstart\n",
       "                        if loops < HOTSTART:\n",
       "                            loops+=1\n",
       "                            continue\n",
       "\n",
       "                        tick = datetime.datetime.now()\n",
       "\n",
       "                        initializer = tf.keras.initializers.HeNormal()\n",
       "                        lstm_model = tf.keras.Sequential([\n",
       "                            # Shape [batch, time, features]\n",
       "                            tf.keras.layers.LSTM(LSTMUNIT, return_sequences=False),\n",
       "                            # Output\n",
       "                            tf.keras.layers.Dense(DLAYER1, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(DLAYER2, activation='relu', kernel_initializer=initializer),\n",
       "                            tf.keras.layers.Dense(1)\n",
       "                        ])\n",
       "\n",
       "                        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, mode='min')\n",
       "\n",
       "                        lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
       "                                      optimizer=tf.optimizers.Adam(),\n",
       "                                      metrics=[tf.metrics.RootMeanSquaredError()])\n",
       "\n",
       "                        lstm_model.fit(trainData, trainLabel, epochs=EPOCH, verbose=0)\n",
       "\n",
       "                        # evaluating model accuracy\n",
       "                        prediction_model = tf.keras.Sequential([lstm_model])\n",
       "                        testPredictionsN = prediction_model.predict(testData)\n",
       "                        trainPredictionsN = prediction_model.predict(trainData)\n",
       "\n",
       "                        # make predictions\n",
       "                        testPredictions = testPredictionsN*(maxStat-minStat)+minStat\n",
       "                        trainPredictions = trainPredictionsN*(maxStat-minStat)+minStat\n",
       "                        realTestLabel = testLabel*(maxStat-minStat)+minStat\n",
       "                        realTrainLabel = trainLabel*(maxStat-minStat)+minStat\n",
       "\n",
       "                        # R^2\n",
       "                        r2Test = r2_score(realTestLabel, testPredictions)\n",
       "                        r2Train = r2_score(realTrainLabel, trainPredictions)\n",
       "\n",
       "                        # save statistics to csv\n",
       "                        statistics = '{},{},{},{},{},{}\\n'.format(LSTMUNIT, DLAYER1, DLAYER2, EPOCH, r2Train, r2Test)\n",
       "                        with open('{}models_statistics/{}_{}_R{}_O{}.csv'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET), 'a') as stat:\n",
       "                            stat.write(statistics)\n",
       "\n",
       "                        # save model to drive\n",
       "                        lstm_model.save('{}models/{}/{}_R{}_O{}_LSTM_{}_{}_{}_{}.h5'.format(ROOT_PATH, ITERATION, current_data_name, R_COMBINATION, OFFSET, LSTMUNIT, DLAYER1, DLAYER2, EPOCH))\n",
       "\n",
       "                        tock = datetime.datetime.now()\n",
       "                        print('LSTM {} : {} - {} - {} - {} - {} : time : {} - R^2 err : train[{}] test[{}]'.format(loops, f'{current_data_name}{dim}', LSTMUNIT, DLAYER1, DLAYER2, EPOCH, tock-tick, r2Train, r2Test))\n",
       "\n",
       "                        loops+=1\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "executeRNN_LSTM_HALF(0, ITERATION, current_data_name, dim, OFFSET, R_COMBINATIONS, EPOCHS, LSTMUNITS, DLAYERS1, DLAYERS2)\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "```\n",
       "\n",
       "# Data Fetch\n",
       "This part used to run on the local machine or anything where you can save and access your file anytime later\n",
       "\n",
       "This whole document is available in this [Google Colaboratory](colab.research.google.com)\\*. You can run the whole project just by following this documents step-by-step, but it can took very long time to complete, because some raw-file-dependencies need to be freshly fetched from remote server that in total contains ~50GB (~100K files). I used to run the part code that involved fetching data by running 10-20 separate running-task to make it complete faster; just copy-paste part of process that need parallelism to new python file (\\*.py), and run it on multiple tab **Terminal**.\n",
       "\n",
       "### Dependencies\n",
       "\n",
       "\\*If you need dependencies without need to pre-fetch the whole data from scratch, let me know by contacting me via [E-mail](mailto:jonathanraditya@live.com). I will send one-time-access Google Drive authorization link that contains all of the dependencies. All of the dependencies are hosted in Google Drive not GitHub because quota and bandwidth limitation with GitHub (1GB), that after preprocessing, total space needed easily exceed >100GB.\n",
       "\n",
       "\n",
       "```python\n",
       "# Run this section if you have Google Drive authorization link\n",
       "from google.colab import drive\n",
       "drive.mount('/content/gdrive', force_remount=True)\n",
       "ROOT_PATH = './gdrive/MyDrive/#PROJECT/rnn_ciliwung/'\n",
       "```\n",
       "\n",
       "\n",
       "```python\n",
       "# Except not, run this section\n",
       "ROOT_PATH = './'\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('code_workflow.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce995e6-73b0-4570-9c01-00a51de4298f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
